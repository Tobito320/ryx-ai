# RYX AI - Empfohlene Modelle

## Ãœbersicht: Multi-Modell-Strategie

Ryx braucht verschiedene Modelle fÃ¼r verschiedene Aufgaben.
Jedes Modell hat seinen optimalen Einsatzzweck.

---

## ðŸš€ FAST (< 3B Parameter) - Instant Response

### **Qwen2.5:1.5b** â­ EMPFOHLEN
```bash
ollama pull qwen2.5:1.5b
```
- **StÃ¤rke**: Extrem schnell, gutes Deutsch, Ã¼berraschend intelligent
- **Nutzen**: Intent-Erkennung, einfache Fragen, Typo-Korrektur
- **VRAM**: ~2GB
- **Speed**: <100ms pro Token

### **Phi-3-mini (3.8B)** 
```bash
ollama pull phi3:mini
```
- **StÃ¤rke**: Beste QualitÃ¤t unter 4B, sehr kohÃ¤rent
- **Nutzen**: Schnelle Chat-Antworten, Zusammenfassungen
- **VRAM**: ~4GB

### **Gemma2:2b**
```bash
ollama pull gemma2:2b
```
- **StÃ¤rke**: Google-QualitÃ¤t, gute Allgemeinbildung
- **Nutzen**: Schnelle Faktenfragen, Definitionen
- **VRAM**: ~3GB

---

## âš–ï¸ BALANCED (7-14B Parameter) - Allround

### **Qwen2.5:7b** â­ EMPFOHLEN
```bash
ollama pull qwen2.5:7b
```
- **StÃ¤rke**: Bestes Preis-Leistungs-VerhÃ¤ltnis, 88% HumanEval!
- **Nutzen**: Default fÃ¼r alles - Chat, Planung, einfaches Coding
- **VRAM**: ~8GB
- **Kontext**: 32K Tokens

### **Llama3.1:8b**
```bash
ollama pull llama3.1:8b
```
- **StÃ¤rke**: Gute Code-Generierung, natÃ¼rliche Konversation
- **Nutzen**: Alternative zu Qwen, gut fÃ¼r englische Tasks
- **VRAM**: ~8GB
- **Kontext**: 128K Tokens!

### **Mistral:7b**
```bash
ollama pull mistral:7b
```
- **StÃ¤rke**: Schnell, effizient, 32K Kontext
- **Nutzen**: Wenn lÃ¤ngerer Kontext wichtiger ist als max. QualitÃ¤t
- **VRAM**: ~6GB

---

## ðŸ§  SMART (14-32B Parameter) - Komplexe Aufgaben

### **Qwen2.5-Coder:14b** â­ EMPFOHLEN FÃœR CODING
```bash
ollama pull qwen2.5-coder:14b
```
- **StÃ¤rke**: Speziell fÃ¼r Code trainiert, versteht Repos
- **Nutzen**: Code-Tasks, Refactoring, Bug-Fixes, PLAN-Phase
- **VRAM**: ~12GB
- **HumanEval**: 88%+

### **DeepSeek-Coder-V2:16b**
```bash
ollama pull deepseek-coder-v2:16b
```
- **StÃ¤rke**: Sehr gut fÃ¼r Debugging, Fill-in-the-middle
- **Nutzen**: Code-Analyse, ErklÃ¤rungen, Fehlersuche
- **VRAM**: ~14GB

### **Codestral:22b** (von Mistral)
```bash
ollama pull codestral:22b
```
- **StÃ¤rke**: 80+ Sprachen, exzellente Code-Completion
- **Nutzen**: Polyglot-Coding, wenn viele Sprachen gebraucht werden
- **VRAM**: ~16GB

---

## ðŸŽ¯ PRECISION (20B+ Parameter) - Maximum QualitÃ¤t

### **Qwen2.5:32b** â­ EMPFOHLEN
```bash
ollama pull qwen2.5:32b
```
- **StÃ¤rke**: Beste lokale Allround-QualitÃ¤t, GPT-4 Level fÃ¼r viele Tasks
- **Nutzen**: Komplexe Planung, kritische Code-Ã„nderungen, VERIFY-Phase
- **VRAM**: ~24GB
- **Kontext**: 128K Tokens

### **DeepSeek-R1:32b** (Distilled)
```bash
ollama pull deepseek-r1:32b
```
- **StÃ¤rke**: Chain-of-Thought Reasoning, Mathe, Logik
- **Nutzen**: Komplexe ProblemlÃ¶sung, mehrstufige Analysen
- **VRAM**: ~24GB
- **MMLU**: 90%+

### **Llama3.3:70b** (Quantized Q4)
```bash
ollama pull llama3.3:70b-instruct-q4_K_M
```
- **StÃ¤rke**: GrÃ¶ÃŸtes praktisch nutzbares Modell
- **Nutzen**: Wenn absolute QualitÃ¤t wichtiger ist als Speed
- **VRAM**: ~40GB (Q4 quantized)

---

## ðŸ”§ SPECIALIZED - Spezialaufgaben

### **FÃ¼r SQL/Datenbank**
```bash
ollama pull sqlcoder:15b
```
- Speziell fÃ¼r SQL-Generierung trainiert

### **FÃ¼r Embeddings/RAG**
```bash
ollama pull nomic-embed-text
```
- Schnelle Vektor-Embeddings fÃ¼r Semantic Search

### **FÃ¼r Vision (Bilder)**
```bash
ollama pull llava:13b
```
- Kann Bilder analysieren (Screenshots, Diagramme)

---

## ðŸ“Š Empfohlene Ryx-Konfiguration

```yaml
# configs/models.yaml

models:
  # Blitzschnell - fÃ¼r Intent-Erkennung
  fast:
    default: "qwen2.5:1.5b"
    alternatives:
      - "phi3:mini"
      - "gemma2:2b"
  
  # Ausgewogen - fÃ¼r normalen Chat
  balanced:
    default: "qwen2.5:7b"
    alternatives:
      - "llama3.1:8b"
      - "mistral:7b"
  
  # Code-Spezialist - fÃ¼r PLAN/APPLY Phasen
  coding:
    default: "qwen2.5-coder:14b"
    alternatives:
      - "deepseek-coder-v2:16b"
      - "codestral:22b"
  
  # Maximum QualitÃ¤t - fÃ¼r kritische Entscheidungen
  precision:
    default: "qwen2.5:32b"
    alternatives:
      - "deepseek-r1:32b"
      - "llama3.3:70b-instruct-q4_K_M"
  
  # FÃ¼r Embeddings
  embedding: "nomic-embed-text"

# Automatische Modellauswahl
routing:
  intent_detection: "fast"
  simple_chat: "balanced"
  code_explore: "balanced"
  code_plan: "coding"
  code_apply: "coding"
  code_verify: "precision"
  web_search_synthesis: "balanced"
  complex_reasoning: "precision"
```

---

## ðŸ† Top-Empfehlung fÃ¼r Ryx

**Minimum Setup (8GB VRAM):**
```bash
ollama pull qwen2.5:1.5b    # Fast
ollama pull qwen2.5:7b      # Balanced
```

**Optimal Setup (16GB VRAM):**
```bash
ollama pull qwen2.5:1.5b         # Fast
ollama pull qwen2.5:7b           # Balanced  
ollama pull qwen2.5-coder:14b    # Coding
```

**Power Setup (24GB+ VRAM):**
```bash
ollama pull qwen2.5:1.5b         # Fast
ollama pull qwen2.5:7b           # Balanced
ollama pull qwen2.5-coder:14b    # Coding
ollama pull qwen2.5:32b          # Precision
ollama pull deepseek-r1:32b      # Reasoning
```

---

## Warum Qwen dominiert (2024/2025)

1. **HumanEval 88%** - SchlÃ¤gt sogar manche GPT-4 Versionen im Coding
2. **Multi-language** - 92+ Sprachen, exzellentes Deutsch
3. **Agentic-ready** - Trainiert fÃ¼r Tool-Use und Function-Calling  
4. **GroÃŸe Kontext-Fenster** - Bis zu 128K Tokens
5. **Aktiv entwickelt** - Alibaba released regelmÃ¤ÃŸig Updates
6. **Effizient** - Gute Performance auch mit Quantisierung

DeepSeek ist die beste Alternative fÃ¼r:
- Chain-of-Thought Reasoning (R1)
- Budget-Setup (V2 Lite)
- Debugging-Tasks
