"""
RyxHub Exam System API v2

FastAPI endpoints for the German Berufsschule exam preparation system.
Features:
- Multi-stage Upload Pipeline with OCR + AI Classification
- AI-driven Exam Generation with context and prompts
- AI-based Grading with confidence scores and feedback
"""

from fastapi import APIRouter, File, UploadFile, HTTPException, BackgroundTasks, Query
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any, Union
from datetime import datetime
from enum import Enum
import uuid
import json
import hashlib
import asyncio
import aiohttp
import os

# Import AI prompts
from .ai_prompts import (
    call_classification_model,
    call_exam_generator_model,
    call_grading_model,
    build_classification_prompt,
    build_exam_generation_prompt,
    build_grading_prompt,
)

router = APIRouter(prefix="/api/exam", tags=["exam"])

# Configuration
OLLAMA_BASE_URL = os.environ.get("OLLAMA_BASE_URL", "http://localhost:11434")
ANTHROPIC_API_KEY = os.environ.get("ANTHROPIC_API_KEY", "")
DEFAULT_CLASSIFIER_MODEL = "qwen2.5:7b"
DEFAULT_GENERATOR_MODEL = "qwen2.5:14b"
DEFAULT_GRADER_MODEL = "qwen2.5:14b"

# ============================================================================
# Enums
# ============================================================================

class TaskType(str, Enum):
    MC_SINGLE = "MC_SingleChoice"
    MC_MULTI = "MC_MultipleChoice"
    SHORT_ANSWER = "ShortAnswer"
    CASE_STUDY = "CaseStudy"
    FILL_BLANK = "FillInBlank"
    CALCULATION = "Calculation"
    DIAGRAM = "DiagramAnalysis"
    MATCHING = "Matching"
    ESSAY = "Essay"

class PipelineStatus(str, Enum):
    IDLE = "idle"
    RUNNING = "running"
    WAITING_REVIEW = "waiting_review"
    COMPLETED = "completed"
    FAILED = "failed"

class ModelPhase(str, Enum):
    VISION_OCR = "vision_ocr"
    NLP_CLASSIFIER = "nlp_classifier"
    EXAM_GENERATOR = "exam_generator"
    GRADER = "grader"

# ============================================================================
# Data Models
# ============================================================================

class School(BaseModel):
    id: str
    name: str
    location: str
    subjects: List[str] = []
    created_at: str = Field(default_factory=lambda: datetime.utcnow().isoformat())

class Subject(BaseModel):
    id: str
    school_id: str
    name: str
    full_name: Optional[str] = None
    teacher_ids: List[str] = []
    thema_ids: List[str] = []

class Teacher(BaseModel):
    id: str
    name: str
    subject_ids: List[str] = []
    tests_count: int = 0
    exam_pattern_profile: Optional[Dict[str, Any]] = None

class Thema(BaseModel):
    id: str
    subject_id: str
    name: str
    parent_thema_id: Optional[str] = None
    frequency: int = 0

class DiagramSpec(BaseModel):
    type: str  # bar, pie, line, scatter
    title: str
    data: List[Dict[str, Any]]
    xLabel: Optional[str] = None
    yLabel: Optional[str] = None

class CalculationData(BaseModel):
    formula: str
    variables: Dict[str, float]
    expectedResult: float
    tolerance: Optional[float] = 0.01

class GradingRubric(BaseModel):
    maxPoints: int
    criteria: List[Dict[str, Any]]
    autoGradable: bool = False
    partialCreditAllowed: bool = True
    keywords: Optional[List[str]] = None

class Task(BaseModel):
    id: str
    type: str
    taskNumber: int
    questionText: str
    questionImage: Optional[str] = None
    options: Optional[List[Dict[str, Any]]] = None
    correctAnswer: Optional[str] = None
    modelAnswer: Optional[str] = None
    calculationData: Optional[Dict[str, Any]] = None
    diagramSpec: Optional[Dict[str, Any]] = None
    matchingPairs: Optional[List[Dict[str, str]]] = None
    blanks: Optional[List[Dict[str, Any]]] = None
    points: int
    difficulty: int = 3
    timeEstimateMinutes: int = 5
    gradingRubric: Dict[str, Any]
    generatedBy: str = "ai"
    confidence: int = 100

class MockExam(BaseModel):
    id: str
    subjectId: str
    themaIds: List[str]
    title: str
    description: Optional[str] = None
    tasks: List[Task]
    totalPoints: int
    estimatedDurationMinutes: int
    difficultyLevel: int
    teacherPatternUsed: Optional[str] = None
    generatedAt: str = Field(default_factory=lambda: datetime.utcnow().isoformat())
    status: str = "ready"
    freePrompt: Optional[str] = None
    contextUsed: Optional[List[str]] = None

class TaskGrade(BaseModel):
    taskId: str
    earnedPoints: float
    maxPoints: int
    percentage: float
    isCorrect: bool
    isPartiallyCorrect: bool = False
    autoGraded: bool
    confidence: int
    feedback: str
    feedbackType: str  # correct, partial, incorrect, needs_review
    detailedFeedback: Optional[Dict[str, Any]] = None
    criteriaScores: Optional[List[Dict[str, Any]]] = None
    needsManualReview: bool = False
    reviewReason: Optional[str] = None

class Attempt(BaseModel):
    id: str
    mockExamId: str
    userId: str = "default-user"
    startedAt: str
    finishedAt: Optional[str] = None
    durationSeconds: int = 0
    taskResponses: List[Dict[str, Any]] = []
    taskGrades: Optional[List[TaskGrade]] = None
    totalScore: float = 0
    totalPoints: int = 0
    grade: float = 6.0
    gradeText: str = "Ungenügend"
    percentage: float = 0.0
    status: str = "in_progress"
    overallFeedback: Optional[str] = None
    strengths: Optional[List[str]] = None
    areasForImprovement: Optional[List[str]] = None
    overallConfidence: int = 0
    tasksNeedingReview: Optional[List[str]] = None

# Pipeline Models
class PipelineStep(BaseModel):
    id: str
    phase: str
    label: str
    status: str = "pending"
    startedAt: Optional[str] = None
    completedAt: Optional[str] = None
    durationMs: Optional[int] = None
    error: Optional[str] = None
    confidence: Optional[int] = None
    output: Optional[Dict[str, Any]] = None

class PipelineProgress(BaseModel):
    pipelineId: str
    status: str
    currentPhase: Optional[str] = None
    steps: List[PipelineStep]
    startedAt: str
    completedAt: Optional[str] = None

class ClassificationResult(BaseModel):
    subject: Dict[str, Any]
    mainThema: Dict[str, Any]
    additionalThemas: List[Dict[str, Any]] = []
    teacher: Dict[str, Any]
    examDate: Dict[str, Any]
    examType: Dict[str, Any]
    totalPoints: Dict[str, Any]
    extractedTasks: List[Dict[str, Any]] = []
    overallConfidence: int
    requiresReview: bool
    reviewReasons: List[str] = []

class UploadAnalysisResult(BaseModel):
    analysisId: str
    filename: str
    fileHash: str
    ocrText: str
    ocrConfidence: int
    classification: ClassificationResult
    pipeline: PipelineProgress
    suggestedCorrections: Optional[List[Dict[str, Any]]] = None

# Request Models
class UploadConfirmRequest(BaseModel):
    analysisId: str
    corrections: Dict[str, Any] = {}
    skipTasks: List[int] = []

class ExamGenerationRequest(BaseModel):
    subjectId: str
    themaIds: List[str]
    difficulty: int = 3
    taskCount: int = 15
    durationMinutes: int = 90
    teacherId: Optional[str] = None
    useTeacherPattern: bool = False
    freePrompt: Optional[str] = None
    contextClassTestIds: Optional[List[str]] = None
    contextText: Optional[str] = None
    includeDiagrams: bool = True
    taskTypeDistribution: Optional[Dict[str, int]] = None

class SubmitAnswerRequest(BaseModel):
    taskId: str
    userAnswer: Any

class FinishAttemptRequest(BaseModel):
    attemptId: str
    forceGrade: bool = False  # Skip confidence check

# ============================================================================
# In-Memory Storage
# ============================================================================

_schools: Dict[str, School] = {}
_subjects: Dict[str, Subject] = {}
_teachers: Dict[str, Teacher] = {}
_themas: Dict[str, Thema] = {}
_class_tests: Dict[str, Dict] = {}
_mock_exams: Dict[str, MockExam] = {}
_attempts: Dict[str, Attempt] = {}
_pending_analyses: Dict[str, UploadAnalysisResult] = {}

def _init_default_data():
    # School
    school = School(
        id="cuno-berufskolleg",
        name="Cuno Berufskolleg Hagen",
        location="Hagen, NRW",
        subjects=["wbl", "bwl", "it", "deutsch", "mathe", "englisch"]
    )
    _schools[school.id] = school
    
    # Subjects - Including IT!
    subjects = [
        Subject(id="wbl", school_id="cuno-berufskolleg", name="WBL", full_name="Wirtschaft und Betriebslehre"),
        Subject(id="bwl", school_id="cuno-berufskolleg", name="BWL", full_name="Betriebswirtschaftslehre"),
        Subject(id="it", school_id="cuno-berufskolleg", name="IT", full_name="IT-Systeme / Informatik"),
        Subject(id="deutsch", school_id="cuno-berufskolleg", name="Deutsch", full_name="Deutsch / Kommunikation"),
        Subject(id="mathe", school_id="cuno-berufskolleg", name="Mathe", full_name="Mathematik"),
        Subject(id="englisch", school_id="cuno-berufskolleg", name="Englisch", full_name="Wirtschaftsenglisch"),
    ]
    for s in subjects:
        _subjects[s.id] = s
    
    # Themas - WBL
    wbl_themas = [
        Thema(id="marktforschung", subject_id="wbl", name="Marktforschung"),
        Thema(id="marketingmix", subject_id="wbl", name="Marketingmix (4Ps)"),
        Thema(id="kundenakquisition", subject_id="wbl", name="Kundenakquisition"),
        Thema(id="preismanagement", subject_id="wbl", name="Preismanagement"),
        Thema(id="werbung", subject_id="wbl", name="Werbung & Kommunikation"),
    ]
    
    # Themas - IT
    it_themas = [
        Thema(id="it-service", subject_id="it", name="IT-Service & Support"),
        Thema(id="netzwerke", subject_id="it", name="Netzwerke & Protokolle"),
        Thema(id="hardware", subject_id="it", name="Hardware & Komponenten"),
        Thema(id="software", subject_id="it", name="Software & Betriebssysteme"),
        Thema(id="datenschutz", subject_id="it", name="Datenschutz & IT-Sicherheit"),
        Thema(id="datenbanken", subject_id="it", name="Datenbanken & SQL"),
    ]
    
    for t in wbl_themas + it_themas:
        _themas[t.id] = t
    
    # Default teachers
    teachers = [
        Teacher(id="hakim", name="Herr Hakim", subject_ids=["wbl", "it"]),
        Teacher(id="mueller", name="Frau Müller", subject_ids=["deutsch"]),
        Teacher(id="schmidt", name="Herr Schmidt", subject_ids=["bwl", "mathe"]),
    ]
    for t in teachers:
        _teachers[t.id] = t

_init_default_data()

# ============================================================================
# Helper Functions
# ============================================================================

def calculate_grade(percentage: float) -> tuple[float, str]:
    """IHK-Standard grade calculation."""
    if percentage >= 92:
        return (1.0, "Sehr gut")
    elif percentage >= 81:
        return (2.0, "Gut")
    elif percentage >= 67:
        return (3.0, "Befriedigend")
    elif percentage >= 50:
        return (4.0, "Ausreichend")
    elif percentage >= 30:
        return (5.0, "Mangelhaft")
    else:
        return (6.0, "Ungenügend")

def gen_id(prefix: str = "") -> str:
    return f"{prefix}{uuid.uuid4().hex[:8]}"

async def call_ollama(prompt: str, model: str = DEFAULT_CLASSIFIER_MODEL, temperature: float = 0.1) -> str:
    """Call Ollama API."""
    async with aiohttp.ClientSession() as session:
        try:
            async with session.post(
                f"{OLLAMA_BASE_URL}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {"temperature": temperature}
                },
                timeout=aiohttp.ClientTimeout(total=120)
            ) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    return data.get("response", "")
                else:
                    return f"Error: {resp.status}"
        except Exception as e:
            return f"Error: {str(e)}"

def parse_json_from_response(response: str) -> Optional[Dict]:
    """Extract JSON from model response."""
    try:
        json_start = response.find('{')
        json_end = response.rfind('}') + 1
        if json_start >= 0 and json_end > json_start:
            return json.loads(response[json_start:json_end])
    except:
        pass
    return None

# ============================================================================
# Upload Pipeline Endpoints
# ============================================================================

@router.post("/upload-analyze")
async def upload_analyze(
    file: UploadFile = File(...),
    subject_hint: Optional[str] = Query(None, description="Optional subject hint"),
) -> UploadAnalysisResult:
    """
    Stage 1: Upload and analyze a class test.
    Returns OCR text + AI classification for user review.
    Does NOT persist until confirmed.
    """
    # Validate file type
    allowed_types = ["application/pdf", "image/png", "image/jpeg", "image/webp"]
    if file.content_type not in allowed_types:
        raise HTTPException(400, f"Unsupported file type: {file.content_type}")
    
    content = await file.read()
    file_hash = hashlib.md5(content).hexdigest()
    analysis_id = gen_id("analysis-")
    
    # Check for duplicates
    for existing_id, existing in _class_tests.items():
        if existing.get("file_hash") == file_hash:
            raise HTTPException(400, f"Diese Datei wurde bereits hochgeladen (ID: {existing_id})")
    
    # Create pipeline progress
    pipeline = PipelineProgress(
        pipelineId=gen_id("pipe-"),
        status="running",
        currentPhase="vision_ocr",
        steps=[
            PipelineStep(
                id="step-1",
                phase="vision_ocr",
                label="Vision-Modell (OCR)",
                status="running",
                startedAt=datetime.utcnow().isoformat()
            ),
            PipelineStep(
                id="step-2",
                phase="nlp_classifier",
                label="KI-Klassifikation",
                status="pending"
            ),
        ],
        startedAt=datetime.utcnow().isoformat()
    )
    
    # TODO: Replace with actual OCR (PaddleOCR / Tesseract)
    # For now, simulate OCR based on filename
    filename_lower = file.filename.lower() if file.filename else ""
    
    # Simulate OCR text based on filename hints
    if "it" in filename_lower or "service" in filename_lower:
        ocr_text = """
Klassenarbeit IT-Service
Herr Hakim
Datum: 10.12.2024

Aufgabe 1 (5 Punkte):
Erklären Sie den Unterschied zwischen First-Level und Second-Level Support.

Aufgabe 2 (10 Punkte):
Ein Kunde meldet, dass sein Computer nicht startet. Beschreiben Sie systematisch Ihre Vorgehensweise zur Fehlerdiagnose.

Aufgabe 3 (8 Punkte):
Was versteht man unter einem Service Level Agreement (SLA)? Nennen Sie drei typische Bestandteile.

Aufgabe 4 (7 Punkte):
Berechnen Sie die Verfügbarkeit eines Systems, das im letzten Monat (30 Tage) insgesamt 4 Stunden ausgefallen war.

Aufgabe 5 (10 Punkte):
Analysieren Sie das folgende Ticket-Aufkommen (Diagramm) und leiten Sie Verbesserungsmaßnahmen ab.
        """
        detected_subject = "it"
        detected_thema = "IT-Service & Support"
        detected_teacher = "Herr Hakim"
    elif "markt" in filename_lower or "wbl" in filename_lower:
        ocr_text = """
Klassenarbeit WBL - Marktforschung
Frau Müller
Datum: 05.12.2024

Aufgabe 1 (5 Punkte):
Was versteht man unter Primärforschung? Nennen Sie zwei Beispiele.

Aufgabe 2 (8 Punkte):
Erklären Sie den Unterschied zwischen qualitativer und quantitativer Marktforschung.
        """
        detected_subject = "wbl"
        detected_thema = "Marktforschung"
        detected_teacher = "Frau Müller"
    else:
        ocr_text = f"[OCR-Text aus Datei: {file.filename}]\n\nKlassenarbeit\n[Inhalt wird hier extrahiert...]"
        detected_subject = subject_hint or "wbl"
        detected_thema = "Allgemein"
        detected_teacher = None
    
    # Mark OCR step complete
    pipeline.steps[0].status = "completed"
    pipeline.steps[0].completedAt = datetime.utcnow().isoformat()
    pipeline.steps[0].confidence = 85
    
    # Run NLP Classification
    pipeline.currentPhase = "nlp_classifier"
    pipeline.steps[1].status = "running"
    pipeline.steps[1].startedAt = datetime.utcnow().isoformat()
    
    # Build classification prompt
    existing_subjects = list(_subjects.keys())
    existing_teachers = [t.name for t in _teachers.values()]
    
    classification_prompt = build_classification_prompt(
        ocr_text=ocr_text,
        existing_subjects=existing_subjects,
        existing_teachers=existing_teachers
    )
    
    # Call classifier model
    classifier_response = await call_ollama(
        prompt=classification_prompt,
        model=DEFAULT_CLASSIFIER_MODEL,
        temperature=0.1
    )
    
    # Parse classification result
    parsed_classification = parse_json_from_response(classifier_response)
    
    if parsed_classification:
        classification = ClassificationResult(
            subject=parsed_classification.get("subject", {"id": detected_subject, "name": detected_subject.upper(), "confidence": 70}),
            mainThema=parsed_classification.get("mainThema", {"id": detected_thema.lower().replace(" ", "-"), "name": detected_thema, "confidence": 70}),
            additionalThemas=parsed_classification.get("additionalThemas", []),
            teacher=parsed_classification.get("teacher", {"name": detected_teacher, "isNew": detected_teacher is None, "confidence": 60}),
            examDate=parsed_classification.get("examDate", {"value": None, "confidence": 50}),
            examType=parsed_classification.get("examType", {"value": "Klassenarbeit", "confidence": 80}),
            totalPoints=parsed_classification.get("totalPoints", {"value": None, "confidence": 50}),
            extractedTasks=parsed_classification.get("extractedTasks", []),
            overallConfidence=parsed_classification.get("overallConfidence", 70),
            requiresReview=parsed_classification.get("requiresReview", True),
            reviewReasons=parsed_classification.get("reviewReasons", ["Bitte überprüfe die Zuordnung"])
        )
    else:
        # Fallback classification
        classification = ClassificationResult(
            subject={"id": detected_subject, "name": _subjects.get(detected_subject, Subject(id=detected_subject, school_id="", name=detected_subject)).name, "confidence": 75},
            mainThema={"id": detected_thema.lower().replace(" ", "-").replace("&", ""), "name": detected_thema, "confidence": 70},
            additionalThemas=[],
            teacher={"name": detected_teacher, "isNew": detected_teacher not in [t.name for t in _teachers.values()], "confidence": 60},
            examDate={"value": None, "confidence": 0},
            examType={"value": "Klassenarbeit", "confidence": 80},
            totalPoints={"value": None, "confidence": 0},
            extractedTasks=[],
            overallConfidence=70,
            requiresReview=True,
            reviewReasons=["KI-Klassifikation abgeschlossen - bitte überprüfen"]
        )
    
    # Complete classification step
    pipeline.steps[1].status = "completed"
    pipeline.steps[1].completedAt = datetime.utcnow().isoformat()
    pipeline.steps[1].confidence = classification.overallConfidence
    
    pipeline.status = "waiting_review"
    pipeline.currentPhase = None
    pipeline.completedAt = datetime.utcnow().isoformat()
    
    # Create analysis result
    result = UploadAnalysisResult(
        analysisId=analysis_id,
        filename=file.filename or "unknown",
        fileHash=file_hash,
        ocrText=ocr_text,
        ocrConfidence=85,
        classification=classification,
        pipeline=pipeline,
        suggestedCorrections=[]
    )
    
    # Store for confirmation
    _pending_analyses[analysis_id] = result
    
    return result


@router.post("/upload-confirm")
async def upload_confirm(request: UploadConfirmRequest):
    """
    Stage 2: Confirm the upload analysis with optional corrections.
    This persists the ClassTest to storage.
    """
    if request.analysisId not in _pending_analyses:
        raise HTTPException(404, "Analysis not found or expired")
    
    analysis = _pending_analyses[request.analysisId]
    corrections = request.corrections
    
    # Apply corrections
    subject_id = corrections.get("subjectId", analysis.classification.subject["id"])
    main_thema_id = corrections.get("mainThemaId", analysis.classification.mainThema["id"])
    teacher_name = corrections.get("teacherName", analysis.classification.teacher.get("name"))
    
    # Create or find teacher
    teacher_id = None
    if teacher_name:
        existing_teacher = next((t for t in _teachers.values() if t.name == teacher_name), None)
        if existing_teacher:
            teacher_id = existing_teacher.id
            existing_teacher.tests_count += 1
        else:
            # Create new teacher
            teacher_id = gen_id("teacher-")
            new_teacher = Teacher(
                id=teacher_id,
                name=teacher_name,
                subject_ids=[subject_id],
                tests_count=1
            )
            _teachers[teacher_id] = new_teacher
    
    # Ensure thema exists
    if main_thema_id not in _themas:
        thema_name = corrections.get("mainThemaName", analysis.classification.mainThema["name"])
        _themas[main_thema_id] = Thema(
            id=main_thema_id,
            subject_id=subject_id,
            name=thema_name
        )
    
    # Create ClassTest
    class_test_id = gen_id("test-")
    class_test = {
        "id": class_test_id,
        "filename": analysis.filename,
        "file_hash": analysis.fileHash,
        "subject_id": subject_id,
        "thema_id": main_thema_id,
        "teacher_id": teacher_id,
        "teacher_name": teacher_name,
        "ocr_text": analysis.ocrText,
        "extracted_tasks": [t for i, t in enumerate(analysis.classification.extractedTasks) if i + 1 not in request.skipTasks],
        "exam_date": corrections.get("examDate", analysis.classification.examDate.get("value")),
        "exam_type": corrections.get("examType", analysis.classification.examType.get("value")),
        "total_points": corrections.get("totalPoints", analysis.classification.totalPoints.get("value")),
        "status": "confirmed",
        "uploaded_at": datetime.utcnow().isoformat(),
        "confirmed_at": datetime.utcnow().isoformat(),
    }
    
    _class_tests[class_test_id] = class_test
    
    # Cleanup pending analysis
    del _pending_analyses[request.analysisId]
    
    return {
        "success": True,
        "classTestId": class_test_id,
        "subjectId": subject_id,
        "themaId": main_thema_id,
        "teacherId": teacher_id,
    }


# ============================================================================
# Exam Generation Pipeline Endpoints
# ============================================================================

@router.post("/generate-exam")
async def generate_exam(request: ExamGenerationRequest) -> Dict[str, Any]:
    """
    Generate a mock exam using AI.
    Supports free-text prompts, context from class tests, and teacher patterns.
    """
    # Validate subject
    if request.subjectId not in _subjects:
        raise HTTPException(404, f"Subject not found: {request.subjectId}")
    
    subject = _subjects[request.subjectId]
    
    # Get thema names
    thema_names = []
    for tid in request.themaIds:
        if tid in _themas:
            thema_names.append(_themas[tid].name)
        else:
            thema_names.append(tid)  # Use ID as name if not found
    
    if not thema_names:
        thema_names = ["Allgemein"]
    
    # Get teacher pattern if requested
    teacher_pattern = None
    if request.useTeacherPattern and request.teacherId:
        teacher = _teachers.get(request.teacherId)
        if teacher and teacher.exam_pattern_profile:
            teacher_pattern = teacher.exam_pattern_profile
    
    # Gather context texts
    context_texts = []
    if request.contextClassTestIds:
        for test_id in request.contextClassTestIds:
            if test_id in _class_tests:
                context_texts.append(_class_tests[test_id].get("ocr_text", ""))
    
    if request.contextText:
        context_texts.append(request.contextText)
    
    # Build generation prompt
    generation_prompt = build_exam_generation_prompt(
        subject_name=subject.full_name or subject.name,
        thema_names=thema_names,
        difficulty=request.difficulty,
        task_count=request.taskCount,
        duration_minutes=request.durationMinutes,
        teacher_pattern=teacher_pattern,
        free_prompt=request.freePrompt,
        context_texts=context_texts if context_texts else None
    )
    
    # Call generator model
    generator_response = await call_ollama(
        prompt=generation_prompt,
        model=DEFAULT_GENERATOR_MODEL,
        temperature=0.7
    )
    
    # Parse response
    parsed_exam = parse_json_from_response(generator_response)
    
    exam_id = gen_id("exam-")
    
    if parsed_exam and "tasks" in parsed_exam:
        # Process AI-generated tasks
        tasks = []
        for i, t in enumerate(parsed_exam.get("tasks", [])[:request.taskCount]):
            task = Task(
                id=gen_id("task-"),
                type=t.get("type", "ShortAnswer"),
                taskNumber=i + 1,
                questionText=t.get("questionText", f"Aufgabe {i+1}"),
                questionImage=t.get("questionImage"),
                options=t.get("options"),
                correctAnswer=t.get("correctAnswer"),
                modelAnswer=t.get("modelAnswer"),
                calculationData=t.get("calculationData"),
                diagramSpec=t.get("diagramSpec"),
                matchingPairs=t.get("matchingPairs"),
                blanks=t.get("blanks"),
                points=t.get("points", 5),
                difficulty=t.get("difficulty", request.difficulty),
                timeEstimateMinutes=t.get("timeEstimateMinutes", 5),
                gradingRubric=t.get("gradingRubric", {
                    "maxPoints": t.get("points", 5),
                    "criteria": [{"name": "Vollständigkeit", "description": "Alle Aspekte behandelt", "maxPoints": t.get("points", 5)}],
                    "autoGradable": t.get("type", "").startswith("MC"),
                    "partialCreditAllowed": True
                }),
                generatedBy="ai",
                confidence=85
            )
            tasks.append(task)
        
        total_points = sum(t.points for t in tasks)
        
        mock_exam = MockExam(
            id=exam_id,
            subjectId=request.subjectId,
            themaIds=request.themaIds,
            title=parsed_exam.get("title", f"Übungsklausur: {', '.join(thema_names)}"),
            description=parsed_exam.get("description", "KI-generierte Übungsklausur"),
            tasks=tasks,
            totalPoints=total_points,
            estimatedDurationMinutes=request.durationMinutes,
            difficultyLevel=request.difficulty,
            teacherPatternUsed=request.teacherId if request.useTeacherPattern else None,
            freePrompt=request.freePrompt,
            contextUsed=request.contextClassTestIds
        )
    else:
        # Fallback: Generate sample exam
        mock_exam = _generate_fallback_exam(
            exam_id=exam_id,
            subject_id=request.subjectId,
            thema_ids=request.themaIds,
            thema_names=thema_names,
            difficulty=request.difficulty,
            task_count=request.taskCount,
            duration=request.durationMinutes,
            include_diagrams=request.includeDiagrams
        )
    
    _mock_exams[exam_id] = mock_exam
    
    return {
        "success": True,
        "mockExam": mock_exam.dict(),
        "generatedBy": "ai" if parsed_exam else "fallback",
        "pipeline": {
            "status": "completed",
            "modelUsed": DEFAULT_GENERATOR_MODEL
        }
    }


def _generate_fallback_exam(
    exam_id: str,
    subject_id: str,
    thema_ids: List[str],
    thema_names: List[str],
    difficulty: int,
    task_count: int,
    duration: int,
    include_diagrams: bool
) -> MockExam:
    """Generate a fallback exam when AI fails."""
    tasks = []
    
    # Task templates by type
    task_templates = [
        {
            "type": "MC_SingleChoice",
            "questionText": "Welche Aussage ist korrekt?",
            "options": [
                {"id": "A", "text": "Option A", "isCorrect": False},
                {"id": "B", "text": "Option B (richtig)", "isCorrect": True},
                {"id": "C", "text": "Option C", "isCorrect": False},
                {"id": "D", "text": "Option D", "isCorrect": False},
            ],
            "correctAnswer": "B",
            "points": 2,
        },
        {
            "type": "ShortAnswer",
            "questionText": "Erklären Sie den Begriff und nennen Sie zwei Beispiele.",
            "modelAnswer": "Der Begriff beschreibt... Beispiele sind...",
            "points": 5,
        },
        {
            "type": "CaseStudy",
            "questionText": "Analysieren Sie die folgende Situation und entwickeln Sie einen Lösungsvorschlag.",
            "modelAnswer": "Die Situation zeigt... Eine Lösung wäre...",
            "points": 10,
        },
        {
            "type": "Calculation",
            "questionText": "Berechnen Sie das Ergebnis basierend auf den gegebenen Daten.",
            "calculationData": {
                "formula": "Ergebnis = Wert1 * Wert2 / 100",
                "variables": {"Wert1": 1500, "Wert2": 8},
                "expectedResult": 120,
                "tolerance": 0.5
            },
            "points": 5,
        },
    ]
    
    if include_diagrams:
        task_templates.append({
            "type": "DiagramAnalysis",
            "questionText": "Analysieren Sie das Diagramm und leiten Sie Handlungsempfehlungen ab.",
            "diagramSpec": {
                "type": "bar",
                "title": "Umsatzentwicklung nach Quartal",
                "data": [
                    {"label": "Q1", "value": 150000},
                    {"label": "Q2", "value": 180000},
                    {"label": "Q3", "value": 165000},
                    {"label": "Q4", "value": 210000},
                ],
                "xLabel": "Quartal",
                "yLabel": "Umsatz (€)"
            },
            "points": 8,
        })
    
    for i in range(task_count):
        template = task_templates[i % len(task_templates)]
        task = Task(
            id=gen_id("task-"),
            type=template["type"],
            taskNumber=i + 1,
            questionText=template["questionText"],
            options=template.get("options"),
            correctAnswer=template.get("correctAnswer"),
            modelAnswer=template.get("modelAnswer"),
            calculationData=template.get("calculationData"),
            diagramSpec=template.get("diagramSpec"),
            points=template["points"],
            difficulty=min(5, max(1, difficulty + (i % 3) - 1)),
            timeEstimateMinutes=template["points"],
            gradingRubric={
                "maxPoints": template["points"],
                "criteria": [{"name": "Vollständigkeit", "maxPoints": template["points"]}],
                "autoGradable": template["type"].startswith("MC"),
                "partialCreditAllowed": True
            },
            generatedBy="fallback",
            confidence=60
        )
        tasks.append(task)
    
    return MockExam(
        id=exam_id,
        subjectId=subject_id,
        themaIds=thema_ids,
        title=f"Übungsklausur: {', '.join(thema_names)}",
        description="Automatisch generierte Übungsklausur",
        tasks=tasks,
        totalPoints=sum(t.points for t in tasks),
        estimatedDurationMinutes=duration,
        difficultyLevel=difficulty,
        status="ready"
    )


# ============================================================================
# Grading Pipeline Endpoints
# ============================================================================

@router.post("/attempts/{attempt_id}/finish")
async def finish_attempt(attempt_id: str, force_grade: bool = Query(False)):
    """
    Finish an attempt and grade using AI.
    Returns detailed scores with confidence levels.
    """
    if attempt_id not in _attempts:
        raise HTTPException(404, "Attempt not found")
    
    attempt = _attempts[attempt_id]
    exam = _mock_exams.get(attempt.mockExamId)
    
    if not exam:
        raise HTTPException(404, "Mock exam not found")
    
    # Prepare tasks with answers for grading
    tasks_with_answers = []
    for task in exam.tasks:
        response = next(
            (r for r in attempt.taskResponses if r.get("taskId") == task.id),
            None
        )
        
        tasks_with_answers.append({
            "taskId": task.id,
            "taskNumber": task.taskNumber,
            "type": task.type,
            "questionText": task.questionText,
            "points": task.points,
            "correctAnswer": task.correctAnswer,
            "modelAnswer": task.modelAnswer,
            "gradingRubric": task.gradingRubric,
            "userAnswer": response.get("userAnswer") if response else None
        })
    
    # Build grading prompt
    grading_prompt = build_grading_prompt(tasks_with_answers)
    
    # Call grader model
    grader_response = await call_ollama(
        prompt=grading_prompt,
        model=DEFAULT_GRADER_MODEL,
        temperature=0.2
    )
    
    # Parse grading result
    parsed_grading = parse_json_from_response(grader_response)
    
    task_grades = []
    total_earned = 0
    tasks_needing_review = []
    
    if parsed_grading and "taskResults" in parsed_grading:
        for tr in parsed_grading["taskResults"]:
            grade = TaskGrade(
                taskId=tr.get("taskId", ""),
                earnedPoints=tr.get("earnedPoints", 0),
                maxPoints=tr.get("maxPoints", 0),
                percentage=tr.get("percentage", 0),
                isCorrect=tr.get("isCorrect", False),
                isPartiallyCorrect=tr.get("isPartiallyCorrect", False),
                autoGraded=tr.get("autoGraded", False),
                confidence=tr.get("confidence", 50),
                feedback=tr.get("feedback", ""),
                feedbackType=tr.get("feedbackType", "needs_review"),
                detailedFeedback=tr.get("detailedFeedback"),
                criteriaScores=tr.get("criteriaScores"),
                needsManualReview=tr.get("needsManualReview", False),
                reviewReason=tr.get("reviewReason")
            )
            task_grades.append(grade)
            total_earned += grade.earnedPoints
            
            if grade.needsManualReview or grade.confidence < 60:
                tasks_needing_review.append(grade.taskId)
        
        percentage = parsed_grading.get("percentage", (total_earned / exam.totalPoints) * 100 if exam.totalPoints > 0 else 0)
        grade_num = parsed_grading.get("grade", calculate_grade(percentage)[0])
        grade_text = parsed_grading.get("gradeText", calculate_grade(percentage)[1])
        overall_feedback = parsed_grading.get("overallFeedback", "")
        strengths = parsed_grading.get("strengths", [])
        areas = parsed_grading.get("areasForImprovement", [])
        overall_confidence = parsed_grading.get("overallConfidence", 70)
    else:
        # Fallback grading
        for task in exam.tasks:
            response = next(
                (r for r in attempt.taskResponses if r.get("taskId") == task.id),
                None
            )
            
            user_answer = response.get("userAnswer") if response else None
            
            # Simple auto-grading for MC
            if task.type.startswith("MC") and task.correctAnswer:
                is_correct = user_answer == task.correctAnswer
                earned = task.points if is_correct else 0
                confidence = 100
                feedback = "Richtig!" if is_correct else f"Falsch. Richtige Antwort: {task.correctAnswer}"
            else:
                # Give partial credit for non-empty answers
                earned = task.points * 0.5 if user_answer else 0
                confidence = 50
                feedback = "Antwort wird überprüft."
                tasks_needing_review.append(task.id)
            
            grade = TaskGrade(
                taskId=task.id,
                earnedPoints=earned,
                maxPoints=task.points,
                percentage=(earned / task.points) * 100 if task.points > 0 else 0,
                isCorrect=earned == task.points,
                isPartiallyCorrect=0 < earned < task.points,
                autoGraded=task.type.startswith("MC"),
                confidence=confidence,
                feedback=feedback,
                feedbackType="correct" if earned == task.points else ("partial" if earned > 0 else "incorrect"),
                needsManualReview=confidence < 60
            )
            task_grades.append(grade)
            total_earned += earned
        
        percentage = (total_earned / exam.totalPoints) * 100 if exam.totalPoints > 0 else 0
        grade_num, grade_text = calculate_grade(percentage)
        overall_feedback = f"Du hast {total_earned:.0f} von {exam.totalPoints} Punkten erreicht."
        strengths = []
        areas = []
        overall_confidence = 60
    
    # Update attempt
    attempt.finishedAt = datetime.utcnow().isoformat()
    attempt.taskGrades = task_grades
    attempt.totalScore = total_earned
    attempt.totalPoints = exam.totalPoints
    attempt.percentage = percentage
    attempt.grade = grade_num
    attempt.gradeText = grade_text
    attempt.status = "graded"
    attempt.overallFeedback = overall_feedback
    attempt.strengths = strengths
    attempt.areasForImprovement = areas
    attempt.overallConfidence = overall_confidence
    attempt.tasksNeedingReview = tasks_needing_review
    
    return {
        "attemptId": attempt_id,
        "totalScore": total_earned,
        "totalPoints": exam.totalPoints,
        "percentage": percentage,
        "grade": grade_num,
        "gradeText": grade_text,
        "taskGrades": [g.dict() for g in task_grades],
        "overallFeedback": overall_feedback,
        "strengths": strengths,
        "areasForImprovement": areas,
        "overallConfidence": overall_confidence,
        "tasksNeedingReview": tasks_needing_review,
        "pipeline": {
            "status": "completed",
            "modelUsed": DEFAULT_GRADER_MODEL
        }
    }


# ============================================================================
# Standard CRUD Endpoints
# ============================================================================

@router.get("/schools")
async def list_schools():
    return {"schools": list(_schools.values())}

@router.get("/schools/{school_id}")
async def get_school(school_id: str):
    if school_id not in _schools:
        raise HTTPException(404, "School not found")
    return _schools[school_id]

@router.get("/subjects")
async def list_subjects(school_id: Optional[str] = None):
    subjects = list(_subjects.values())
    if school_id:
        subjects = [s for s in subjects if s.school_id == school_id]
    return {"subjects": subjects}

@router.get("/subjects/{subject_id}")
async def get_subject(subject_id: str):
    if subject_id not in _subjects:
        raise HTTPException(404, "Subject not found")
    return _subjects[subject_id]

@router.get("/themas")
async def list_themas(subject_id: Optional[str] = None):
    themas = list(_themas.values())
    if subject_id:
        themas = [t for t in themas if t.subject_id == subject_id]
    return {"themas": themas}

@router.post("/themas")
async def create_thema(subject_id: str, name: str):
    thema_id = gen_id("thema-")
    thema = Thema(id=thema_id, subject_id=subject_id, name=name)
    _themas[thema_id] = thema
    return thema

@router.get("/teachers")
async def list_teachers(subject_id: Optional[str] = None):
    teachers = list(_teachers.values())
    if subject_id:
        teachers = [t for t in teachers if subject_id in t.subject_ids]
    return {"teachers": teachers}

@router.get("/teachers/{teacher_id}")
async def get_teacher(teacher_id: str):
    if teacher_id not in _teachers:
        raise HTTPException(404, "Teacher not found")
    return _teachers[teacher_id]

@router.get("/teachers/{teacher_id}/pattern")
async def get_teacher_pattern(teacher_id: str):
    if teacher_id not in _teachers:
        raise HTTPException(404, "Teacher not found")
    teacher = _teachers[teacher_id]
    
    if not teacher.exam_pattern_profile:
        return {
            "teacherId": teacher_id,
            "patternAvailable": False,
            "testsNeeded": max(0, 3 - teacher.tests_count),
            "message": "Lade mindestens 3 Tests hoch, um das Muster dieses Lehrers zu lernen."
        }
    
    return {
        "teacherId": teacher_id,
        "patternAvailable": True,
        "pattern": teacher.exam_pattern_profile
    }

@router.get("/class-tests")
async def list_class_tests(subject_id: Optional[str] = None, teacher_id: Optional[str] = None):
    tests = list(_class_tests.values())
    if subject_id:
        tests = [t for t in tests if t.get("subject_id") == subject_id]
    if teacher_id:
        tests = [t for t in tests if t.get("teacher_id") == teacher_id]
    return {"classTests": tests}

@router.get("/mock-exams")
async def list_mock_exams(subject_id: Optional[str] = None, thema_id: Optional[str] = None):
    exams = list(_mock_exams.values())
    if subject_id:
        exams = [e for e in exams if e.subjectId == subject_id]
    if thema_id:
        exams = [e for e in exams if thema_id in e.themaIds]
    return {"mockExams": [e.dict() for e in exams]}

@router.get("/mock-exams/{exam_id}")
async def get_mock_exam(exam_id: str):
    if exam_id not in _mock_exams:
        raise HTTPException(404, "Mock exam not found")
    return _mock_exams[exam_id].dict()

@router.post("/start-attempt/{exam_id}")
async def start_attempt(exam_id: str):
    if exam_id not in _mock_exams:
        raise HTTPException(404, "Mock exam not found")
    
    exam = _mock_exams[exam_id]
    attempt_id = gen_id("attempt-")
    
    attempt = Attempt(
        id=attempt_id,
        mockExamId=exam_id,
        startedAt=datetime.utcnow().isoformat(),
        totalPoints=exam.totalPoints
    )
    
    _attempts[attempt_id] = attempt
    return attempt.dict()

@router.post("/attempts/{attempt_id}/submit-answer")
async def submit_answer(attempt_id: str, request: SubmitAnswerRequest):
    if attempt_id not in _attempts:
        raise HTTPException(404, "Attempt not found")
    
    attempt = _attempts[attempt_id]
    
    # Update or add response
    existing = next((r for r in attempt.taskResponses if r.get("taskId") == request.taskId), None)
    new_response = {
        "taskId": request.taskId,
        "userAnswer": request.userAnswer,
        "answeredAt": datetime.utcnow().isoformat()
    }
    
    if existing:
        attempt.taskResponses = [
            r if r.get("taskId") != request.taskId else new_response
            for r in attempt.taskResponses
        ]
    else:
        attempt.taskResponses.append(new_response)
    
    return {"success": True}

@router.get("/attempts")
async def list_attempts(mock_exam_id: Optional[str] = None):
    attempts = list(_attempts.values())
    if mock_exam_id:
        attempts = [a for a in attempts if a.mockExamId == mock_exam_id]
    return {"attempts": [a.dict() for a in attempts]}

@router.get("/attempts/{attempt_id}")
async def get_attempt(attempt_id: str):
    if attempt_id not in _attempts:
        raise HTTPException(404, "Attempt not found")
    return _attempts[attempt_id].dict()

@router.get("/statistics")
async def get_statistics(subject_id: Optional[str] = None, thema_id: Optional[str] = None):
    completed = [a for a in _attempts.values() if a.status == "graded"]
    
    if subject_id:
        completed = [
            a for a in completed
            if _mock_exams.get(a.mockExamId) and _mock_exams[a.mockExamId].subjectId == subject_id
        ]
    
    if thema_id:
        completed = [
            a for a in completed
            if _mock_exams.get(a.mockExamId) and thema_id in _mock_exams[a.mockExamId].themaIds
        ]
    
    if not completed:
        return {
            "totalAttempts": 0,
            "averageGrade": 0,
            "averagePercentage": 0,
            "bestGrade": 0,
            "totalStudyTimeMinutes": 0
        }
    
    return {
        "totalAttempts": len(completed),
        "averageGrade": sum(a.grade for a in completed) / len(completed),
        "averagePercentage": sum(a.percentage for a in completed) / len(completed),
        "bestGrade": min(a.grade for a in completed),
        "totalStudyTimeMinutes": sum(a.durationSeconds for a in completed) // 60,
        "progressOverTime": [
            {"date": a.finishedAt, "grade": a.grade, "percentage": a.percentage}
            for a in sorted(completed, key=lambda x: x.finishedAt or "")
        ]
    }
