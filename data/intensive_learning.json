{
  "timestamp": "2025-12-10T21:56:56.184067",
  "repos_scanned": [
    "SelfImprovingAgent",
    "gpt-pilot",
    "plandex",
    "aider",
    "anthropic-cookbook",
    "devika",
    "RepairAgent",
    "swarm",
    "playwright-python",
    "browser-use",
    "crewAI",
    "LaVague",
    "langchain",
    "self_improving_coding_agent",
    "gpt-engineer",
    "openhands-ai",
    "build-your-claude-code-from-scratch",
    "dgm",
    "Aider Repo",
    "autogen"
  ],
  "patterns_found": {
    "file_discovery": [
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/agents/orchestrator.py",
        "keyword": "tree",
        "snippet": "            base_path = project_root.split(\"/pythagora-core\")[0] + \"/pythagora-core\"\n            template_path = os.path.join(\n                base_path, \"core\", \"templates\", \"tree\", \"vite_react\", \"client\", \"vite.config.ts\"\n            )\n\n            # Read the template file\n            with open(template_path, \"r\", encoding=\"utf-8\") as file:\n                template_content = file.read()\n"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/templates/base.py",
        "keyword": "tree",
        "snippet": "        self.process_manager = process_manager\n\n        self.file_renderer = Renderer(join(dirname(__file__), \"tree\"))\n        self.info_renderer = Renderer(join(dirname(__file__), \"info\"))\n\n    def filter(self, path: str) -> Optional[str]:\n        \"\"\"\n        Filter a file path to be included in the rendered template.\n\n        The method is called for every file in the template tree before rendering.\n        If the method returns None or an empty string, the file will be skipped.\n        Otherw"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/cli/helpers.py",
        "keyword": "ast",
        "snippet": "    Calculate file changes between initial and final versions, similar to a Git pull request.\n\n    This function tracks the initial (first seen) and final (last seen) versions of each file,\n    and calculates the diff between those two versions, ignoring intermediate changes.\n\n    :param convo_entries: List of conversation entries containing file changes\n    :return: List of file changes with initial and final versions\n    \"\"\"\n--\n    Load Pythagora JSON configuration file and apply command-line "
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/cli/main.py",
        "keyword": "ast",
        "snippet": "        # SPECIFICATION\n        fe_states = await sm.get_fe_states(limit=10)\n        be_back_logs, last_task_in_db = await sm.get_be_back_logs()\n\n        if sm.current_state.specification:\n            if not sm.current_state.specification.original_description:\n                spec = sm.current_state.specification\n                spec.description = project_state.epics[0][\"description\"]\n--\n                ]\n            )\n            if not fe_states and be_back_logs and not last_task_in_db:\n      "
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/cli/main.py",
        "keyword": "glob",
        "snippet": "\nasync def cleanup(ui: UIBase):\n    global telemetry_sent\n    if not telemetry_sent:\n        await telemetry.send()\n        telemetry_sent = True\n    await ui.stop()\n\n--\n    :return: True if the application ran successfully, False otherwise.\n    \"\"\"\n    global telemetry_sent\n\n    if args.list:\n        await list_projects_branches_states(db)\n        return True\n    elif args.list_json:\n"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/config/user_settings.py",
        "keyword": "glob",
        "snippet": "def resolve_config_dir() -> Path:\n    \"\"\"\n    Figure out where to store the global config file(s).\n\n    :return: path to the desired location config directory\n\n    See the UserSettings docstring for details on how the config directory is\n    determined.\n--\nclass UserSettings(BaseModel):\n    \"\"\"\n    This object holds all the global user settings, that are applicable for\n    all Pythagora/GPT-Pilot installations.\n\n    The use settings are stored in a JSON file in the config directory.\n\n    The con"
      },
      {
        "repo": "plandex",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/plandex/app/server/litellm_proxy.py",
        "keyword": "ast",
        "snippet": "AnthropicModelInfo.get_anthropic_headers = _oauth_get_hdrs\n\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import StreamingResponse, JSONResponse\nfrom litellm import completion, _turn_on_debug\nimport json\nimport re\n\n# _turn_on_debug()\n--\nprint(\"Litellm proxy: starting proxy server on port 4000...\")\n\napp = FastAPI()\n\n@app.get(\"/health\")\nasync def health():\n  return {\"status\": \"ok\"}\n\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/aider/commands.py",
        "keyword": "repo_map",
        "snippet": "        # repo map\n        other_files = set(self.coder.get_all_abs_files()) - set(self.coder.abs_fnames)\n        if self.coder.repo_map:\n            repo_content = self.coder.repo_map.get_repo_map(self.coder.abs_fnames, other_files)\n            if repo_content:\n                tokens = self.coder.main_model.token_count(repo_content)\n                res.append((tokens, \"repository map\", \"use --map-tokens to resize\"))\n\n        fence = \"`\" * 3\n--\n        coder.run(user_msg, preproc=False)\n\n       "
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/aider/models.py",
        "keyword": "repo_map",
        "snippet": "    edit_format: str = \"whole\"\n    weak_model_name: Optional[str] = None\n    use_repo_map: bool = False\n    send_undo_reply: bool = False\n    lazy: bool = False\n    overeager: bool = False\n    reminder: str = \"user\"\n    examples_as_sys_msg: bool = False\n--\n        if \"/o3-mini\" in model:\n            self.edit_format = \"diff\"\n            self.use_repo_map = True\n            self.use_temperature = False\n            self.system_prompt_prefix = \"Formatting re-enabled. \"\n            self.system_promp"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/aider/special.py",
        "keyword": "codebase",
        "snippet": "def filter_important_files(file_paths):\n    \"\"\"\n    Filter a list of file paths to return only those that are commonly important in codebases.\n\n    :param file_paths: List of file paths to check\n    :return: List of file paths that match important file patterns\n    \"\"\"\n    return list(filter(is_important, file_paths))\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/scripts/tsl_pack_langs.py",
        "keyword": "tree",
        "snippet": "def main():\n    # Path to the language definitions file\n    lang_def_path = \"../../tmp/tree-sitter-language-pack/sources/language_definitions.json\"\n\n    # Path to store the tags.scm files\n    output_dir = \"aider/queries/tree-sitter-language-pack\"\n\n    # Create the output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Common branch names to try if API fails and config branch doesn't work\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/scripts/blame.py",
        "keyword": "tree",
        "snippet": "\n    revision = end_tag if end_tag else \"HEAD\"\n    files = run([\"git\", \"ls-tree\", \"-r\", \"--name-only\", revision]).strip().split(\"\\n\")\n    test_files = [f for f in files if f.startswith(\"tests/fixtures/languages/\") and \"/test.\" in f]\n    files = [\n        f\n        for f in files\n        if f.endswith((\".js\", \".py\", \".scm\", \".sh\", \"Dockerfile\", \"Gemfile\"))\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/scripts/clean_metadata.py",
        "keyword": "ast",
        "snippet": "            # Generate unified diff\n            # If dictionaries differ, generate JSON strings to show the diff\n            # Add a dummy key to ensure the *real* last key gets a comma\n            litellm_entry_copy = litellm_entry.copy()\n            aider_entry_copy = aider_entry.copy()\n            dummy_key = \"zzzdummykey\"\n            litellm_entry_copy[dummy_key] = True\n            aider_entry_copy[dummy_key] = True\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/scripts/redact-cast.py",
        "keyword": "ast",
        "snippet": "def main():\n    if len(sys.argv) != 3:\n        print(f\"Usage: {sys.argv[0]} input_cast_file output_cast_file\")\n        sys.exit(1)\n\n    input_file = sys.argv[1]\n    output_file = sys.argv[2]\n\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/aider/linter.py",
        "keyword": "find_file",
        "snippet": "\n        linenums = []\n        filenames_linenums = find_filenames_and_linenums(errors, [rel_fname])\n        if filenames_linenums:\n            filename, linenums = next(iter(filenames_linenums.items()))\n            linenums = [num - 1 for num in linenums]\n\n        return LintResult(text=errors, lines=linenums)\n--\n\n\ndef find_filenames_and_linenums(text, fnames):\n    \"\"\"\n    Search text for all occurrences of <filename>:\\\\d+ and make a list of them\n    where <filename> is one of the filenames in "
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/tests/basic/test_editblock.py",
        "keyword": "find_file",
        "snippet": "        self.GPT35 = Model(\"gpt-3.5-turbo\")\n\n    def test_find_filename(self):\n        fence = (\"```\", \"```\")\n        valid_fnames = [\"file1.py\", \"file2.py\", \"dir/file3.py\", r\"\\windows\\__init__.py\"]\n\n        # Test with filename on a single line\n        lines = [\"file1.py\", \"```\"]\n        self.assertEqual(eb.find_filename(lines, fence, valid_fnames), \"file1.py\")\n\n        # Test with filename in fence\n        lines = [\"```python\", \"file3.py\", \"```\"]\n        self.assertEqual(eb.find_filename(lines"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/scripts/recording_audio.py",
        "keyword": "glob",
        "snippet": "    args = parser.parse_args()\n\n    # Use args.voice directly instead of modifying global VOICE\n    selected_voice = args.voice\n    selected_bitrate = args.bitrate\n\n    # Check if FFmpeg is available for compression\n    if not check_ffmpeg() and not args.dry_run:\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/benchmark/problem_stats.py",
        "keyword": "glob",
        "snippet": "\n    # Look in language subdirectories under exercises/practice\n    for fname in benchmark_dir.glob(\"*/exercises/practice/*/.aider.results.json\"):\n        error = False\n        try:\n            results = json.loads(fname.read_text())\n            error = \"testcase\" not in results\n            if not error:\n--\n        # Copy each hard set problem's directory\n        copied_by_lang = defaultdict(int)\n        for lang_dir in src_dir.glob(\"*/exercises/practice\"):\n            if not lang_dir.is_dir():\n"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/tool_use/utils/visualize.py",
        "keyword": "tree",
        "snippet": "from rich.syntax import Syntax\nfrom rich.text import Text\nfrom rich.tree import Tree\n\n\nclass ParsedContent:\n    \"\"\"Represents a parsed content block from a Claude message.\"\"\"\n\n--\n\n\ndef render_text_content(content: ParsedContent, tree: Tree) -> None:\n    \"\"\"Render a text content block.\"\"\"\n    text = content.data.get(\"text\", \"\")\n    if text:\n        # Truncate very long text\n        if len(text) > 1000:\n            text = text[:1000] + \"\\n... (truncated)\"\n        text_node = tree.add(\"[cyan]Text[/"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/capabilities/summarization/data/multiple_subleases.py",
        "keyword": "tree",
        "snippet": "THIS COMMERCIAL SUBLEASE AGREEMENT (hereinafter referred to as the \"Sublease\") is made and entered into on this 1st day of September, 2023 (the \"Effective Date\"), by and between:\n\nSUBLESSOR: TechHub Enterprises, LLC, a California limited liability company with its principal place of business at 5678 Market Street, Suite 3000, San Francisco, CA 94103 (hereinafter referred to as the \"Sublessor\")\n\nAND\n\nSUBLESSEE: NanoSphere Solutions, Inc., a Delaware corporation with its principal place of busines"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/scripts/validate_all_notebooks.py",
        "keyword": "ast",
        "snippet": "        return {\n            \"version\": \"1.0\",\n            \"last_full_run\": None,\n            \"notebooks\": {},\n            \"history\": [],\n            \"ignored\": {},\n        }\n\n--\n            self.state[\"history\"].append({\"date\": today, \"passing\": passing, \"total\": total})\n\n        # Keep only last 30 days of history\n        self.state[\"history\"] = self.state[\"history\"][-30:]\n\n        with open(self.state_file, \"w\") as f:\n            json.dump(self.state, f, indent=2, default=str)\n\n    def valida"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/tool_use/utils/team_expense_api.py",
        "keyword": "ast",
        "snippet": "        (\"meals\", \"Client dinner\", 50, 250),\n        (\"meals\", \"Team lunch\", 20, 100),\n        (\"meals\", \"Conference breakfast\", 15, 40),\n        (\"meals\", \"Coffee meeting\", 5, 25),\n        (\"software\", \"SaaS subscription\", 10, 200),\n        (\"software\", \"API credits\", 50, 500),\n        (\"equipment\", \"Monitor\", 200, 800),\n        (\"equipment\", \"Keyboard\", 50, 200),\n--\n        \"conference\": [\"EventBrite\", \"WWDC\", \"AWS re:Invent\", \"Google I/O\", \"ReactConf\"],\n        \"office\": [\"Staples\", \"Office D"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/scripts/validate_all_notebooks.py",
        "keyword": "glob",
        "snippet": "    def run_validation(self, mode=\"quick\", pattern=\"**/*.ipynb\"):\n        \"\"\"Run validation on all notebooks.\"\"\"\n        notebooks = list(Path(\".\").glob(pattern))\n        notebooks = [n for n in notebooks if \".ipynb_checkpoints\" not in str(n)]\n\n        if not notebooks:\n            print(f\"No notebooks found matching pattern: {pattern}\")\n            return\n--\n    def run_progressive_validation(self):\n        \"\"\"Run validation in batches with user control.\"\"\"\n        notebooks = list(Path(\".\").gl"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/claude_agent_sdk/utils/agent_visualizer.py",
        "keyword": "glob",
        "snippet": "\n# Track subagent state for activity display\n# WARNING: This global state is NOT thread-safe. If using this module in concurrent\n# scenarios (e.g., multiple asyncio tasks processing different conversations simultaneously),\n# each task should call reset_activity_context() before starting and be aware that\n# interleaved operations may produce incorrect subagent tracking. For thread-safe usage,\n# consider passing context explicitly or using contextvars.\n_subagent_context: dict[str, Any] = {\n--\n    "
      },
      {
        "repo": "devika",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/devika/src/browser/interaction.py",
        "keyword": "tree",
        "snippet": "\t\t# Removed unused percentage_progress variables\n\n\t\ttree = self.client.send(\n\t\t\t\"DOMSnapshot.captureSnapshot\",\n\t\t\t{\"computedStyles\": [], \"includeDOMRects\": True, \"includePaintOrder\": True},\n\t\t)\n\t\tstrings\t \t= tree[\"strings\"]\n\t\tdocument \t= tree[\"documents\"][0]\n\t\tnodes \t\t= document[\"nodes\"]\n\t\tbackend_node_id = nodes[\"backendNodeId\"]\n\t\tattributes \t= nodes[\"attributes\"]\n\t\tnode_value \t= nodes[\"nodeValue\"]\n\t\tparent \t\t= nodes[\"parentIndex\"]\n--\n\t\t\treturn values\n\n\t\tdef add_to_hash_tree(hash_tree, tag, nod"
      },
      {
        "repo": "devika",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/devika/devika.py",
        "keyword": "ast",
        "snippet": "        else:\n            emit_agent(\"info\", {\"type\": \"warning\", \"message\": \"previous agent doesn't completed it's task.\"})\n            last_state = AgentState.get_latest_state(project_name)\n            if last_state[\"agent_is_active\"] or not last_state[\"completed\"]:\n                thread = Thread(target=lambda: agent.execute(message, project_name))\n                thread.start()\n            else:\n                thread = Thread(target=lambda: agent.subsequent_execute(message, project_name))\n  "
      },
      {
        "repo": "devika",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/devika/src/logger.py",
        "keyword": "ast",
        "snippet": "from functools import wraps\n\nfrom fastlogging import LogInit\nfrom flask import request\n\nfrom src.config import Config\n\n\n"
      },
      {
        "repo": "devika",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/devika/src/llm/llm.py",
        "keyword": "glob",
        "snippet": "\n    @staticmethod\n    def update_global_token_usage(string: str, project_name: str):\n        token_usage = len(TIKTOKEN_ENC.encode(string))\n        agentState.update_token_usage(project_name, token_usage)\n\n        total = agentState.get_latest_token_usage(project_name) + token_usage\n        emit_agent(\"tokens\", {\"token_usage\": total})\n\n    def inference(self, prompt: str, project_name: str) -> str:\n        self.update_global_token_usage(prompt, project_name)\n\n        model_enum, model_name = se"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/JavaListener.py",
        "keyword": "tree",
        "snippet": "    from JavaParser import JavaParser\n\n# This class defines a complete listener for a parse tree produced by JavaParser.\nclass JavaListener(ParseTreeListener):\n\n    # Enter a parse tree produced by JavaParser#compilationUnit.\n    def enterCompilationUnit(self, ctx:JavaParser.CompilationUnitContext):\n        pass\n\n    # Exit a parse tree produced by JavaParser#compilationUnit.\n    def exitCompilationUnit(self, ctx:JavaParser.CompilationUnitContext):\n        pass\n\n\n    # Enter a parse tree produce"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/java_antlr_test.py",
        "keyword": "tree",
        "snippet": "    parser = JavaParser(token_stream)\n    \n    tree = parser.compilationUnit()\n    \n    extractor = FunctionExtractor()\n    walker = ParseTreeWalker()\n    walker.walk(extractor, tree)\n    print(extractor.matched_methods[0])\n"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/JavaParser.py",
        "keyword": "ast",
        "snippet": "    RULE_formalParameterList = 45\n    RULE_formalParameter = 46\n    RULE_lastFormalParameter = 47\n    RULE_methodBody = 48\n    RULE_constructorBody = 49\n    RULE_qualifiedName = 50\n    RULE_literal = 51\n    RULE_annotation = 52\n--\n                   \"primitiveType\", \"typeArguments\", \"typeArgument\", \"qualifiedNameList\", \n                   \"formalParameters\", \"formalParameterList\", \"formalParameter\", \n                   \"lastFormalParameter\", \"methodBody\", \"constructorBody\", \n                   \""
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/JavaListener.py",
        "keyword": "ast",
        "snippet": "\n\n    # Enter a parse tree produced by JavaParser#lastFormalParameter.\n    def enterLastFormalParameter(self, ctx:JavaParser.LastFormalParameterContext):\n        pass\n\n    # Exit a parse tree produced by JavaParser#lastFormalParameter.\n    def exitLastFormalParameter(self, ctx:JavaParser.LastFormalParameterContext):\n        pass\n\n\n    # Enter a parse tree produced by JavaParser#methodBody.\n    def enterMethodBody(self, ctx:JavaParser.MethodBodyContext):\n"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/construct_commands_descriptions.py",
        "keyword": "search_code",
        "snippet": "## collect information to fix the bug\n\nsearch_code_desc = \"\"\"search_code_base: This utility function scans all Java files within a specified project for a given list of keywords. It generates a dictionary as output, organized by file names, classes, and method names. Within each method name, it provides a list of keywords that match the method's content. The resulting structure is as follows: { file_name: { class_name: { method_name: [...list of matched keywords...] } } }. This functionality pro"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/autogpt/agents/agent.py",
        "keyword": "search_code",
        "snippet": "                    \"try_fixes\", \n                    \"read_range\", \n                    \"search_code_base\", \n                    \"get_classes_and_methods\",\n                    \"extract_similar_functions_calls\",\n                    \"extract_method_code\",\n                    \"extract_test_code\"]:\n                    new_command_dict[\"args\"][\"project_name\"] = self.project_name\n"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/scripts/install_plugin_deps.py",
        "keyword": "glob",
        "snippet": "import sys\nimport zipfile\nfrom glob import glob\nfrom pathlib import Path\n\nfrom autogpt.logs import logger\n\n\n--\n\n    # Install zip-based plugins\n    for plugin_archive in plugins_dir.glob(\"*.zip\"):\n        logger.debug(f\"Checking for requirements in '{plugin_archive}'...\")\n        with zipfile.ZipFile(str(plugin_archive), \"r\") as zfile:\n            if not zfile.namelist():\n                continue\n\n--\n\n    # Install directory-based plugins\n"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/autogpt/plugins/__init__.py",
        "keyword": "glob",
        "snippet": "\n    # Zip-based plugins\n    for plugin in plugins_path.glob(\"*.zip\"):\n        if moduleList := inspect_zip_for_modules(str(plugin), debug):\n            for module in moduleList:\n                plugin = Path(plugin)\n                module = Path(module)\n                logger.debug(f\"Zipped Plugin: {plugin}, Module: {module}\")\n"
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/swarm/core.py",
        "keyword": "ast",
        "snippet": "                    return Result(value=str(result))\n                except Exception as e:\n                    error_message = f\"Failed to cast response to string: {result}. Make sure agent functions return a string or Result object. Error: {str(e)}\"\n                    debug_print(debug, error_message)\n                    raise TypeError(error_message)\n\n    def handle_tool_calls(\n        self,\n"
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/swarm/repl/repl.py",
        "keyword": "ast",
        "snippet": "def process_and_print_streaming_response(response):\n    content = \"\"\n    last_sender = \"\"\n\n    for chunk in response:\n        if \"sender\" in chunk:\n            last_sender = chunk[\"sender\"]\n\n        if \"content\" in chunk and chunk[\"content\"] is not None:\n            if not content and last_sender:\n                print(f\"\\033[94m{last_sender}:\\033[0m\", end=\" \", flush=True)\n                last_sender = \"\"\n            print(chunk[\"content\"], end=\"\", flush=True)\n            content += chunk[\"conte"
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/examples/personal_shopper/database.py",
        "keyword": "glob",
        "snippet": "import sqlite3\n\n# global connection\nconn = None\n\n\ndef get_connection():\n    global conn\n    if conn is None:\n        conn = sqlite3.connect(\"application.db\")\n    return conn\n\n\n--\n\ndef close_connection():\n    global conn\n    if conn:\n        conn.close()\n        conn = None\n"
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/examples/customer_service_streaming/configs/tools/query_docs/handler.py",
        "keyword": "glob",
        "snippet": "\n# # Set embedding model\n# # TODO: Add this to global config\nEMBEDDING_MODEL = 'text-embedding-3-large'\n\n# # # Set qdrant collection\ncollection_name = 'help_center'\n\n"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/playwright/async_api/_generated.py",
        "keyword": "tree",
        "snippet": "        \"\"\"ElementHandle.query_selector\n\n        The method finds an element matching the specified selector in the `ElementHandle`'s subtree. If no elements match\n        the selector, returns `null`.\n\n        Parameters\n        ----------\n        selector : str\n--\n        \"\"\"ElementHandle.query_selector_all\n\n        The method finds all elements matching the specified selector in the `ElementHandle`s subtree. If no elements match\n        the selector, returns empty array.\n\n        Parameters\n "
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/tests/async/conftest.py",
        "keyword": "tree",
        "snippet": "\n    @property\n    def actions_tree(self) -> Locator:\n        return self.page.get_by_test_id(\"actions-tree\")\n\n    @property\n    def action_titles(self) -> Locator:\n        return self.page.locator(\".action-title\")\n\n--\n\n    async def expand_action(self, title: str, ordinal: int = 0) -> None:\n        await self.actions_tree.locator(\".tree-view-entry\", has_text=title).nth(\n            ordinal\n        ).locator(\".codicon-chevron-right\").click()\n\n\n@pytest.fixture\n"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/scripts/generate_api.py",
        "keyword": "ast",
        "snippet": "import sys\nfrom types import FunctionType\nfrom typing import Any, Dict, List, Match, Optional, Union, cast, get_args, get_origin\nfrom typing import get_type_hints as typing_get_type_hints\n\nfrom playwright._impl._assertions import (\n    APIResponseAssertions,\n    LocatorAssertions,\n--\n\ndef short_name(t: Any) -> str:\n    match = cast(\n        Match[str], re.compile(r\"playwright\\._impl\\.[^.]+\\.([^']+)\").search(str(t))\n    )\n    return match.group(1)\n\n\n"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/tests/server.py",
        "keyword": "ast",
        "snippet": "    TypeVar,\n    Union,\n    cast,\n)\nfrom urllib.parse import urlparse\n\nfrom autobahn.twisted.resource import WebSocketResource\nfrom autobahn.twisted.websocket import WebSocketServerFactory, WebSocketServerProtocol\n--\n\n_dirname = get_file_dirname()\nreactor = cast(SelectReactor, _twisted_reactor)\n\n\ndef find_free_port() -> int:\n    with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:\n        s.bind((\"\", 0))\n"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/scripts/generate_api.py",
        "keyword": "glob",
        "snippet": "\ndef signature(func: FunctionType, indent: int) -> str:\n    hints = get_type_hints(func, globals())\n    tokens = [\"self\"]\n    split = \",\\n\" + \" \" * indent\n\n    saw_optional = False\n    for [name, value] in hints.items():\n--\n\ndef arguments(func: FunctionType, indent: int) -> str:\n    hints = get_type_hints(func, globals())\n    tokens = []\n    split = \",\\n\" + \" \" * indent\n    for [name, value] in hints.items():\n        value_str = str(value)\n        if name == \"return\":\n--\n\ndef return_type(func: F"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/scripts/generate_async_api.py",
        "keyword": "glob",
        "snippet": "from documentation_provider import DocumentationProvider\nfrom generate_api import (\n    api_globals,\n    arguments,\n    generated_types,\n    get_type_hints,\n    header,\n    process_type,\n--\n    print(\"\")\n    documentation_provider.print_events(class_name)\n    for [name, type] in get_type_hints(t, api_globals).items():\n        print(\"\")\n        print(\"    @property\")\n        print(f\"    def {name}(self) -> {process_type(type)}:\")\n        documentation_provider.print_entry(class_name, name, {\"retu"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/browser_use/browser/events.py",
        "keyword": "codebase",
        "snippet": "# overlapping event names are a nightmare to trace and rename later, dont do it!\n# e.g. prevent ClickEvent and FailedClickEvent are terrible names because one is a substring of the other,\n# must be ClickEvent and ClickFailedEvent to preserve the usefulnes of codebase grep/sed/awk as refactoring tools.\n# at import time, we do a quick check that all event names defined above are valid and non-overlapping.\n# this is hand written in blood by a human! not LLM slop. feel free to optimize but do not re"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/browser_use/dom/enhanced_snapshot.py",
        "keyword": "codebase",
        "snippet": "# Only the ESSENTIAL computed styles for interactivity and visibility detection\nREQUIRED_COMPUTED_STYLES = [\n\t# Only styles actually accessed in the codebase (prevents Chrome crashes on heavy sites)\n\t'display',  # Used in service.py visibility detection\n\t'visibility',  # Used in service.py visibility detection\n\t'opacity',  # Used in service.py visibility detection\n\t'overflow',  # Used in views.py scrollability detection\n\t'overflow-x',  # Used in views.py scrollability detection\n"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/tests/scripts/debug_iframe_scrolling.py",
        "keyword": "tree",
        "snippet": "\"\"\"\nDebug test for iframe scrolling issue where DOM tree only shows top elements after scrolling.\n\nThis test verifies that after scrolling inside an iframe, the selector_map correctly\ncontains lower input elements like City, State, Zip Code, etc.\n\"\"\"\n\n--\n\t\t\t\t\tfor field in missing_fields:\n\t\t\t\t\t\tprint(f'    \u2717 {field}')\n\t\t\t\t\tprint('\\nThis confirms that scrolling inside iframes does not update the DOM tree properly.')\n\t\t\t\telse:\n\t\t\t\t\tprint('\\n\u2705 SUCCESS: All lower form fields are visible after scrolli"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/tests/scripts/test_frame_hierarchy.py",
        "keyword": "tree",
        "snippet": "\t\t\t\tprint(f'      Has Session: {has_session}')\n\n\t\t# Get main page frame tree\n\t\tmain_target = next((t for t in page_targets if url in t.url), page_targets[0] if page_targets else None)\n\n\t\tif main_target:\n\t\t\tprint('\\n\ud83d\udcd0 Main Page Frame Tree:')\n\t\t\tprint(f'  Target: {main_target.url}')\n--\n\t\t\ttry:\n\t\t\t\tawait session.cdp_client.send.Page.enable(session_id=sid)\n\t\t\t\ttree = await session.cdp_client.send.Page.getFrameTree(session_id=sid)\n\n\t\t\t\tprint('\\n  Frame Tree Structure:')\n\n\t\t\t\tdef print_tree(node, inde"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/browser_use/observability.py",
        "keyword": "ast",
        "snippet": "from collections.abc import Callable\nfrom functools import wraps\nfrom typing import Any, Literal, TypeVar, cast\n\nlogger = logging.getLogger(__name__)\nfrom dotenv import load_dotenv\n\nload_dotenv()\n--\n\t\t\t\treturn await func(*args, **kwargs)\n\n\t\t\treturn cast(F, async_wrapper)\n\t\telse:\n\n\t\t\t@wraps(func)\n\t\t\tdef sync_wrapper(*args, **kwargs):\n\t\t\t\treturn func(*args, **kwargs)\n\n\t\t\treturn cast(F, sync_wrapper)\n\n"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/tests/scripts/debug_iframe_scrolling.py",
        "keyword": "ast",
        "snippet": "\t\t\t\texpected_checks = [\n\t\t\t\t\t('First Name', ['firstName', 'first name']),\n\t\t\t\t\t('Last Name', ['lastName', 'last name']),\n\t\t\t\t\t('Email', ['email']),\n\t\t\t\t\t('City', ['city']),\n\t\t\t\t\t('State', ['state']),\n\t\t\t\t\t('Zip', ['zip', 'zipCode']),\n\t\t\t\t]\n"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/browser_use/tools/service.py",
        "keyword": "find_file",
        "snippet": "\n\t\t\t# Helper function to find file input near the selected element\n\t\t\tdef find_file_input_near_element(\n\t\t\t\tnode: EnhancedDOMTreeNode, max_height: int = 3, max_descendant_depth: int = 3\n\t\t\t) -> EnhancedDOMTreeNode | None:\n\t\t\t\t\"\"\"Find the closest file input to the selected element.\"\"\"\n\n\t\t\t\tdef find_file_input_in_descendants(n: EnhancedDOMTreeNode, depth: int) -> EnhancedDOMTreeNode | None:\n\t\t\t\t\tif depth < 0:\n\t\t\t\t\t\treturn None\n\t\t\t\t\tif browser_session.is_file_input(n):\n\t\t\t\t\t\treturn n\n\t\t\t\t\tfor child"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/examples/custom-functions/action_filters.py",
        "keyword": "glob",
        "snippet": "\nFor example:\n    - only run on certain domains @registry.action(..., domains=['example.com', '*.example.com', 'example.co.*']) (supports globs, but no regex)\n    - only fill in a password on a specific login page url\n    - only run if this action has not run before on this page (e.g. by looking up the url in a file on disk)\n\nDuring each step, the agent recalculates the actions available specifically for that page, and informs the LLM.\n\"\"\"\n"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/browser_use/code_use/service.py",
        "keyword": "glob",
        "snippet": "\t\t\t\tif has_await:\n\t\t\t\t\t# When code has await, we must wrap in async function\n\t\t\t\t\t# To make variables persist naturally (like Jupyter without needing 'global'):\n\t\t\t\t\t# 1. Extract all assigned variable names from the code\n\t\t\t\t\t# 2. Inject 'global' declarations for variables that already exist in namespace\n\t\t\t\t\t# 3. Extract user's explicit global declarations and pre-define those vars\n\t\t\t\t\t# 4. Return locals() so we can update namespace with new variables\n\n\t\t\t\t\t# Find all variable names being assi"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/utilities/logger_utils.py",
        "keyword": "codebase",
        "snippet": "    Note:\n        This implementation consolidates warning suppression used throughout\n        the codebase, including specific deprecation warnings from dependencies.\n    \"\"\"\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\")\n        warnings.filterwarnings(\n            \"ignore\", message=\"open_text is deprecated*\", category=DeprecationWarning\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/llm.py",
        "keyword": "codebase",
        "snippet": "            # Attempt to make the completion call, but catch context window errors\n            # and convert them to our own exception type for consistent handling\n            # across the codebase. This allows CrewAgentExecutor to handle context\n            # length issues appropriately.\n            if response_model:\n                params[\"response_model\"] = response_model\n            response = litellm.completion(**params)\n\n        except ContextWindowExceededError as e:\n            # Conver"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/cli/create_crew.py",
        "keyword": "tree",
        "snippet": "            sys.exit(0)\n        click.secho(f\"Overriding folder {folder_name}...\", fg=\"green\", bold=True)\n        shutil.rmtree(folder_path)  # Delete the existing folder and its contents\n\n    click.secho(\n        f\"Creating {'crew' if parent_folder else 'folder'} {folder_name}...\",\n        fg=\"green\",\n        bold=True,\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/cli/utils.py",
        "keyword": "tree",
        "snippet": "\n\ndef tree_copy(source: Path, destination: Path) -> None:\n    \"\"\"Copies the entire directory structure from the source to the destination.\"\"\"\n    for item in os.listdir(source):\n        source_item = os.path.join(source, item)\n        destination_item = os.path.join(destination, item)\n        if os.path.isdir(source_item):\n            shutil.copytree(source_item, destination_item)\n        else:\n            shutil.copy2(source_item, destination_item)\n\n\ndef tree_find_and_replace(directory: Path, f"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/crews/utils.py",
        "keyword": "ast",
        "snippet": "    start_index: int | None,\n    task_outputs: list[Any],\n    last_sync_output: Any | None,\n) -> tuple[TaskExecutionData, list[Any], Any | None]:\n    \"\"\"Prepare a task for execution, handling replay skip logic and agent/tool setup.\n\n    Args:\n        crew: The crew instance.\n--\n        start_index: Index to start execution from (for replay).\n        task_outputs: Current list of task outputs.\n        last_sync_output: Last synchronous task output.\n\n    Returns:\n        A tuple of (TaskExecutionD"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/cli/utils.py",
        "keyword": "ast",
        "snippet": "import shutil\nimport sys\nfrom typing import Any, cast, get_type_hints\n\nimport click\nfrom rich.console import Console\nimport tomli\n\n--\n    - None\n    \"\"\"\n    provider_config = cast(\n        list[str],\n        ENV_VARS.get(\n            provider,\n            [\n                click.prompt(\n--\n        and module_attr.is_crew_class\n    ):\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai-tools/src/crewai_tools/rag/loaders/directory_loader.py",
        "keyword": "find_file",
        "snippet": "        max_files: int | None = kwargs.get(\"max_files\", None)\n\n        files = self._find_files(\n            dir_path, recursive, include_extensions, exclude_extensions\n        )\n\n        if max_files is not None and len(files) > max_files:\n            files = files[:max_files]\n--\n        )\n\n    def _find_files(\n        self,\n        dir_path: str,\n        recursive: bool,\n        include_ext: list[str] | None = None,\n        exclude_ext: list[str] | None = None,\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/cli/utils.py",
        "keyword": "glob",
        "snippet": "    \"\"\"\n    try:\n        init_files = Path(dir_path).glob(\"**/__init__.py\")\n        available_exports = []\n\n        for init_file in init_files:\n            tools = _load_tools_from_init(init_file)\n            available_exports.extend(tools)\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/cli/create_flow.py",
        "keyword": "glob",
        "snippet": "        dst_crew_folder = project_root / \"src\" / folder_name / \"crews\" / crew_folder\n        if src_crew_folder.exists():\n            for src_file in src_crew_folder.rglob(\"*\"):\n                if src_file.is_file():\n                    relative_path = src_file.relative_to(src_crew_folder)\n                    dst_file = dst_crew_folder / relative_path\n                    dst_file.parent.mkdir(parents=True, exist_ok=True)\n                    process_file(src_file, dst_file)\n"
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-core/lavague/core/python_engine.py",
        "keyword": "tree",
        "snippet": "                )\n            if screenshot_folder.exists():\n                shutil.rmtree(screenshot_folder)\n\n            screenshot_folder.mkdir(parents=True, exist_ok=True)\n            self.get_screenshots_batch()\n            screenshots = SimpleDirectoryReader(screenshot_folder).load_data()\n            output = self.ocr_mm_llm.complete(\n--\n\n            # delete temp image folder\n            shutil.rmtree(Path(self.temp_screenshots_path))\n\n        if context_score >= self.confidence_threshold"
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-core/lavague/core/base_driver.py",
        "keyword": "tree",
        "snippet": "        childList: true,\n        attributes: true,\n        subtree: true,\n    });\n    waitForIdle();\n\n    setTimeout(() => {\n        resolve(false);\n"
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-qa/gradio_demo.py",
        "keyword": "ast",
        "snippet": "            scenario = Scenario(parsed_scenario[\"scenario\"][\"name\"])\n            scenarios.append(scenario)\n            last_keyword: str = None\n\n            for step in parsed_scenario[\"scenario\"][\"steps\"]:\n                keyword = step[\"keywordType\"]\n                if keyword == \"Conjunction\":\n                    keyword = last_keyword\n                else:\n                    last_keyword = keyword\n\n                if keyword == \"Context\":\n                    scenario.context.append(step[\"t"
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-qa/lavague/qa/generator.py",
        "keyword": "ast",
        "snippet": "            cleaned_logs[\"instruction\"] + \" \" + cleaned_logs[\"action\"]\n        )\n        last_screenshot = SimpleDirectoryReader(\n            logs.iloc[-1][\"screenshots_path\"]\n        ).load_data()\n        return actions, last_screenshot\n\n    def _build_prompt(self, nodes, actions):\n        return FULL_PROMPT_TEMPLATE.format(\n            feature_file_name=self.feature_file_name,\n            url=self.url,\n--\n                scenario = Scenario(parsed_scenario[\"scenario\"][\"name\"])\n                "
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-core/lavague/core/base_driver.py",
        "keyword": "glob",
        "snippet": "        self,\n        code: str,\n        globals: dict[str, Any] = None,\n        locals: Mapping[str, object] = None,\n    ):\n        \"\"\"Exec generated code\"\"\"\n        pass\n\n"
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-server/lavague/server/driver.py",
        "keyword": "glob",
        "snippet": "        self,\n        code: str,\n        globals: dict[str, Any] = None,\n        locals: Mapping[str, object] = None,\n    ):\n        if code is not None and len(code.strip()) > 0:\n            url = self.send_command_and_get_response_sync(\"exec_code\", code)\n            return url\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/langchain_v1/langchain/agents/middleware/todo.py",
        "keyword": "codebase",
        "snippet": "\n        # Agent now has access to write_todos tool and todo state tracking\n        result = await agent.invoke({\"messages\": [HumanMessage(\"Help me refactor my codebase\")]})\n\n        print(result[\"todos\"])  # Array of todo items with status tracking\n        ```\n    \"\"\"\n\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/langchain_v1/langchain/agents/middleware/file_search.py",
        "keyword": "codebase",
        "snippet": "        @tool\n        def glob_search(pattern: str, path: str = \"/\") -> str:\n            \"\"\"Fast file pattern matching tool that works with any codebase size.\n\n            Supports glob patterns like `**/*.js` or `src/**/*.ts`.\n\n            Returns matching file paths sorted by modification time.\n\n--\n            output_mode: Literal[\"files_with_matches\", \"content\", \"count\"] = \"files_with_matches\",\n        ) -> str:\n            \"\"\"Fast content search tool that works with any codebase size.\n\n     "
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/tracers/root_listeners.py",
        "keyword": "tree",
        "snippet": "\n    def _persist_run(self, run: Run) -> None:\n        # This is a legacy method only called once for an entire run tree\n        # therefore not useful here\n        pass\n\n    def _on_run_create(self, run: Run) -> None:\n        if self.root_id is not None:\n--\n\n    async def _persist_run(self, run: Run) -> None:\n        # This is a legacy method only called once for an entire run tree\n        # therefore not useful here\n        pass\n\n    async def _on_run_create(self, run: Run) -> None:\n        if"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/tracers/context.py",
        "keyword": "tree",
        "snippet": "def _get_tracer_project() -> str:\n    tracing_context = ls_rh.get_tracing_context()\n    run_tree = tracing_context[\"parent\"]\n    if run_tree is None and tracing_context[\"project_name\"] is not None:\n        return tracing_context[\"project_name\"]\n    return getattr(\n        run_tree,\n        \"session_name\",\n        getattr(\n            # Note, if people are trying to nest @traceable functions and the\n            # tracing_v2_enabled context manager, this will likely mess up the\n            # tree "
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/vectorstores/base.py",
        "keyword": "ast",
        "snippet": "                ids = [doc.id for doc in documents]\n\n                # If there's at least one valid ID, we'll assume that IDs\n                # should be used.\n                if any(ids):\n                    kwargs[\"ids\"] = ids\n\n            texts = [doc.page_content for doc in documents]\n--\n                ids = [doc.id for doc in documents]\n\n                # If there's at least one valid ID, we'll assume that IDs\n                # should be used.\n                if any(ids):\n                "
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/outputs/run_info.py",
        "keyword": "ast",
        "snippet": "\n    Users can acquire the run_id information from callbacks or via run_id\n    information present in the astream_event API (depending on the use case).\n    \"\"\"\n\n    run_id: UUID\n    \"\"\"A unique identifier for the model or chain run.\"\"\"\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/vectorstores/__init__.py",
        "keyword": "glob",
        "snippet": "    module_name = _dynamic_imports.get(attr_name)\n    result = import_attr(attr_name, module_name, __spec__.parent)\n    globals()[attr_name] = result\n    return result\n\n\ndef __dir__() -> list[str]:\n    return list(__all__)\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/document_loaders/__init__.py",
        "keyword": "glob",
        "snippet": "    module_name = _dynamic_imports.get(attr_name)\n    result = import_attr(attr_name, module_name, __spec__.parent)\n    globals()[attr_name] = result\n    return result\n\n\ndef __dir__() -> list[str]:\n    return list(__all__)\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/langchain_v1/langchain/agents/middleware/file_search.py",
        "keyword": "ripgrep",
        "snippet": "\n    - Glob: Fast file pattern matching by file path\n    - Grep: Fast content search using ripgrep or Python fallback\n\n    Example:\n        ```python\n        from langchain.agents import create_agent\n        from langchain.agents.middleware import (\n--\n        *,\n        root_path: str,\n        use_ripgrep: bool = True,\n        max_file_size_mb: int = 10,\n    ) -> None:\n        \"\"\"Initialize the search middleware.\n\n        Args:\n            root_path: Root directory to search.\n            use_ri"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/langchain_v1/tests/unit_tests/agents/middleware/implementations/test_file_search.py",
        "keyword": "ripgrep",
        "snippet": "        (tmp_path / \"example.py\").write_text(\"print('hello')\\n\", encoding=\"utf-8\")\n\n        middleware = FilesystemFileSearchMiddleware(root_path=str(tmp_path), use_ripgrep=False)\n\n        result = middleware.grep_search.func(pattern=\"print\", include=\"*.{py\")\n\n        assert result == \"Invalid include pattern\"\n\n    def test_ripgrep_command_uses_literal_pattern(\n        self, tmp_path: Path, monkeypatch: pytest.MonkeyPatch\n    ) -> None:\n        \"\"\"Ensure ripgrep receives pattern after ``--`` to "
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/__main__.py",
        "keyword": "codebase",
        "snippet": "You should pick an appropriate prompt to test the intended code paths and tools. There is no need to mess with the installs or pythonpath. If it isn't working, double check that you are in /home/agent. Check that the tool or agent is being used (you might have forgotten to import it or add it to AVAILABLE_TOOLS / AVAILABLE_AGENTS). You should then thoroughly inspect the outputs in /tmp/workdir as well as the traces in /tmp/test_agent_logs. Are the outputs as expected? Are all the tool and functi"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/benchmarks/aiq_benchmark.py",
        "keyword": "codebase",
        "snippet": "    Benchmark for evaluating agent capability in improving software through the AIQ protocol.\n\n    This benchmark assesses an agent's ability to understand existing codebases and implement\n    effective changes that enhance quality metrics as defined by the AIQ v5.0 protocol.\n    \"\"\"\n\n    name = \"aiq\"\n\n"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/__main__.py",
        "keyword": "tree",
        "snippet": "#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\"\"\"\nMain entrypoint when running the agent defined in this directory with\n`python -m path.to.this.directory`.\n\"\"\"\n\n--\nYou must focus on things like:\n- improving the efficiency and accuracy of the agent in making code edits to files through improved tools (efficient approaches based on find-and-replace, line edits, or generating and applying diffs, etc)\n- exposing traditio"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/conftest.py",
        "keyword": "tree",
        "snippet": "#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\nimport pytest\n\n# Enable asyncio support for pytest\npytest_plugins = [\"pytest_asyncio\"]\n\n"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/__main__.py",
        "keyword": "ast",
        "snippet": "        \"--workdir\",\n        type=str,\n        help=\"The path of the agent code on which to work on / improve. Usually a copy of the last agent iteration's code\",\n    )\n    improve_parser.add_argument(\n        \"--logdir\",\n        type=str,\n        help=\"The full path where the logging outputs should be saved\",\n--\n\n    testing_procedure = \"\"\"\nIt is vital that you finally try to run the entire agent system before you complete, so as to make absolutely sure that it is still functional, and doesn't "
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/events/event_bus.py",
        "keyword": "ast",
        "snippet": "            \"version\": \"1.0\",\n            \"created_at\": datetime.now().isoformat(),\n            \"last_saved\": None,\n        }\n    )\n\n    class Config:\n        arbitrary_types_allowed = True\n--\n\n        # Save metadata\n        self._metadata[\"last_saved\"] = datetime.now().isoformat()\n        (directory / \"metadata.json\").write_text(json.dumps(self._metadata, indent=2))\n\n        # Save event store\n        event_store_dir = directory / \"event_store\"\n        event_store_dir.mkdir(exist_ok=True)\n"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/events/event_bus.py",
        "keyword": "glob",
        "snippet": "\n        Returns:\n            The global EventBus instance.\n        \"\"\"\n        if not cls._lock:\n            cls._lock = asyncio.Lock()\n\n        async with cls._lock:\n"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/oversight/overseer.py",
        "keyword": "glob",
        "snippet": "    - DO NOT jump to negative conclusions too early, particularly if there have only been a handful of events in a agent call. You should sooner wait for a couple of iterations to collect enough data and reduce your uncertainty about the state of a agent execution, than preemptively assuming the agent is unhealthy.\n    - Don't rush the agent too much. There is no strict time budget, although if there seems to have been no progress within 1 or 2 minutes, then intervene.\n    - Consider both local "
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/__main__.py",
        "keyword": "ripgrep",
        "snippet": "You must focus on things like:\n- improving the efficiency and accuracy of the agent in making code edits to files through improved tools (efficient approaches based on find-and-replace, line edits, or generating and applying diffs, etc)\n- exposing traditional developer tools (ripgrep, tree-sitter, etc) to the agent to make it faster and more efficient. When in doubt, just support Python.\n- building reasoning and organisational structures which enable the agent to reliably generate high-quality c"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/tools/__init__.py",
        "keyword": "ripgrep",
        "snippet": "from .execute_command import ExecuteCommand\nfrom .directory_tools import ViewDirectory\nfrom .ripgrep_tool import RipGrepTool\n\n# TODO: expand the concept of toolkits and use throughout the agent implementations\ntoolkits: dict[str, list[BaseTool]] = dict(\n    coding=[\n        ViewDirectory,\n"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/gpt_engineer/core/default/simple_agent.py",
        "keyword": "codebase",
        "snippet": "\nThis module provides a class that represents an agent capable of initializing and improving\na codebase using AI. It handles interactions with the AI model, memory, and execution\nenvironment to generate and refine code based on user prompts.\n\n\"\"\"\n\nimport tempfile\n--\n    An agent that uses AI to generate and improve code based on a given prompt.\n\n    This agent is capable of initializing a codebase from a prompt and improving an existing\n    codebase based on user input. It uses an AI model to ge"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/gpt_engineer/core/default/steps.py",
        "keyword": "codebase",
        "snippet": "This module provides functions that represent different steps in the process of generating\nand improving code using an AI model. These steps include generating code from a prompt,\ncreating an entrypoint for the codebase, executing the entrypoint, and refining code edits.\n\nFunctions\n---------\ncurr_fn : function\n    Returns the name of the current function.\n--\n\ngen_entrypoint : function\n    Generates an entrypoint for the codebase and returns the entrypoint files.\n\nexecute_entrypoint : function\n  "
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/docs/create_api_rst.py",
        "keyword": "tree",
        "snippet": "\n.. autosummary::\n    :toctree: {module}\n    :template: class.rst\n\n    {cstring}\n\n\"\"\"\n--\n\n.. autosummary::\n    :toctree: {module}\n\n    {fstring}\n\n\"\"\"\n    return full_doc\n"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/docs/conf.py",
        "keyword": "tree",
        "snippet": "\n\n# The master toctree document.\nmaster_doc = \"index\"\n\n# General information about the project.\nproject = data[\"tool\"][\"poetry\"][\"name\"]\ncopyright = \"2023 Anton Osika\"\n--\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title, author, documentclass\n# [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \"gpt_engineer.tex\", \"GPT-ENgineer Documentation\", \"manual\"),\n]\n--\n# -- Options for Texinfo output -------------------------------"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/docs/conf.py",
        "keyword": "ast",
        "snippet": "\n\n# The master toctree document.\nmaster_doc = \"index\"\n\n# General information about the project.\nproject = data[\"tool\"][\"poetry\"][\"name\"]\ncopyright = \"2023 Anton Osika\"\nauthor = \" Anton Osika & Contributors\"\n--\n# a list of builtin themes.\n#\n# html_theme = 'alabaster'\nhtml_theme = \"sphinx_rtd_theme\"\n\n# Theme options are theme-specific and customize the look and feel of a\n# theme further.  For a list of options available for each theme, see the\n# documentation.\n--\n# [howto, manual, or own class]).\n"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/gpt_engineer/core/chat_to_files.py",
        "keyword": "ast",
        "snippet": "            hunk_lines.append((RETAIN, line[1:]))\n\n    # Append the last hunk if any\n    if current_diff is not None and hunk_lines and hunk_header is not None:\n        current_diff.hunks.append(Hunk(*hunk_header, hunk_lines))\n\n    return diffs\n\n"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/docs/create_api_rst.py",
        "keyword": "glob",
        "snippet": "\"\"\"Script for auto-generating api_reference.rst\"\"\"\nimport glob\nimport re\n\nfrom pathlib import Path\n\nROOT_DIR = Path(__file__).parents[1].absolute()\n--\ndef load_members() -> dict:\n    members: dict = {}\n    for py in glob.glob(str(PKG_DIR) + \"/**/*.py\", recursive=True):\n        module = py[len(str(PKG_DIR)) + 1 :].replace(\".py\", \"\").replace(\"/\", \".\")\n        top_level = module.split(\".\")[0]\n        if top_level not in members:\n            members[top_level] = {\"classes\": [], \"functions\": []}\n    "
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/gpt_engineer/core/default/disk_memory.py",
        "keyword": "glob",
        "snippet": "            sorted(\n                str(item.relative_to(self.path))\n                for item in sorted(self.path.rglob(\"*\"))\n                if item.is_file()\n            )\n        )\n\n    def __len__(self) -> int:\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/enterprise/integrations/utils.py",
        "keyword": "repo_map",
        "snippet": "from storage.repository_store import RepositoryStore\nfrom storage.stored_repository import StoredRepository\nfrom storage.user_repo_map import UserRepositoryMap\nfrom storage.user_repo_map_store import UserRepositoryMapStore\n\nfrom openhands.core.config.openhands_config import OpenHandsConfig\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.core.schema.agent import AgentState\nfrom openhands.events import Event, EventSource\n--\n        )\n        stored_repos.append(stored_r"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/enterprise/storage/user_repo_map_store.py",
        "keyword": "repo_map",
        "snippet": "from sqlalchemy.orm import sessionmaker\nfrom storage.database import session_maker\nfrom storage.user_repo_map import UserRepositoryMap\n\nfrom openhands.core.config.openhands_config import OpenHandsConfig\n\n\n@dataclass\n--\n    config: OpenHandsConfig\n\n    def store_user_repo_mappings(self, mappings: list[UserRepositoryMap]) -> None:\n        \"\"\"\n        Store user-repository mappings in database\n\n        1. Make sure to store mappings if they don't exist\n        2. If a mapping already exists (same u"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/tests/unit/utils/test_circular_imports.py",
        "keyword": "codebase",
        "snippet": "\nclass TestCircularImports(unittest.TestCase):\n    \"\"\"Test to detect circular imports in the codebase.\"\"\"\n\n    def test_no_circular_imports_in_key_modules(self):\n        \"\"\"\n        Test that there are no circular imports in key modules that were previously problematic.\n\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/tests/unit/server/session/test_agent_session.py",
        "keyword": "codebase",
        "snippet": "from openhands.storage.memory import InMemoryFileStore\n\n# We'll use the DeprecatedState class from the main codebase\n\n\n@pytest.fixture\ndef mock_llm_registry():\n    \"\"\"Create a mock LLM registry that properly simulates LLM registration\"\"\"\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/evaluation/benchmarks/visual_swe_bench/run_infer.py",
        "keyword": "tree",
        "snippet": "    workspace_dir_name = _get_swebench_workspace_dir_name(instance)\n    # Instruction based on Anthropic's official trajectory\n    # https://github.com/eschluntz/swe-bench-experiments/tree/main/evaluation/verified/20241022_tools_claude-3-5-sonnet-updated/trajs\n    instruction = (\n        '<uploaded_files>\\n'\n        f'/workspace/{workspace_dir_name}\\n'\n        '</uploaded_files>\\n'\n        f\"I've uploaded a python code repository in the directory {workspace_dir_name}. Consider the following issu"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/evaluation/benchmarks/scienceagentbench/run_infer.py",
        "keyword": "tree",
        "snippet": "        'task_inst': example['task_inst'],\n        'dataset_path': '/benchmark/datasets/'\n        + example['dataset_folder_tree'].split('\\n')[0][4:],\n        'dataset_folder_tree': example['dataset_folder_tree'],\n        'dataset_preview': example['dataset_preview'],\n        'pred_program_name': 'pred_' + example['gold_program_name'],\n    }\n\n    if use_knowledge:\n--\n    assert obs.exit_code == 0\n\n    dataset_name = instance['dataset_folder_tree'].split('\\n')[0][4:].rstrip('/')\n\n    # Copy the d"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/scripts/update_openapi.py",
        "keyword": "ast",
        "snippet": "Update OpenHands OpenAPI documentation.\n\nGenerates the OpenAPI specification from the FastAPI application and writes it\nto docs/openapi.json.\n\nUsage:\n    python scripts/update_openapi.py\n\n--\n\ndef generate_openapi_spec():\n    \"\"\"Generate the OpenAPI specification from the FastAPI app.\"\"\"\n    spec = app.openapi()\n\n    # Explicitly exclude certain endpoints that are operational, experimental, or UI-only convenience\n    excluded_endpoints = [\n        '/api/conversations/{conversation_id}/exp-config'"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/evaluation/benchmarks/EDA/run_infer.py",
        "keyword": "ast",
        "snippet": "    # retrieve the latest model message from history\n    if state.history:\n        last_agent_message = state.get_last_agent_message()\n        model_guess = last_agent_message.content if last_agent_message else ''\n\n    assert game is not None, 'Game is not initialized.'\n    msg = game.generate_user_response(model_guess)\n    game.curr_turn += 1\n    logger.info(f'Model guess: {model_guess}')\n--\n        raise ValueError('State should not be None.')\n\n    last_agent_message = state.get_last_agent_mes"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/openhands/runtime/plugins/agent_skills/file_ops/file_ops.py",
        "keyword": "find_file",
        "snippet": "- search_dir(search_term: str, dir_path: str = './'): Searches for a term in all files in the specified directory.\n- search_file(search_term: str, file_path: str | None = None): Searches for a term in the specified file or the currently open file.\n- find_file(file_name: str, dir_path: str = './'): Finds all files with the given name in the specified directory.\n\nNote:\n    All functions return string representations of their results.\n\"\"\"\n\n--\n\n\ndef find_file(file_name: str, dir_path: str = './') ->"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/tests/unit/runtime/plugins/test_agent_skill.py",
        "keyword": "find_file",
        "snippet": "    WINDOW,\n    _print_window,\n    find_file,\n    goto_line,\n    open_file,\n    scroll_down,\n    scroll_up,\n    search_dir,\n--\n\n\ndef test_find_file(tmp_path):\n    temp_file_path = tmp_path / 'a.txt'\n    temp_file_path.write_text('Line 1\\nLine 2\\nLine 3\\nLine 4\\nLine 5')\n\n    with io.StringIO() as buf:\n        with contextlib.redirect_stdout(buf):\n            find_file('a.txt', str(tmp_path))\n        result = buf.getvalue()\n    assert result is not None\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/openhands/runtime/plugins/agent_skills/repo_ops/repo_ops.py",
        "keyword": "search_code",
        "snippet": "    explore_tree_structure,\n    get_entity_contents,\n    search_code_snippets,\n)\n\n__all__ = [\n    'get_entity_contents',\n    'search_code_snippets',\n    'explore_tree_structure',\n]\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/openhands/agenthub/loc_agent/function_calling.py",
        "keyword": "search_code",
        "snippet": "            ALL_FUNCTIONS = [\n                'explore_tree_structure',\n                'search_code_snippets',\n                'get_entity_contents',\n            ]\n            if tool_call.function.name in ALL_FUNCTIONS:\n                # We implement this in agent_skills, which can be used via Jupyter\n                func_name = tool_call.function.name\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/scripts/dump_config_schema.py",
        "keyword": "glob",
        "snippet": "import json\n\nfrom openhands.app_server.config import get_global_config\n\nif __name__ == '__main__':\n    config = get_global_config()\n    schema = config.model_json_schema()\n    print(json.dumps(schema, indent=2))\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/evaluation/benchmarks/EDA/run_infer.py",
        "keyword": "glob",
        "snippet": "\ndef codeact_user_response_eda(state: State) -> str:\n    global game\n    model_guess = ''\n\n    # retrieve the latest model message from history\n    if state.history:\n        last_agent_message = state.get_last_agent_message()\n--\n\n    # Use codeactagent as guesser_model\n    global game\n    assert metadata.dataset is not None\n    assert metadata.details is not None\n    game = _game_class[metadata.dataset](\n        item=instance['text'].strip(),\n        answerer_model=metadata.details['answerer_mod"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/tests/runtime/test_glob_and_grep.py",
        "keyword": "ripgrep",
        "snippet": "\n# Skip all tests in this file if running with CLIRuntime,\n# as they depend on `rg` (ripgrep) which is not guaranteed to be available.\n# The underlying ReadOnlyAgent tools (GrepTool, GlobTool) also currently depend on `rg`.\n# TODO: implement a fallback version of these tools that uses `find` and `grep`.\npytestmark = pytest.mark.skipif(\n    os.environ.get('TEST_RUNTIME') == 'cli',\n    reason=\"CLIRuntime: ReadOnlyAgent's GrepTool/GlobTool tests require `rg` (ripgrep), which may not be installed.\","
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/openhands/agenthub/readonly_agent/function_calling.py",
        "keyword": "ripgrep",
        "snippet": "    pattern: str, path: str | None = None, include: str | None = None\n) -> str:\n    # NOTE: This function currently relies on `rg` (ripgrep).\n    # `rg` may not be installed when using CLIRuntime or LocalRuntime.\n    # TODO: Implement a fallback to `grep` if `rg` is not available.\n    \"\"\"Convert grep tool arguments to a shell command string.\n\n    Args:\n--\n\n    Returns:\n        A properly escaped shell command string for ripgrep\n    \"\"\"\n    # Use shlex.quote to properly escape all shell special c"
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter6_to_do_write/src/tools/todo_write.py",
        "keyword": "codebase",
        "snippet": "<example>\nUser: Help me rename the function getCwd to getCurrentWorkingDirectory across my project\nAssistant: Let me first search through your codebase to find all occurrences of 'getCwd'.\n*Uses grep or search tools to locate all instances of getCwd in the codebase*\nAssistant: I've found 15 instances of 'getCwd' across 8 different files. Let me create a todo list to track these changes.\n*Creates todo list with specific items for each file that needs updating*\n\n<reasoning>\nThe assistant used the "
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter6_to_do_write/src/core/prompt/system_rule.py",
        "keyword": "codebase",
        "snippet": "## Following conventions\nWhen making changes to files, first understand the file's code conventions. Mimic code style, use existing libraries and utilities, and follow existing patterns.\n- NEVER assume that a given library is available, even if it is well known. Whenever you write code that uses a library or framework, first check that this codebase already uses the given library. For example, you might look at neighboring files, or check the package.json (or cargo.toml, and so on depending on t"
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter1_tool_call_api/xml_tool_call.py",
        "keyword": "tree",
        "snippet": "from openai import OpenAI\nimport requests\nimport xml.etree.ElementTree as ET\nimport re\n\nOPENROUTER_API_KEY = \"REDACTED_OPENROUTER_API_KEY\"  \nMODEL = \"anthropic/claude-sonnet-4\"\n\n"
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter6_to_do_write/src/core/conversation.py",
        "keyword": "ast",
        "snippet": "        \"\"\"Handle tool calls with user approval when needed.\"\"\"\n        for i, tool_call in enumerate(tool_calls):\n            is_last_tool = (i == len(tool_calls) - 1)\n            try:\n                args = json.loads(tool_call.function.arguments)\n            except json.JSONDecodeError as e:\n                self._ui_manager.print_error(f\"Tool parameter parsing failed: {e}\")\n                tool_response = {\n--\n\n            if should_execute:\n                await self._execute_tool(tool_call,"
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter8_mcp_client/src/core/conversation.py",
        "keyword": "ast",
        "snippet": "        \"\"\"Handle tool calls with user approval when needed.\"\"\"\n        for i, tool_call in enumerate(tool_calls):\n            is_last_tool = (i == len(tool_calls) - 1)\n            try:\n                args = json.loads(tool_call.function.arguments)\n            except json.JSONDecodeError as e:\n                self._ui_manager.print_error(f\"Tool parameter parsing failed: {e}\")\n                tool_response = {\n--\n\n            if should_execute:\n                await self._execute_tool(tool_call,"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/prompts/self_improvement_prompt.py",
        "keyword": "codebase",
        "snippet": "  - See other tools for reference.\n- **Utilities**: `utils/`\n  - The `utils/` directory contains utility functions used across the codebase.\n\n- **Additional Details**:\n  - The agent is very good at automatically utilizing the right available tools at the right time. So do not have an agentic flow that explicitly forces a tool's usage.\n  - Common tools, such as file editing and bash commands, are easy for the agent to recognize and use appropriately. However, more complex and niche tools may requ"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/analysis/visualize_archive.py",
        "keyword": "tree",
        "snippet": "def visualize_experiment_run(dgm_dir, archives, metadata_name=\"metadata.json\"):\n    \"\"\"\n    Visualize the experiment run as a directed tree-like graph (using accuracy_score).\n    \"\"\"\n    graph, pos = build_graph(dgm_dir, archives, score_func=get_performance_score, metadata_name=metadata_name)\n\n    # Decide on output HTML name\n    if metadata_name == \"metadata_new.json\":\n        html_path = os.path.join(dgm_dir, \"dgm_tree_new.html\")\n    else:\n        html_path = os.path.join(dgm_dir, \"dgm_tree.ht"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/polyglot/docker_build.py",
        "keyword": "tree",
        "snippet": "            if local_folder.is_dir():\n                target_folder = build_dir / repo\n                shutil.copytree(local_folder, target_folder, dirs_exist_ok=True)\n                logger.info(f\"Copied folder {local_folder} to {target_folder}\")\n\n        # Write the setup scripts to the build directory\n        for setup_script_name, setup_script in setup_scripts.items():\n            setup_script_path = build_dir / setup_script_name\n"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/prompts/testrepo_prompt.py",
        "keyword": "ast",
        "snippet": "def get_test_command(eval_script):\n    test_hint = ''\n    # test_command is the 2nd last line in eval_script\n    lines = eval_script.strip().split('\\n')\n    test_command = lines[-2].strip()\n    # Remove trailing arguments specifying filepaths\n    parts = test_command.split()\n    if '.' in parts[-1] and not parts[-1].endswith('.py'):\n"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/prompts/self_improvement_prompt.py",
        "keyword": "ast",
        "snippet": "Your response will be automatically parsed, so ensure that the string response is precisely in the correct format. Do NOT include the `<JSON>` tag in your output.\"\"\"\n\ndiagnose_prompt_emptypatches = \"\"\"There are some empty patches when attempting to solve GitHub issues. Since the coding agent is stochastic, it may not always produce a patch. Handle cases where the coding agent fails to generate a patch or generates one that only modifies the test cases without editing the primary source code. For"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/analysis/visualize_archive.py",
        "keyword": "glob",
        "snippet": "        metadata_name = \"metadata.json\"\n\n    # Load the global archives (these are read from dgm_metadata.jsonl, not from the node metadata)\n    archives = load_dgm_metadata(os.path.join(dgm_dir, \"dgm_metadata.jsonl\"))\n    archives = archives[:args.trunc_gens] if args.trunc_gens > 0 else archives\n\n    # Standard visualization & analysis (using accuracy_score)\n    visualize_experiment_run(dgm_dir, archives, metadata_name=metadata_name)\n"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/self_improve_step.py",
        "keyword": "glob",
        "snippet": "):  \n\n    global dataset\n    if polyglot:\n        with open(\"polyglot/polyglot_benchmark_metadata.json\") as f:\n            dataset = json.loads(f.read())\n    else:\n        from datasets import load_dataset\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/aider/models.py",
        "keyword": "repo_map",
        "snippet": "    edit_format: str = \"whole\"\n    weak_model_name: Optional[str] = None\n    use_repo_map: bool = False\n    send_undo_reply: bool = False\n    lazy: bool = False\n    overeager: bool = False\n    reminder: str = \"user\"\n    examples_as_sys_msg: bool = False\n--\n        if \"/o3-mini\" in model:\n            self.edit_format = \"diff\"\n            self.use_repo_map = True\n            self.use_temperature = False\n            self.system_prompt_prefix = \"Formatting re-enabled. \"\n            self.system_promp"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/aider/main.py",
        "keyword": "repo_map",
        "snippet": "\n    if args.map_tokens is None:\n        map_tokens = main_model.get_repo_map_tokens()\n    else:\n        map_tokens = args.map_tokens\n\n    # Track auto-commits configuration\n    analytics.event(\"auto_commits\", enabled=bool(args.auto_commits))\n--\n        return\n\n    if args.show_repo_map:\n        repo_map = coder.get_repo_map()\n        if repo_map:\n            io.tool_output(repo_map)\n        analytics.event(\"exit\", reason=\"Showed repo map\")\n        return\n\n    if args.apply:\n        content = io"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/aider/special.py",
        "keyword": "codebase",
        "snippet": "def filter_important_files(file_paths):\n    \"\"\"\n    Filter a list of file paths to return only those that are commonly important in codebases.\n\n    :param file_paths: List of file paths to check\n    :return: List of file paths that match important file patterns\n    \"\"\"\n    return list(filter(is_important, file_paths))\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/scripts/blame.py",
        "keyword": "tree",
        "snippet": "\n    revision = end_tag if end_tag else \"HEAD\"\n    files = run([\"git\", \"ls-tree\", \"-r\", \"--name-only\", revision]).strip().split(\"\\n\")\n    test_files = [f for f in files if f.startswith(\"tests/fixtures/languages/\") and \"/test.\" in f]\n    files = [\n        f\n        for f in files\n        if f.endswith((\".js\", \".py\", \".scm\", \".sh\", \"Dockerfile\", \"Gemfile\"))\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/scripts/tsl_pack_langs.py",
        "keyword": "tree",
        "snippet": "def main():\n    # Path to the language definitions file\n    lang_def_path = \"../../tmp/tree-sitter-language-pack/sources/language_definitions.json\"\n\n    # Path to store the tags.scm files\n    output_dir = \"aider/queries/tree-sitter-language-pack\"\n\n    # Create the output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Common branch names to try if API fails and config branch doesn't work\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/scripts/clean_metadata.py",
        "keyword": "ast",
        "snippet": "            # Generate unified diff\n            # If dictionaries differ, generate JSON strings to show the diff\n            # Add a dummy key to ensure the *real* last key gets a comma\n            litellm_entry_copy = litellm_entry.copy()\n            aider_entry_copy = aider_entry.copy()\n            dummy_key = \"zzzdummykey\"\n            litellm_entry_copy[dummy_key] = True\n            aider_entry_copy[dummy_key] = True\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/scripts/redact-cast.py",
        "keyword": "ast",
        "snippet": "def main():\n    if len(sys.argv) != 3:\n        print(f\"Usage: {sys.argv[0]} input_cast_file output_cast_file\")\n        sys.exit(1)\n\n    input_file = sys.argv[1]\n    output_file = sys.argv[2]\n\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/tests/basic/test_editblock.py",
        "keyword": "find_file",
        "snippet": "        self.GPT35 = Model(\"gpt-3.5-turbo\")\n\n    def test_find_filename(self):\n        fence = (\"```\", \"```\")\n        valid_fnames = [\"file1.py\", \"file2.py\", \"dir/file3.py\", r\"\\windows\\__init__.py\"]\n\n        # Test with filename on a single line\n        lines = [\"file1.py\", \"```\"]\n        self.assertEqual(eb.find_filename(lines, fence, valid_fnames), \"file1.py\")\n\n        # Test with filename in fence\n        lines = [\"```python\", \"file3.py\", \"```\"]\n        self.assertEqual(eb.find_filename(lines"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/aider/linter.py",
        "keyword": "find_file",
        "snippet": "\n        linenums = []\n        filenames_linenums = find_filenames_and_linenums(errors, [rel_fname])\n        if filenames_linenums:\n            filename, linenums = next(iter(filenames_linenums.items()))\n            linenums = [num - 1 for num in linenums]\n\n        return LintResult(text=errors, lines=linenums)\n--\n\n\ndef find_filenames_and_linenums(text, fnames):\n    \"\"\"\n    Search text for all occurrences of <filename>:\\\\d+ and make a list of them\n    where <filename> is one of the filenames in "
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/scripts/recording_audio.py",
        "keyword": "glob",
        "snippet": "    args = parser.parse_args()\n\n    # Use args.voice directly instead of modifying global VOICE\n    selected_voice = args.voice\n    selected_bitrate = args.bitrate\n\n    # Check if FFmpeg is available for compression\n    if not check_ffmpeg() and not args.dry_run:\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/benchmark/problem_stats.py",
        "keyword": "glob",
        "snippet": "\n    # Look in language subdirectories under exercises/practice\n    for fname in benchmark_dir.glob(\"*/exercises/practice/*/.aider.results.json\"):\n        error = False\n        try:\n            results = json.loads(fname.read_text())\n            error = \"testcase\" not in results\n            if not error:\n--\n        # Copy each hard set problem's directory\n        copied_by_lang = defaultdict(int)\n        for lang_dir in src_dir.glob(\"*/exercises/practice\"):\n            if not lang_dir.is_dir():\n"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/docs/src/generate_api_reference.py",
        "keyword": "tree",
        "snippet": "Script to automatically generate the API reference table of contents for AutoGen.\n\nThis script scans all packages and their modules to generate the toctree entries\nfor the API documentation index.md file.\n\"\"\"\n\nimport os\nfrom pathlib import Path\n--\n\n\ndef generate_toctree_from_rst_files(reference_dir: Path) -> Dict[str, List[str]]:\n    \"\"\"Generate toctree entries directly from existing .rst files.\"\"\"\n    # Initialize sections using constants\n    toctree_sections: Dict[str, List[str]] = {section: ["
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/docs/src/conf.py",
        "keyword": "tree",
        "snippet": "\ndef setup_to_main(\n    app: Sphinx, pagename: str, templatename: str, context, doctree\n) -> None:\n    \"\"\"Add a function that jinja can access for returning an \"edit this page\" link pointing to `main`.\"\"\"\n\n    def to_main(link: str) -> str:\n        \"\"\"Transform \"edit on github\" links and make sure they always point to the main branch.\n"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/docs/src/_extension/code_lint.py",
        "keyword": "ast",
        "snippet": "                # Create a temporary file to store the code block\n                with tempfile.NamedTemporaryFile(mode=\"wb\", suffix=\".py\") as temp_file:\n                    temp_file.write(code.astext().encode())\n                    temp_file.flush()\n\n                    # Run pyright on the temporary file using subprocess.run\n                    import subprocess\n\n--\n                    if result.returncode != 0:\n                        logger.info(\" \" + darkred(\"FAIL\"))\n                      "
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/docs/src/conf.py",
        "keyword": "ast",
        "snippet": "#\n# For the full list of built-in configuration values, see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\nfrom sphinx.application import Sphinx\nfrom typing import Any, Dict\nfrom pathlib import Path\nimport sys\n--\nimport subprocess\n# -- Project information -----------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n\nimport autogen_core\n\nproject = \"autogen_core\"\ncopyright = \"2024, Mic"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/run_task_in_pkgs_if_exist.py",
        "keyword": "glob",
        "snippet": "import glob\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport tomli\n--\n    for project in projects:\n        if \"*\" in project:\n            globbed = glob.glob(str(project), root_dir=workspace_pyproject_file.parent)\n            globbed_paths = [Path(p) for p in globbed]\n            all_projects.extend(globbed_paths)\n        else:\n            all_projects.append(Path(project))\n\n    for project in exclude:\n        if \"*\" in project:\n            globbed = glob.glob(str(project), ro"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/docs/src/generate_api_reference.py",
        "keyword": "glob",
        "snippet": "    if python_ref_dir.exists():\n        print(\"\ud83e\uddf9 Cleaning existing .rst files...\")\n        rst_files = list(python_ref_dir.glob(\"*.rst\"))\n        for rst_file in rst_files:\n            rst_file.unlink()\n        print(f\"   Removed {len(rst_files)} existing .rst files\")\n\n\n--\n    \n    # Get all .rst files and organize them by package\n    for rst_file in python_ref_dir.glob(\"*.rst\"):\n        module_name = rst_file.stem  # filename without .rst extension\n        \n        # Find which documented packa"
      }
    ],
    "code_editing": [
      {
        "repo": "SelfImprovingAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/SelfImprovingAgent/SelfImprover.py",
        "keyword": "diff",
        "snippet": "import re\nimport subprocess\nfrom openai import OpenAI  # Adjust if your library import differs\n\nclass SelfImprovingAgent:\n    def __init__(self, model=\"MODELHERE\", iterations=5):\n        self.model = model\n        self.iterations = iterations\n"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/agents/spec_writer.py",
        "keyword": "edit",
        "snippet": "\n            await self.send_message(\"## Refining specification\\n\\nPythagora is refining the specs based on your input.\")\n            # if user edits the spec with extension, it will be commited to db immediately, so we have to check if the description has changed\n            if current_description != self.current_state.specification.description:\n                convo = AgentConvo(self).template(\n                    \"build_full_specification\",\n                    initial_prompt=self.current_stat"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/ui/ipc_client.py",
        "keyword": "edit",
        "snippet": "    PROJECT_SPECS = \"getProjectSpecs\"\n    TASK_CONVO = \"getTaskConvo\"\n    EDIT_SPECS = \"editSpecs\"\n    FILE_DIFF = \"getFileDiff\"\n    TASK_CURRENT_STATUS = \"getCurrentTaskStatus\"\n    TASK_ADD_NEW = \"addNewTask\"\n    TASK_START_OTHER = \"startOtherTask\"\n    CHAT_MESSAGE_RESPONSE = \"chatMessageResponse\"\n--\n        )\n\n    async def open_editor(self, file: str, line: Optional[int] = None, wait_for_response: bool = False):\n        if not line:\n            pass\n        await self._send(\n            Messa"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/tests/cli/test_cli.py",
        "keyword": "patch",
        "snippet": "from argparse import ArgumentParser, ArgumentTypeError\nfrom datetime import datetime\nfrom unittest.mock import AsyncMock, MagicMock, patch\n\nimport pytest\n\nfrom core.cli.helpers import (\n    init,\n--\n\n\n@patch(\"core.cli.helpers.ArgumentParser\")\ndef test_parse_arguments(mock_ArgumentParser):\n    parser = mock_ArgumentParser.return_value\n\n    parse_arguments()\n\n--\n\n\n"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/tests/disk/test_ignore.py",
        "keyword": "patch",
        "snippet": "from io import StringIO\nfrom os.path import join\nfrom unittest.mock import MagicMock, patch\n\nimport pytest\n\nfrom core.disk.ignore import IgnoreMatcher\n\n--\n    ],\n)\n@patch(\"os.path.isfile\", return_value=True)\n@patch(\"builtins.open\", return_value=MagicMock(read=StringIO(\"\")))\ndef test_ignore_paths(_mock_open, _mock_isfile, path, expected):\n    matcher = IgnoreMatcher(\n        \"/tmp\",\n        [\n            \"*.pyc\",\n--\n    ],\n"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/cli/helpers.py",
        "keyword": "diff",
        "snippet": "import sys\nfrom argparse import ArgumentParser, ArgumentTypeError, Namespace\nfrom difflib import unified_diff\nfrom typing import Optional\nfrom urllib.parse import urlparse\nfrom uuid import UUID\n\nfrom core.config import Config, LLMProvider, LocalIPCConfig, ProviderConfig, UIAdapter, get_config, loader\n--\n    Get the number of added and deleted lines between two files.\n\n    This uses Python difflib to produce a unified diff, then counts\n    the number of added and deleted lines.\n\n    :param old_co"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/ui/ipc_client.py",
        "keyword": "diff",
        "snippet": "        structure is parsed into a Message object.\n\n        This lets us raise different errors based on whether the\n        data is not valid JSON or the JSON structure is not valid\n        for a Message object.\n\n        :param data: Raw byte payload.\n        :return: Message object.\n--\n        )\n\n    async def generate_diff(\n        self,\n        file_path: str,\n        old_content: str,\n        new_content: str,\n        n_new_lines: int = 0,\n--\n        await self._send(MessageType.STOP_APP)\n\n"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/agents/spec_writer.py",
        "keyword": "replace",
        "snippet": "\n        self.state_manager.project.name = project_name\n        self.state_manager.project.folder_name = project_name.replace(\" \", \"_\").replace(\"-\", \"_\")\n\n        self.state_manager.file_system = await self.state_manager.init_file_system(load_existing=False)\n\n        self.process_manager.root_dir = self.state_manager.file_system.root\n\n"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/agents/orchestrator.py",
        "keyword": "replace",
        "snippet": "                if title_match:\n                    # Insert after title\n                    new_head = head_content.replace(title_match.group(1), f\"{title_match.group(1)}\\n    {script_tag}\")\n                else:\n                    # Insert at the beginning of head\n                    new_head = f\"\\n    {script_tag}{head_content}\"\n\n                # Replace old head content with new one\n                new_content = content.replace(head_content, new_head)\n\n                await self.state_mana"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/agents/response.py",
        "keyword": "modify",
        "snippet": "\n    INPUT_REQUIRED = \"input-required\"\n    \"\"\"User needs to modify a line in the generated code.\"\"\"\n\n    IMPORT_PROJECT = \"import-project\"\n    \"\"\"User wants to import an existing project.\"\"\"\n\n    EXTERNAL_DOCS_REQUIRED = \"external-docs-required\"\n"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/llm/convo.py",
        "keyword": "modify",
        "snippet": "\n        This performs a deep copy of all the message\n        contents, so you can safely modify both the\n        parent and the child conversation.\n\n        :return: A copy of the conversation.\n        \"\"\"\n        child = Convo()\n"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/agents/orchestrator.py",
        "keyword": "refactor",
        "snippet": "        )\n\n        # TODO: consider refactoring this into two loop; the outer with one iteration per committed step,\n        # and the inner which runs the agents for the current step until they're done. This would simplify\n        # handle_done() and let us do other per-step processing (eg. describing files) in between agent runs.\n        while True:\n            # If the task is marked as \"redo_human_instructions\", we need to reload the project at the state before the current task breakdown\n   "
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/agents/executor.py",
        "keyword": "refactor",
        "snippet": "\n    def for_step(self, step):\n        # FIXME: not needed, refactor to use self.current_state.current_step\n        # in general, passing current step is not needed\n        self.step = step\n        return self\n\n    async def output_handler(self, out, err):\n"
      },
      {
        "repo": "plandex",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/plandex/app/server/litellm_proxy.py",
        "keyword": "patch",
        "snippet": "  return hdrs\n\n# monkey-patch AnthropicModelInfo.get_anthropic_headers to handle OAuth headers\nAnthropicModelInfo.get_anthropic_headers = _oauth_get_hdrs\n\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import StreamingResponse, JSONResponse\nfrom litellm import completion, _turn_on_debug\n"
      },
      {
        "repo": "plandex",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/plandex/app/server/litellm_proxy.py",
        "keyword": "replace",
        "snippet": "\n  if api_key and api_key.startswith(\"Bearer \"):\n    api_key = api_key.replace(\"Bearer \", \"\")\n\n  # api key optional for local/ollama models, so no need to error if not provided\n\n  # clean up for ollama if needed\n  payload = normalise_for_ollama(payload)\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/scripts/history_prompts.py",
        "keyword": "edit",
        "snippet": "\nOnly add new items not already listed in the history markdown.\nDo NOT edit or update existing history entries.\nDo NOT add duplicate entries for changes that have existing history entries.\nDo NOT add additional entries for small tweaks to features which are already listed in the existing history.\n\nPay attention to see if changes are later modified or superseded in the commit logs.\nThe history doc should only reflect the *final* version of changes which have evolved within a version's commit hist"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/aider/repo.py",
        "keyword": "edit",
        "snippet": "            self.aider_ignore_file = Path(aider_ignore_file)\n\n    def commit(self, fnames=None, context=None, message=None, aider_edits=False, coder=None):\n        \"\"\"\n        Commit the specified files or all dirty files if none are specified.\n\n        Args:\n            fnames (list, optional): List of filenames to commit. Defaults to None (commit all\n--\n            context (str, optional): Context for generating commit message. Defaults to None.\n            message (str, optional): Explicit co"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/scripts/issues.py",
        "keyword": "patch",
        "snippet": "\n    # Close issue\n    response = requests.patch(close_url, headers=headers, json={\"state\": \"closed\"})\n    response.raise_for_status()\n\n    print(f\"  - Commented and closed issue #{issue['number']}\")\n\n\n--\n    for issue in unlabeled_issues:\n        url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}\"\n        response = requests.patch(url, headers=headers, json={\"labels\": [\"question\"]})\n        response.raise_for_status()\n        print(f\"  - Added 'question' label to #"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/tests/scrape/test_playwright_disable.py",
        "keyword": "patch",
        "snippet": "\n\ndef test_scraper_disable_playwright_flag(monkeypatch):\n    io = DummyIO()\n    # Simulate that playwright is not available\n    # (disable_playwright just means playwright_available=False)\n    scraper = Scraper(print_error=io.tool_error, playwright_available=False)\n    # Patch scrape_with_httpx to check it is called\n--\n\n\ndef test_scraper_enable_playwright(monkeypatch):\n    io = DummyIO()\n    # Simulate that playwright is available and should be used\n    scraper = Scraper(print_error=io.tool_erro"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/scripts/clean_metadata.py",
        "keyword": "diff",
        "snippet": "#!/usr/bin/env python\n\nimport difflib\nimport json\nimport re\nfrom pathlib import Path\n\nimport json5\n--\n                print(\"\\n\" + \"=\" * 40)\n                print(\"-\" * 40 + \"\\n\")  # Separator for the next model\n                continue  # Skip diff and removal prompt for identical entries\n\n            # Generate unified diff\n            # If dictionaries differ, generate JSON strings to show the diff\n            # Add a dummy key to ensure the *real* last key gets a comma\n            litellm_en"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/scripts/history_prompts.py",
        "keyword": "diff",
        "snippet": "history_prompt = \"\"\"\nUpdate the history markdown doc with changes shown in the diffs.\nSuccinctly describe actual user-facing changes, not every single commit or detail that was made implementing them.\n\nOnly add new items not already listed in the history markdown.\nDo NOT edit or update existing history entries.\nDo NOT add duplicate entries for changes that have existing history entries.\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/scripts/recording_audio.py",
        "keyword": "replace",
        "snippet": "\n                        # Replace original with compressed version\n                        os.replace(temp_file, file_path)\n                        print(\n                            f\"  \u2713 Compressed: {original_size} \u2192 {compressed_size} bytes\"\n                            f\" ({reduction:.1f}% reduction)\"\n                        )\n                    else:\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/benchmark/plots.py",
        "keyword": "replace",
        "snippet": "\n    # Fix wrong column label\n    df[\"model\"] = df[\"model\"].replace(\"gpt-4-0314\", \"gpt-4-0613\")\n\n    tries = [\n        df[[\"model\", \"pass_rate_2\"]],\n        df[[\"model\", \"pass_rate_1\"]],\n    ]\n--\n            if fmt == \"diff\":\n                color = \"#b3e6a8\"\n                label = \"Search/replace blocks\"\n            elif fmt == \"udiff\":\n                color = \"#b3d1e6\"\n                label = \"Unified diffs\"\n            elif fmt == \"difffolk\":\n                label = \"Baseline + blind, no han"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/scripts/recording_audio.py",
        "keyword": "modify",
        "snippet": "    args = parser.parse_args()\n\n    # Use args.voice directly instead of modifying global VOICE\n    selected_voice = args.voice\n    selected_bitrate = args.bitrate\n\n    # Check if FFmpeg is available for compression\n    if not check_ffmpeg() and not args.dry_run:\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/scripts/logo_svg.py",
        "keyword": "modify",
        "snippet": "\n    # Calculate SVG dimensions based on text length\n    # These values can be adjusted to modify the appearance\n    char_width = 40\n    width = len(text) * char_width\n    height = 60\n    text_x = width / 2  # Center point of the SVG width\n    text_y = height * 0.62  # Center point of the SVG height\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/benchmark/plots.py",
        "keyword": "refactor",
        "snippet": "\n\ndef plot_refactoring(df):\n    tries = [df.groupby([\"model\", \"edit_format\"])[\"pass_rate_1\"].mean()]\n\n    plt.rcParams[\"hatch.linewidth\"] = 0.5\n    plt.rcParams[\"hatch.color\"] = \"#444444\"\n\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/benchmark/benchmark.py",
        "keyword": "refactor",
        "snippet": "import typer\nfrom dotenv import load_dotenv\nfrom plots import plot_refactoring\nfrom rich.console import Console\n\nfrom aider import models, sendchat\nfrom aider.coders import Coder, base_coder\nfrom aider.dump import dump  # noqa: F401\n--\n        # plot_outcomes(df, repeats, repeat_hi, repeat_lo, repeat_avg)\n        # plot_outcomes_claude(df)\n        plot_refactoring(df)\n\n\ndef resolve_dirname(dirname, use_single_prior, make_new):\n    if len(dirname.parts) > 1:\n        return dirname\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/aider/coders/udiff_coder.py",
        "keyword": "search_replace",
        "snippet": "from ..dump import dump  # noqa: F401\nfrom .base_coder import Coder\nfrom .search_replace import (\n    SearchTextNotUnique,\n    all_preprocs,\n    diff_lines,\n    flexible_search_and_replace,\n    search_and_replace,\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/aider/coders/search_replace.py",
        "keyword": "search_replace",
        "snippet": "    if debug:\n        html = dmp.diff_prettyHtml(diff)\n        Path(\"tmp.search_replace_diff.html\").write_text(html)\n\n        for d in diff:\n            print(d[0], repr(d[1]))\n\n        for patch in patches:\n--\n        # dump(diff)\n        html = dmp.diff_prettyHtml(diff)\n        Path(\"tmp.search_replace_diff.html\").write_text(html)\n\n        for d in diff:\n            print(d[0], repr(d[1]))\n\n    new_lines, success = dmp.patch_apply(patches, original_lines)\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/tests/basic/test_ssl_verification.py",
        "keyword": "fuzzy_match",
        "snippet": "    @patch(\"httpx.Client\")\n    @patch(\"httpx.AsyncClient\")\n    @patch(\"aider.models.fuzzy_match_models\", return_value=[])\n    def test_no_verify_ssl_flag_sets_model_info_manager(\n        self,\n        mock_fuzzy_match,\n        mock_async_client,\n        mock_client,\n        mock_load_litellm,\n        mock_set_verify_ssl,\n        mock_offer_url,\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/tests/basic/test_main.py",
        "keyword": "fuzzy_match",
        "snippet": "                }\n\n                # Mock fuzzy_match_models to avoid string operations on MagicMock\n                with patch(\"aider.models.fuzzy_match_models\", return_value=[]):\n                    main(\n                        [\"--no-verify-ssl\", \"--exit\", \"--yes\"],\n                        input=DummyInput(),\n                        output=DummyOutput(),\n                    )\n"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/capabilities/summarization/data/multiple_subleases.py",
        "keyword": "edit",
        "snippet": "(d) Business Interruption Insurance in an amount sufficient to cover Sublessee's ongoing expenses, including without limitation, rent payments under this Sublease, for a period of not less than twelve (12) months.\n\n9.2 Policy Requirements: All insurance policies required to be maintained by Sublessee shall (i) be issued by insurance companies authorized to do business in the State of California with a financial rating of at least A-:VIII as rated in the most recent edition of Best's Insurance Re"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/tool_use/memory_tool.py",
        "keyword": "edit",
        "snippet": "            full_path.write_text(new_content, encoding=\"utf-8\")\n\n            return {\"success\": f\"File {path} has been edited successfully\"}\n\n        except Exception as e:\n            return {\"error\": f\"Cannot edit file {path}: {e}\"}\n\n    def _insert(self, params: dict[str, Any]) -> dict[str, str]:\n        \"\"\"Insert text at a specific line.\"\"\"\n        path = params.get(\"path\")\n        insert_line = params.get(\"insert_line\")\n"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/claude_agent_sdk/utils/html_renderer.py",
        "keyword": "patch",
        "snippet": "    Detect content type and render to HTML.\n\n    This is the main content routing function that dispatches to\n    the appropriate renderer based on content type.\n\n    Args:\n        content: Content to render (image path, DataFrame, messages, etc.)\n        is_image: If True, treat string content as an image path\n"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/tool_use/utils/team_expense_api.py",
        "keyword": "diff",
        "snippet": "\n        # Generate random date within quarter\n        days_diff = (end_date - start_date).days\n        random_days = random.randint(0, days_diff)\n        expense_date = start_date + timedelta(days=random_days)\n\n        # Generate amount\n        amount = round(random.uniform(min_amt, max_amt), 2)\n\n"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/tool_use/utils/customer_service_api.py",
        "keyword": "diff",
        "snippet": "                \"I would love to see a dark mode option. I use the app late at night and the bright interface strains my eyes. This would be a great addition.\",\n                \"Is there a mobile app for iOS/Android? I can only find the web version and it's not very mobile-friendly. Would really help my workflow.\",\n                \"What's the difference between the Standard and Premium plans? The pricing page mentions 'advanced analytics' but doesn't explain what that includes.\",\n               "
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/scripts/validate_all_notebooks.py",
        "keyword": "replace",
        "snippet": "            dashboard += \"Must fix immediately:\\n\"\n            for path, issue in critical_issues[:5]:\n                dashboard += f\"  \u2022 {Path(path).name}: {issue['type'].replace('_', ' ')}\\n\"\n            if len(critical_issues) > 5:\n                dashboard += f\"  ... and {len(critical_issues) - 5} more\\n\"\n\n        if error_issues:\n            dashboard += f\"\\n\ud83d\udfe0 Errors ({len(error_issues)})\\n\"\n--\n\n            for wtype, count in warning_types.items():\n                dashboard += f\"  \u2022 {wtype"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/tool_use/utils/customer_service_api.py",
        "keyword": "replace",
        "snippet": "                \"I ordered {product} on {date1} (Order #{order_id}). Tracking shows it was delivered on {date2}, but I never received it. Can you investigate?\",\n                \"I received my order #{order_id} today but it's the wrong item. I ordered {ordered_item} but received {received_item} instead. How do we fix this?\",\n                \"My package (Order #{order_id}) arrived today but the box was badly damaged and the product inside is broken. I need a replacement.\",\n                \"I need "
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/tool_use/memory_demo/sample_code/web_scraper_v1.py",
        "keyword": "modify",
        "snippet": "\"\"\"\nConcurrent web scraper with a race condition bug.\nMultiple threads modify shared state without synchronization.\n\"\"\"\n\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom typing import Any\n\n"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/tool_use/memory_demo/sample_code/data_processor_v1.py",
        "keyword": "modify",
        "snippet": "    A shared cache with thread-safety issues.\n\n    BUG: Classic read-modify-write race condition pattern.\n    \"\"\"\n\n    def __init__(self):\n        self.cache = {}  # BUG: Shared dict without locking\n        self.hit_count = 0  # BUG: Race condition\n"
      },
      {
        "repo": "devika",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/devika/devika.py",
        "keyword": "patch",
        "snippet": "\"\"\"\nfrom gevent import monkey\nmonkey.patch_all()\nfrom src.init import init_devika\ninit_devika()\n\n\nfrom flask import Flask, request, jsonify, send_file\n"
      },
      {
        "repo": "devika",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/devika/src/agents/patcher/__init__.py",
        "keyword": "patch",
        "snippet": "from .patcher import Patcher\n"
      },
      {
        "repo": "devika",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/devika/src/filesystem/read_code.py",
        "keyword": "replace",
        "snippet": "        config = Config()\n        project_path = config.get_projects_dir()\n        self.directory_path = os.path.join(project_path, project_name.lower().replace(\" \", \"-\"))\n\n    def read_directory(self):\n        files_list = []\n        for root, _dirs, files in os.walk(self.directory_path):\n            for file in files:\n"
      },
      {
        "repo": "devika",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/devika/src/project.py",
        "keyword": "replace",
        "snippet": "\n    def get_project_path(self, project: str):\n        return os.path.join(self.project_path, project.lower().replace(\" \", \"-\"))\n\n    def project_to_zip(self, project: str):\n        project_path = self.get_project_path(project)\n        zip_path = f\"{project_path}.zip\"\n\n"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/autogpt/core/planning/schema.py",
        "keyword": "edit",
        "snippet": "    RESEARCH: str = \"research\"\n    WRITE: str = \"write\"\n    EDIT: str = \"edit\"\n    CODE: str = \"code\"\n    DESIGN: str = \"design\"\n    TEST: str = \"test\"\n    PLAN: str = \"plan\"\n\n"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/autogpt/commands/file_operations.py",
        "keyword": "edit",
        "snippet": "        content = read_textual_file(filename, logger)\n\n        # TODO: invalidate/update memory when file is edited\n        file_memory = MemoryItem.from_text_file(content, filename, agent.config)\n        if len(file_memory.chunks) > 1:\n            return file_memory.summary\n\n        return content\n"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/get_defects4j_list.py",
        "keyword": "patch",
        "snippet": "\ndef get_list_of_bugs(project_path):\n    patches = [p.split(\".\")[0] for p in os.listdir(os.path.join(project_path, \"patches\")) if \"src\" in p]\n    return patches\n\n\nprojects = get_list_of_projects()\nbugs_list = []\nfor p in projects:\n"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/python_server.py",
        "keyword": "patch",
        "snippet": "        return jsonify({\"error\": str(e)}), 500\n\n@app.route('/patch', methods=['POST'])\ndef write_file():\n    data = request.get_json()    \n    try:\n        apply_changes(data)\n        return jsonify({\"message\": \"Patch applied succefully\"})\n"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/construct_commands_descriptions.py",
        "keyword": "diff",
        "snippet": "\n## collect information to understand the bug\nextract_test_desc = \"\"\"extract_test_code: This function allows you to extract the code of the failing test cases which will help you understand the test case that led to failure for example by looking at the assertions and the given input and expected output, params: (project_name: string, bug_index: integer, test_file_path: string). You are allowed to execute this command for once only, unless it returns an error message, in which case you can try a"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/run_batch_on_defects4j.py",
        "keyword": "diff",
        "snippet": "\nif __name__ == '__main__':\n    params_list = [(\"experimental_setups/batches/{}\".format(i), \"hyperparams.json\", i) for i in range(20)]  # List of different parameter values\n    finished_processes = []\n\n    with multiprocessing.Pool() as pool:\n        results = pool.map(run_bash_script, params_list)\n        for result in results:\n"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/create_fix_template.py",
        "keyword": "replace",
        "snippet": "\n\tfix_template_str = json.dumps(fix_template)\n\tfix_template_str = fix_template_str.replace('\"modifications\": []', '\"modifications\": [here put the list of modification dictionaries {\"line_number\":..., \"modified_line\":...}, ...]')\n\tfix_template_str = fix_template_str.replace('\"deletions\": []', '\"deletions\": [here put the lines number to delete...]')\n\tfix_template_str = fix_template_str.replace('\"insertions\": []', '\"insertions\": [here put the list of insertion dictionaries to add new lines of code:"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/create_files_index.py",
        "keyword": "replace",
        "snippet": "        for file in files:\n            if file.endswith(\".java\"):\n                java_files.append(os.path.join(root.replace(\"{}/\".format(main_dir), \"\"), file))\n\n    return java_files\n\n#with open(os.path.join(sys.argv[1], \"files_index.txt\"), \"w\") as fit:\n#    fit.write(\"\\n\".join(list_java_files(sys.argv[1])))\n"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/prepare_ai_settings.py",
        "keyword": "modify",
        "snippet": "- Locate the Bug: Execute test cases and get info to systematically identify and isolate the bug within the project \\\"{name}\\\" and bug index \\\"{bug_index}\\\".\n- Perform code Analysis: Analyze the lines of code associated with the bug to discern and comprehend the potentially faulty sections.\n- Try simple Fixes: Attempt straightforward remedies, such as altering operators, changing identifiers, modifying numerical or boolean literals, adjusting function arguments, or refining conditional statement"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/autogpt/memory/message_history.py",
        "keyword": "modify",
        "snippet": "            max_summary_length = self.max_summary_tlength\n\n        # Create a copy of the new_events list to prevent modifying the original list\n        new_events = copy.deepcopy(new_events)\n\n        # Replace \"assistant\" with \"you\". This produces much better first person past tense results.\n        for event in new_events:\n            if event.role.lower() == \"assistant\":\n"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/python_server.py",
        "keyword": "apply_changes",
        "snippet": "import os\n\nfrom apply_changes import apply_changes\napp = Flask(__name__)\n\ndef execute_tests(path):\n    # Implement your test logic here\n    print(\"PATH:::::::\", path)\n--\n    data = request.get_json()    \n    try:\n        apply_changes(data)\n        return jsonify({\"message\": \"Patch applied succefully\"})\n    except Exception as e:\n        print(e)\n        return jsonify({\"error\": str(e)}), 500\n\n"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/apply_changes.py",
        "keyword": "apply_changes",
        "snippet": "from fuzzywuzzy import fuzz\ndef apply_changes(change_dict):\n    file_name = change_dict.get(\"file_name\", \"\")\n    insertions = change_dict.get(\"insertions\", [])\n    deletions = change_dict.get(\"deletions\", [])\n    modifications = change_dict.get(\"modifications\", [])\n\n"
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/examples/airline/configs/tools.py",
        "keyword": "edit",
        "snippet": "\n\ndef initiate_flight_credits():\n    status = \"Successfully initiated flight credits\"\n    return status\n\n\ndef case_resolved():\n    return \"Case resolved. No further questions.\"\n"
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/examples/airline/configs/agents.py",
        "keyword": "edit",
        "snippet": "        escalate_to_agent,\n        initiate_refund,\n        initiate_flight_credits,\n        transfer_to_triage,\n        case_resolved,\n    ],\n)\n\n"
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/tests/mock_client.py",
        "keyword": "diff",
        "snippet": "    def set_sequential_responses(self, responses: list[ChatCompletion]):\n        \"\"\"\n        Set the mock to return different responses sequentially.\n        :param responses: A list of ChatCompletion responses to return in order.\n        \"\"\"\n        self.chat.completions.create.side_effect = responses\n\n    def assert_create_called_with(self, **kwargs):\n"
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/examples/airline/configs/agents.py",
        "keyword": "diff",
        "snippet": "\ndef transfer_to_triage():\n    \"\"\"Call this function when a user needs to be transferred to a different agent and a different policy.\n    For instance, if a user is asking about a topic that is not handled by the current agent, call this function.\n    \"\"\"\n    return triage_agent\n\n\n"
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/swarm/repl/repl.py",
        "keyword": "replace",
        "snippet": "            f = tool_call[\"function\"]\n            name, args = f[\"name\"], f[\"arguments\"]\n            arg_str = json.dumps(json.loads(args)).replace(\":\", \"=\")\n            print(f\"\\033[95m{name}\\033[0m({arg_str[1:-1]})\")\n\n\ndef run_demo_loop(\n    starting_agent, context_variables=None, stream=False, debug=False\n"
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/examples/customer_service_streaming/src/swarm/assistants.py",
        "keyword": "replace",
        "snippet": "        '''Evaluates the assistant's performance on a task'''\n        output = get_completion(client, [{'role': 'user', 'content': EVALUATE_TASK_PROMPT.format(task.description, plan_log)}])\n        output.content = output.content.replace(\"'\",'\"')\n        try:\n            return json.loads(output.content)\n        except json.JSONDecodeError:\n            print(\"An error occurred while decoding the JSON.\")\n            return None\n"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/examples/todomvc/mvctests/test_editing.py",
        "keyword": "edit",
        "snippet": "\n\ndef test_should_hide_other_controls_when_editing(page: Page) -> None:\n    todo_item = page.locator(\".todo-list li\").nth(1)\n    todo_item.dblclick()\n    expect(todo_item.locator(\".toggle\")).not_to_be_visible()\n    expect(todo_item.locator(\"label\")).not_to_be_visible()\n    assert_number_of_todos_in_local_storage(page, 3)\n\n\ndef test_should_save_edits_on_blur(page: Page) -> None:\n    todo_items = page.locator(\".todo-list li\")\n    todo_items.nth(1).dblclick()\n    todo_items.nth(1).locator(\".edit\")."
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/examples/todomvc/mvctests/test_item.py",
        "keyword": "edit",
        "snippet": "\n\ndef test_should_allow_me_to_edit_an_item(page: Page) -> None:\n    create_default_todos(page)\n\n    todo_items = page.locator(\".todo-list li\")\n    secondTodo = todo_items.nth(1)\n    secondTodo.dblclick()\n    expect(secondTodo.locator(\".edit\")).to_have_value(TODO_ITEMS[1])\n    secondTodo.locator(\".edit\").fill(\"buy some sausages\")\n    secondTodo.locator(\".edit\").press(\"Enter\")\n\n    # Explicitly assert the new text value.\n    expect(todo_items).to_have_text([TODO_ITEMS[0], \"buy some sausages\", TODO"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/scripts/generate_api.py",
        "keyword": "patch",
        "snippet": "    r\"add_init_script\\.script\",\n    r\"cookies\\.urls\",\n    r\"dispatch_event\\.eventInit\",\n    r\"eval.*\\.arg\",\n    r\"expect_.*\\.predicate\",\n    r\"evaluate_handle\\.arg\",\n    r\"frame.*\\.name\",\n    r\"register\\.script\",\n"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/scripts/documentation_provider.py",
        "keyword": "patch",
        "snippet": "        self.api = json.loads(process_output.stdout)\n        self.errors: Set[str] = set()\n        self._patch_case()\n\n    def _patch_case(self) -> None:\n        self.classes = {}\n        for clazz in self.api:\n            if not works_for_python(clazz):\n                continue\n            members = {}\n"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/examples/todomvc/mvctests/test_new_todo.py",
        "keyword": "diff",
        "snippet": "    create_default_todos(page)\n\n    # Check test using different methods.\n    expect(page.locator(\".todo-count\")).to_have_text(\"3 items left\")\n    expect(page.locator(\".todo-count\")).to_contain_text(\"3\")\n    expect(page.locator(\".todo-count\")).to_have_text(re.compile(\"3\"))\n\n    # Check all items in one call.\n"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/tests/conftest.py",
        "keyword": "diff",
        "snippet": "\n            if golden_image.size != received_image.size:\n                pytest.fail(\"Image size differs to golden image\")\n                return\n            diff_pixels = pixelmatch(\n                from_PIL_to_raw_data(received_image),\n                from_PIL_to_raw_data(golden_image),\n                golden_image.size[0],\n                golden_image.size[1],\n                threshold=0.2,\n            )\n            assert diff_pixels == 0\n        except Exception:\n            if os.getenv(\""
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/scripts/generate_async_api.py",
        "keyword": "replace",
        "snippet": "            return_type_value = return_type(value)\n            return_type_value = re.sub(r\"\\\"([^\\\"]+)Impl\\\"\", r\"\\1\", return_type_value)\n            return_type_value = return_type_value.replace(\n                \"EventContextManager\", \"AsyncEventContextManager\"\n            )\n            print(\"\")\n            async_prefix = \"async \" if is_async else \"\"\n            print(\n"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/scripts/documentation_provider.py",
        "keyword": "replace",
        "snippet": "                        func_arg = \"typing.Dict\"\n                    if \"Union[\" in func_arg:\n                        func_arg = func_arg.replace(\"Union[\", \"typing.Union[\")\n                    if len(events) > 1:\n                        doc.append(\"    @typing.overload\")\n                    impl = \"\"\n                    if len(events) == 1:\n                        impl = f\"        return super().{event_type}(event=event,f=f)\"\n--\n    def beautify_method_comment(self, comment: str, indent: str) ->"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/playwright/async_api/_generated.py",
        "keyword": "modify",
        "snippet": "        ```\n\n        One can also modify request while falling back to the subsequent handler, that way intermediate route handler can\n        modify url, method, headers and postData of the request.\n\n        ```py\n        async def handle(route, request):\n            # override headers\n            headers = {\n--\n        \"\"\"Page.route\n\n        Routing provides the capability to modify network requests that are made by a page.\n\n        Once routing is enabled, every request matching the url patte"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/playwright/sync_api/_generated.py",
        "keyword": "modify",
        "snippet": "        ```\n\n        One can also modify request while falling back to the subsequent handler, that way intermediate route handler can\n        modify url, method, headers and postData of the request.\n\n        ```py\n        def handle(route, request):\n            # override headers\n            headers = {\n--\n        \"\"\"Page.route\n\n        Routing provides the capability to modify network requests that are made by a page.\n\n        Once routing is enabled, every request matching the url pattern wil"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/tests/ci/test_fallback_llm.py",
        "keyword": "edit",
        "snippet": "\n\tdef test_switch_on_402_error(self):\n\t\t\"\"\"Test that agent switches to fallback on 402 Payment Required (insufficient credits).\"\"\"\n\t\tfrom browser_use import Agent\n\n\t\tprimary = create_mock_llm('primary-model')\n\t\tfallback = create_mock_llm('fallback-model')\n\n\t\tagent = Agent(task='Test task', llm=primary, fallback_llm=fallback)\n\n\t\terror = ModelProviderError(message='Insufficient credits', status_code=402, model='primary-model')\n\t\tresult = agent._try_switch_to_fallback_llm(error)\n\n\t\tassert result is"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/examples/custom-functions/advanced_search.py",
        "keyword": "edit",
        "snippet": "\tserp_data = json.loads(data.decode('utf-8'))\n\n\t# exclude searchParameters and credits\n\tserp_data = {k: v for k, v in serp_data.items() if k not in ['searchParameters', 'credits']}\n\n\t# keep the value of the key \"organic\"\n\n\torganic = serp_data.get('organic', [])\n\t# remove the key \"position\"\n"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/tests/scripts/debug_iframe_scrolling.py",
        "keyword": "patch",
        "snippet": "\t\t\t\"\"\"Capture DOM state and return analysis\"\"\"\n\t\t\tprint(f'\\n\ud83d\udcf8 Capturing DOM state: {label}')\n\t\t\tstate_event = browser_session.event_bus.dispatch(\n\t\t\t\tBrowserStateRequestEvent(include_dom=True, include_screenshot=False, include_recent_events=False)\n\t\t\t)\n\t\t\tbrowser_state = await state_event.event_result()\n\n\t\t\tif browser_state and browser_state.dom_state and browser_state.dom_state.selector_map:\n"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/tests/scripts/test_frame_hierarchy.py",
        "keyword": "patch",
        "snippet": "\t\tfrom browser_use.browser.events import BrowserStopEvent\n\n\t\tstop_event = session.event_bus.dispatch(BrowserStopEvent())\n\t\ttry:\n\t\t\tawait asyncio.wait_for(stop_event, timeout=2.0)\n\t\texcept TimeoutError:\n\t\t\tprint('\u26a0\ufe0f Browser stop timed out')\n\n"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/browser_use/mcp/server.py",
        "keyword": "diff",
        "snippet": "\t\t\t\ttypes.Tool(\n\t\t\t\t\tname='browser_switch_tab',\n\t\t\t\t\tdescription='Switch to a different tab',\n\t\t\t\t\tinputSchema={\n\t\t\t\t\t\t'type': 'object',\n\t\t\t\t\t\t'properties': {'tab_id': {'type': 'string', 'description': '4 Character Tab ID of the tab to switch to'}},\n\t\t\t\t\t\t'required': ['tab_id'],\n\t\t\t\t\t},\n--\n\n\tasync def _switch_tab(self, tab_id: str) -> str:\n\t\t\"\"\"Switch to a different tab.\"\"\"\n\t\tif not self.browser_session:\n\t\t\treturn 'Error: No browser session active'\n\n\t\tfrom browser_use.browser.events import Switc"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/examples/features/rerun_history.py",
        "keyword": "diff",
        "snippet": "1. Run an agent and save its history (including initial URL navigation)\n2. Detect variables in the saved history (emails, names, dates, etc.)\n3. Rerun the history with substituted values (different data)\n4. Get AI-generated summary of rerun completion (with screenshot analysis)\n\nUseful for:\n- Debugging agent behavior\n- Testing changes with consistent scenarios\n- Replaying successful workflows with different data\n- Understanding what values can be substituted in reruns\n- Getting automated verific"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/browser_use/init_cmd.py",
        "keyword": "replace",
        "snippet": "\t\t\t\tfor cmd in step.get('commands', []):\n\t\t\t\t\t# Replace placeholders\n\t\t\t\t\tcmd = cmd.replace('{template}', template)\n\t\t\t\t\tcmd = cmd.replace('{output}', output_path.name)\n\t\t\t\t\tnext_steps.append(f'   {cmd}\\n', style='dim')\n\n\t\t\t\t# Optional note\n\t\t\t\tif 'note' in step:\n\t\t\t\t\tnext_steps.append(f'   {step[\"note\"]}\\n', style='dim italic')\n"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/tests/ci/conftest.py",
        "keyword": "replace",
        "snippet": "\t\t'ANONYMIZED_TELEMETRY': 'false',\n\t\t'BROWSER_USE_CLOUD_SYNC': 'true',\n\t\t'BROWSER_USE_CLOUD_API_URL': 'http://placeholder-will-be-replaced-by-specific-test-fixtures',\n\t\t'BROWSER_USE_CLOUD_UI_URL': 'http://placeholder-will-be-replaced-by-specific-test-fixtures',\n\t\t# Don't set BROWSER_USE_CONFIG_DIR anymore - let it use the default ~/.config/browseruse\n\t\t# This way extensions will be cached in ~/.config/browseruse/extensions\n\t}\n\n\tfor key, value in test_env_vars.items():\n"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/tests/ci/test_variable_substitution.py",
        "keyword": "modify",
        "snippet": "\ndef test_substitute_preserves_original_history(mock_llm):\n\t\"\"\"Test that substitution doesn't modify the original history\"\"\"\n\tagent = Agent(task='test', llm=mock_llm)\n\n\t# Create mock history\n\telement = create_test_element(attributes={'type': 'email'})\n\thistory = create_mock_history(\n"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/browser_use/tools/service.py",
        "keyword": "modify",
        "snippet": "\n\t\t@self.registry.action(\n\t\t\t'Replace specific text within a file by searching for old_str and replacing with new_str. Use this for targeted edits like updating todo checkboxes or modifying specific lines without rewriting the entire file.'\n\t\t)\n\t\tasync def replace_file(file_name: str, old_str: str, new_str: str, file_system: FileSystem):\n\t\t\tresult = await file_system.replace_file_str(file_name, old_str, new_str)\n\t\t\tlogger.info(f'\ud83d\udcbe {result}')\n\t\t\treturn ActionResult(extracted_content=result, long_"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/browser_use/browser/events.py",
        "keyword": "refactor",
        "snippet": "\n\n# TODO: refactor this to:\n# - on_BrowserConnectedEvent() -> dispatch(LoadStorageStateEvent()) -> _copy_storage_state_from_json_to_browser(json_file, new_cdp_session) + return storage_state from handler\n# - on_BrowserStopEvent() -> dispatch(SaveStorageStateEvent()) -> _copy_storage_state_from_browser_to_json(new_cdp_session, json_file)\n# and get rid of StorageStateSavedEvent and StorageStateLoadedEvent, have the original events + provide handler return values for any results\nclass StorageStateL"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai-tools/tests/test_optional_dependencies.py",
        "keyword": "edit",
        "snippet": "    (project_dir / \"pyproject.toml\").write_text(pyproject_content)\n    run_command(\n        [\"uv\", \"add\", \"--editable\", f\"file://{Path.cwd().absolute()}\"], project_dir\n    )\n    run_command([\"uv\", \"sync\"], project_dir)\n    yield project_dir\n\n\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/devtools/src/crewai_devtools/cli.py",
        "keyword": "edit",
        "snippet": "    \"--dry-run\", is_flag=True, help=\"Show what would be done without making changes\"\n)\n@click.option(\"--no-edit\", is_flag=True, help=\"Skip editing release notes\")\ndef tag(dry_run: bool, no_edit: bool) -> None:\n    \"\"\"Create and push a version tag on main branch.\n\n    Run this after the version bump PR has been merged.\n    Automatically detects version from __version__ in packages.\n\n    Args:\n        dry_run: Show what would be done without making changes.\n        no_edit: Skip editing release no"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai-tools/src/crewai_tools/aws/bedrock/browser/browser_toolkit.py",
        "keyword": "patch",
        "snippet": "\n            # Override _run to use _arun when in an asyncio loop\n            def patched_run(*args, **kwargs):\n                try:\n                    import nest_asyncio  # type: ignore[import-untyped]\n\n                    loop = asyncio.get_event_loop()\n                    nest_asyncio.apply(loop)\n--\n                    )\n                except Exception as e:\n                    return f\"Error in patched _run: {e!s}\"\n\n            self._run = patched_run  # type: ignore[method-assign]\n\n    a"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai-tools/tests/file_read_tool_test.py",
        "keyword": "patch",
        "snippet": "import os\nfrom unittest.mock import mock_open, patch\n\nfrom crewai_tools import FileReadTool\n\n\ndef test_file_read_tool_constructor():\n--\n\n    # Use mock_open to mock file operations\n    with patch(\"builtins.open\", mock_open(read_data=test_content)):\n        # Test reading file with runtime file_path\n        tool = FileReadTool()\n        result = tool._run(file_path=test_file)\n        assert result == test_content\n\n--\n\n    # Test permission error\n    with patch(\"builtins.open\", side_effect=Permiss"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/mcp/transports/http.py",
        "keyword": "diff",
        "snippet": "                    await self._transport_context.__aexit__(None, None, None)\n                except (RuntimeError, asyncio.CancelledError) as e:\n                    # Ignore \"exit cancel scope in different task\" errors and cancellation\n                    # These happen when asyncio.run() closes the event loop\n                    # while background tasks are still running\n                    error_msg = str(e).lower()\n                    if \"cancel scope\" not in error_msg and \"task\" not in erro"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/cli/tools/main.py",
        "keyword": "diff",
        "snippet": "            )\n            console.print(\n                \"[bold yellow]Tip:[/bold yellow] Navigate to a different directory and try again.\"\n            )\n            raise SystemExit\n\n    def _print_current_organization(self) -> None:\n        settings = Settings()\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/conftest.py",
        "keyword": "replace",
        "snippet": "def _filter_request_headers(request: Request) -> Request:  # type: ignore[no-any-unimported]\n    \"\"\"Filter sensitive headers from request before recording.\"\"\"\n    for header_name, replacement in HEADERS_TO_FILTER.items():\n        for variant in [header_name, header_name.upper(), header_name.title()]:\n            if variant in request.headers:\n                request.headers[variant] = [replacement]\n\n    request.method = request.method.upper()\n    return request\n\n\ndef _filter_response_headers(res"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/cli/crew_chat.py",
        "keyword": "replace",
        "snippet": "    # Derive the crew class name from the project name\n    # E.g., if project_name is 'my_project', crew_class_name is 'MyProject'\n    crew_class_name = project_name.replace(\"_\", \" \").title().replace(\" \", \"\")\n\n    # Add the 'src' directory to sys.path\n    src_path = cwd / \"src\"\n    if str(src_path) not in sys.path:\n        sys.path.insert(0, str(src_path))\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/utilities/agent_utils.py",
        "keyword": "modify",
        "snippet": "                content=(\n                    \"Warning: before_llm_call hook replaced messages with non-list. \"\n                    \"Restoring original messages list. Hooks should modify messages in-place, \"\n                    \"not replace the list (e.g., use context.messages.append() not context.messages = []).\"\n                ),\n                color=\"yellow\",\n            )\n            if isinstance(original_messages, list):\n--\n                content=(\n                    \"Warning: after_ll"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/hooks/tool_hooks.py",
        "keyword": "modify",
        "snippet": "\n    Args:\n        hook: Function that receives ToolCallHookContext and can modify\n            the tool result. Return modified result string or None to keep\n            the original result. The tool_result is available in context.tool_result.\n\n    Example:\n        >>> def sanitize_output(context: ToolCallHookContext) -> str | None:\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/devtools/src/crewai_devtools/prompts.py",
        "keyword": "refactor",
        "snippet": "\n### Refactoring\n- [List refactor: commits here, using imperative mood like \"Refactor X\", \"Simplify Y\"]\n\n### Breaking Changes\n- [List commits with BREAKING CHANGE in footer or ! after type, using imperative mood]$contributors_section\n\nInstructions:\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/utilities/prompts.py",
        "keyword": "refactor",
        "snippet": "\n    Notes:\n        - Need to refactor so that prompt is not tightly coupled to agent.\n    \"\"\"\n\n    i18n: I18N = Field(default_factory=get_i18n)\n    has_tools: bool = Field(\n        default=False, description=\"Indicates if the agent has access to tools\"\n"
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-core/lavague/core/action_engine.py",
        "keyword": "patch",
        "snippet": "        self.navigation_control.set_logger(logger)\n\n    def dispatch_instruction_gradio(self, next_engine_name: str, instruction: str):\n        \"\"\"\n        Dispatch the instruction to the appropriate ActionEngine\n\n        Args:\n            next_engine_name (`str`): The name of the engine to call\n            instruction (`str`): The instruction to perform\n\n--\n            )\n\n    def dispatch_instruction(\n        self, next_engine_name: str, instruction: str\n    ) -> ActionResult:\n        \"\"\"\n     "
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-core/lavague/core/agents.py",
        "keyword": "patch",
        "snippet": "                break\n\n            yield from self.action_engine.dispatch_instruction_gradio(\n                next_engine_name, instruction\n            )\n\n            success = self.action_engine.ret.success\n            output = self.action_engine.ret.output\n--\n            return\n\n        yield from self.action_engine.dispatch_instruction_gradio(\n            next_engine_name, instruction\n        )\n\n        success = self.action_engine.ret.success\n\n--\n            return self.result\n\n"
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-core/lavague/core/base_driver.py",
        "keyword": "diff",
        "snippet": "\nclass ScrollDirection(Enum):\n    \"\"\"Enum for the different scroll directions. Value is (x, y, dimension_index)\"\"\"\n\n    LEFT = (-1, 0, 0)\n    RIGHT = (1, 0, 0)\n    UP = (0, -1, 1)\n    DOWN = (0, 1, 1)\n"
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-integrations/drivers/lavague-drivers-selenium/lavague/drivers/selenium/base.py",
        "keyword": "diff",
        "snippet": "        viewport_height = self.driver.execute_script(\"return window.innerHeight;\")\n\n        height_difference = height - viewport_height\n        self.driver.set_window_size(width, height + height_difference)\n        self.width = width\n        self.height = height\n\n    def code_for_resize(self, width, height) -> str:\n        return f\"\"\"\ndriver.set_window_size({width}, {height})\nviewport_height = driver.execute_script(\"return window.innerHeight;\")\nheight_difference = {height} - viewport_height\ndri"
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-qa/lavague/qa/prompts.py",
        "keyword": "replace",
        "snippet": "            EC.presence_of_element_located((By.ID, \"cart-total\"))\n        )\n        total_value = float(total_element.text.replace('$', ''))\n        assert total_value > 0, \"Cart total should be greater than zero\"\n    except Exception as e:\n        pytest.fail(\"Failed to verify cart total\" + str(e))\n\n@then('the product list should be visible')\n"
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-qa/lavague/qa/utils.py",
        "keyword": "replace",
        "snippet": "\ndef clean_llm_output(code: str) -> str:\n    return code.replace(\"```python\", \"\").replace(\"```\", \"\").replace(\"```\\n\", \"\")\n\n\ndef build_run_summary(logs, final_feature_path, final_pytest_path, execution_time):\n    token_summary = {\n        \"world_model_input_tokens\": 0,\n"
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-integrations/drivers/lavague-drivers-playwright/lavague/drivers/playwright/base.py",
        "keyword": "modify",
        "snippet": "        super().__init__(url, get_sync_playwright_page)\n\n    # Before modifying this function, check if your changes are compatible with code_for_init which parses this code\n    # these imports are necessary as they will be pasted to the output\n    def default_init_code(self) -> Page:\n        from lavague.core.base_driver import JS_SETUP_GET_EVENTS\n\n        try:\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/rate_limiters.py",
        "keyword": "edit",
        "snippet": "        Args:\n            requests_per_second: The number of tokens to add per second to the bucket.\n                The tokens represent \"credit\" that can be used to make requests.\n            check_every_n_seconds: Check whether the tokens are available\n                every this many seconds. Can be a float to represent\n                fractions of a second.\n            max_bucket_size: The maximum number of tokens that can be in the bucket.\n                Must be at least `1`. Used to preve"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/partners/anthropic/langchain_anthropic/middleware/anthropic_tools.py",
        "keyword": "edit",
        "snippet": "\"\"\"Anthropic text editor and memory tool middleware.\n\nThis module provides client-side implementations of Anthropic's text editor and\nmemory tools using schema-less tool definitions and tool call interception.\n\"\"\"\n\nfrom __future__ import annotations\n\n--\n\n# Tool type constants\nTEXT_EDITOR_TOOL_TYPE = \"text_editor_20250728\"\nTEXT_EDITOR_TOOL_NAME = \"str_replace_based_edit_tool\"\nMEMORY_TOOL_TYPE = \"memory_20250818\"\nMEMORY_TOOL_NAME = \"memory\"\n\nMEMORY_SYSTEM_PROMPT = \"\"\"IMPORTANT: ALWAYS VIEW YOUR ME"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/runnables/__init__.py",
        "keyword": "patch",
        "snippet": "        ensure_config,\n        get_config_list,\n        patch_config,\n        run_in_executor,\n    )\n    from langchain_core.runnables.fallbacks import RunnableWithFallbacks\n    from langchain_core.runnables.history import RunnableWithMessageHistory\n    from langchain_core.runnables.passthrough import (\n--\n    \"ensure_config\",\n    \"get_config_list\",\n    \"patch_config\",\n    \"run_in_executor\",\n)\n\n_dynamic_imports = {\n    \"chain\": \"base\",\n--\n    \"ensure_config\": \"config\",\n    \"get_config_list\": \"co"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/runnables/base.py",
        "keyword": "patch",
        "snippet": "    get_executor_for_config,\n    merge_configs,\n    patch_config,\n    run_in_executor,\n    set_config_context,\n)\nfrom langchain_core.runnables.utils import (\n    AddableDict,\n--\n\n        Output is streamed as Log objects, which include a list of\n        Jsonpatch ops that describe how the state of the run has changed in each\n        step, and the final state of the run.\n\n        The Jsonpatch ops can be applied in order to construct state.\n\n        Args:\n            input: The input to the `Runn"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/prompt_values.py",
        "keyword": "diff",
        "snippet": "\"\"\"**Prompt values** for language model prompts.\n\nPrompt values are used to represent different pieces of prompts.\nThey can be used to represent text, images, or chat message pieces.\n\"\"\"\n\nfrom __future__ import annotations\n\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/vectorstores/base.py",
        "keyword": "diff",
        "snippet": "        \"\"\"Return a similarity score on a scale [0, 1].\"\"\"\n        # The 'correct' relevance function\n        # may differ depending on a few things, including:\n        # - the distance / similarity metric used by the VectorStore\n        # - the scale of your embeddings (OpenAI's are unit normed. Many\n        #  others are not!)\n        # - embedding dimensionality\n        # - etc.\n--\n        \"\"\"The 'correct' relevance function.\n\n        may differ depending on a few things, including:\n\n        "
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/prompts/chat.py",
        "keyword": "replace",
        "snippet": "        \"\"\"\n        # TODO: Handle partials\n        title = self.__class__.__name__.replace(\"MessagePromptTemplate\", \" Message\")\n        title = get_msg_title_repr(title, bold=html)\n        return f\"{title}\\n\\n{self.prompt.pretty_repr(html=html)}\"\n\n\nclass ChatMessagePromptTemplate(BaseStringMessagePromptTemplate):\n--\n        \"\"\"\n        # TODO: Handle partials\n        title = self.__class__.__name__.replace(\"MessagePromptTemplate\", \" Message\")\n        title = get_msg_title_repr(title, bold=html)"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/tools/base.py",
        "keyword": "replace",
        "snippet": "            generic_map = dict(zip(generic_type_vars, get_args(parent), strict=False))\n            for field in getattr(parent_origin, \"__annotations__\", {}):\n                annotations[field] = _replace_type_vars(\n                    annotations[field], generic_map, default_to_bound=default_to_bound\n                )\n\n    return {\n        k: _replace_type_vars(v, default_to_bound=default_to_bound)\n        for k, v in annotations.items()\n    }\n\n\ndef _replace_type_vars(\n    type_: type | TypeVar"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/langchain_v1/langchain/agents/factory.py",
        "keyword": "modify",
        "snippet": "        middleware: A sequence of middleware instances to apply to the agent.\n\n            Middleware can intercept and modify agent behavior at various stages.\n\n            !!! tip \"\"\n\n                See the [Middleware](https://docs.langchain.com/oss/python/langchain/middleware)\n                docs for more information.\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/runnables/base.py",
        "keyword": "modify",
        "snippet": "    ================\n\n    All `Runnable`s expose additional methods that can be used to modify their\n    behavior (e.g., add a retry policy, add lifecycle listeners, make them\n    configurable, etc.).\n\n    These methods will work on any `Runnable`, including `Runnable` chains\n    constructed by composing other `Runnable`s.\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/langchain_v1/langchain/agents/middleware/todo.py",
        "keyword": "refactor",
        "snippet": "\n        # Agent now has access to write_todos tool and todo state tracking\n        result = await agent.invoke({\"messages\": [HumanMessage(\"Help me refactor my codebase\")]})\n\n        print(result[\"todos\"])  # Array of todo items with status tracking\n        ```\n    \"\"\"\n\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/partners/fireworks/langchain_fireworks/chat_models.py",
        "keyword": "refactor",
        "snippet": "# (not sure how important it is)\n# - Environment variable is different\n# we should refactor into some OpenAI-like class in the future\nclass ChatFireworks(BaseChatModel):\n    \"\"\"`Fireworks` Chat large language models API.\n\n    To use, you should have the\n    environment variable `FIREWORKS_API_KEY` set with your API key.\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/langchain/tests/unit_tests/chains/test_imports.py",
        "keyword": "fuzzy_match",
        "snippet": "    \"VectorDBQA\",\n    \"VectorDBQAWithSourcesChain\",\n    \"create_citation_fuzzy_match_chain\",\n    \"create_citation_fuzzy_match_runnable\",\n    \"create_extraction_chain\",\n    \"create_extraction_chain_pydantic\",\n    \"create_qa_with_sources_chain\",\n    \"create_qa_with_structure_chain\",\n    \"create_tagging_chain\",\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/langchain/langchain_classic/chains/openai_functions/__init__.py",
        "keyword": "fuzzy_match",
        "snippet": "    create_structured_output_chain,\n)\nfrom langchain_classic.chains.openai_functions.citation_fuzzy_match import (\n    create_citation_fuzzy_match_chain,\n    create_citation_fuzzy_match_runnable,\n)\nfrom langchain_classic.chains.openai_functions.extraction import (\n    create_extraction_chain,\n    create_extraction_chain_pydantic,\n)\n--\n__all__ = [\n    \"convert_to_openai_function\",\n    \"create_citation_fuzzy_match_chain\",\n    \"create_citation_fuzzy_match_runnable\",\n    \"create_extraction_chain\",\n "
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/__main__.py",
        "keyword": "edit",
        "snippet": "\nYou must focus on things like:\n- improving the efficiency and accuracy of the agent in making code edits to files through improved tools (efficient approaches based on find-and-replace, line edits, or generating and applying diffs, etc)\n- exposing traditional developer tools (ripgrep, tree-sitter, etc) to the agent to make it faster and more efficient. When in doubt, just support Python.\n- building reasoning and organisational structures which enable the agent to reliably generate high-quality "
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/oversight/overseer.py",
        "keyword": "edit",
        "snippet": "Meta Improvement:\n    - The agent may be instructed to work on its own code\n    - It sometimes gets confused, especially when testing tools, about what code is running: the code it is editing is not the code that is currently running\n    - If the agent attempts to test one of its own tools by invoking it directly (which is wrong and will fail), then you MUST intervene, and remind it that the code it is editing is not the code that it is currently running\n\nReasoning Structures\n    - When an agent"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/benchmarks/file_editing.py",
        "keyword": "patch",
        "snippet": "from pathlib import Path\nfrom dataclasses import dataclass\nfrom diff_match_patch import diff_match_patch\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport git\n\nfrom .base import BaseBenchmark, Problem\n\n--\ndef compute_diff_similarity_old(base_content: str, target_content: str, edited_content: str) -> Tuple[float, Optional[str]]:\n    \"\"\"\n    Compute similarity score between target and edited content using diff-match-patch.\n    Optimized for large files with timeout protection."
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/benchmarks/swebench_verified.py",
        "keyword": "patch",
        "snippet": "    environment_setup_commit: str\n    problem_statement: str\n    patch: str\n    test_patch: str\n    hints_text: str\n    fail_to_pass: str\n    pass_to_pass: str\n    version: str = None\n\n--\n            \"environment_setup_commit\": self.environment_setup_commit,\n            \"problem_statement\": self.problem_statement,\n            \"patch\": self.patch,\n            \"test_patch\": self.test_patch,\n            \"hints_text\": self.hints_text,\n            \"FAIL_TO_PASS\": self.fail_to_pass,\n            \"PASS_"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/__main__.py",
        "keyword": "diff",
        "snippet": "\nYou must focus on things like:\n- improving the efficiency and accuracy of the agent in making code edits to files through improved tools (efficient approaches based on find-and-replace, line edits, or generating and applying diffs, etc)\n- exposing traditional developer tools (ripgrep, tree-sitter, etc) to the agent to make it faster and more efficient. When in doubt, just support Python.\n- building reasoning and organisational structures which enable the agent to reliably generate high-quality "
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/oversight/overseer.py",
        "keyword": "diff",
        "snippet": "\n        Args:\n            model: The LLM model to use for analysis - try to use a different\n                model to the main loop model.\n            check_interval: How often to check execution state (seconds)\n        \"\"\"\n        self.model = model\n        self.check_interval = check_interval\n"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/__main__.py",
        "keyword": "replace",
        "snippet": "\nYou must focus on things like:\n- improving the efficiency and accuracy of the agent in making code edits to files through improved tools (efficient approaches based on find-and-replace, line edits, or generating and applying diffs, etc)\n- exposing traditional developer tools (ripgrep, tree-sitter, etc) to the agent to make it faster and more efficient. When in doubt, just support Python.\n- building reasoning and organisational structures which enable the agent to reliably generate high-quality "
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/events/event_bus_utils.py",
        "keyword": "replace",
        "snippet": "    def truncate(text: str, length: int = max_content_len) -> str:\n        \"\"\"Helper to truncate text and handle newlines\"\"\"\n        text = text.replace(\"\\n\", \" \")\n        return f\"{text[:length]}...\" if len(text) > length else text\n\n    def format_output(prefix: str, content: str, metadata: str = \"\") -> None:\n        \"\"\"Helper to format and print consistent output\"\"\"\n        print(\n"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/conftest.py",
        "keyword": "modify",
        "snippet": "\n# Skip tests based on markers unless the corresponding option is provided\ndef pytest_collection_modifyitems(config, items):\n    if not config.getoption(\"--run-llm\"):\n        skip_llm = pytest.mark.skip(reason=\"need --run-llm option to run\")\n        for item in items:\n            if \"uses_llm\" in item.keywords:\n                item.add_marker(skip_llm)\n"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/benchmarks/file_editing.py",
        "keyword": "modify",
        "snippet": "        for _ in range(num_changes):\n            idx = random.randint(0, len(lines) - 1)\n            change_type = random.choice(['modify', 'insert', 'delete'])\n\n            if change_type == 'modify':\n                lines[idx] = lines[idx] + \"  # Modified\"\n            elif change_type == 'insert':\n                indent = lines[idx].count('    ') * '    '\n                lines.insert(idx, indent + f\"# New line {idx}\")\n            elif change_type == 'delete':\n"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/gpt_engineer/core/chat_to_files.py",
        "keyword": "edit",
        "snippet": "            line_dict = file_to_lines_dict(files[diff.filename_pre])\n            for hunk in diff.hunks:\n                current_line = hunk.start_line_pre_edit\n                for line in hunk.lines:\n                    if line[0] == RETAIN:\n                        current_line += 1\n                    elif line[0] == ADD:\n                        # Handle added lines\n--\n    if not diffs:\n        print(\n            \"GPT did not provide any proposed changes. Please try to reselect the files for u"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/gpt_engineer/core/default/constants.py",
        "keyword": "edit",
        "snippet": "---------\nMAX_EDIT_REFINEMENT_STEPS : int\n    The maximum number of refinement steps allowed when generating edit blocks.\n\"\"\"\nMAX_EDIT_REFINEMENT_STEPS = 2\n"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/gpt_engineer/benchmark/benchmarks/gptme/load.py",
        "keyword": "patch",
        "snippet": "            ),\n            Task(\n                name=\"hello-patch\",\n                initial_code=FilesDict({\"hello.py\": \"print('Hello, world!')\"}),\n                command=\"python hello.py\",\n                prompt=Prompt(\"Patch the code in hello.py to print 'Hello, human!'\"),\n                assertions={\n                    \"correct output\": lambda assertable: assertable.stdout\n"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/tests/test_install.py",
        "keyword": "patch",
        "snippet": "\n@pytest.mark.requires_key\ndef test_installed_main_execution(tmp_path, monkeypatch):\n    # Ignore git installation check\n    monkeypatch.setattr(\"gpt_engineer.core.git.is_git_installed\", lambda: False)\n    tmp_path = Path(tmp_path)\n    p = tmp_path / \"projects/example\"\n    p.mkdir(parents=True)\n    (p / \"prompt\").write_text(\"make a program that prints the outcome of 4+4\")\n    proc = subprocess.Popen(\n"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/scripts/legacy_benchmark.py",
        "keyword": "diff",
        "snippet": "\"\"\"\nThis module provides functionality to run benchmarks on different folders within\nthe 'benchmark' directory, wait for their completion, and generate a report.\n\"\"\"\n\n# list all folders in benchmark folder\n# for each folder, run the benchmark\n"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/gpt_engineer/core/project_config.py",
        "keyword": "diff",
        "snippet": "        default_config = Config().to_dict()\n        for k, v in self.to_dict().items():\n            # only write values that are already explicitly set, or that differ from defaults\n            if k in config or v != default_config[k]:\n                if isinstance(v, dict):\n                    config[k] = {\n                        k2: v2\n                        for k2, v2 in v.items()\n"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/docs/create_api_rst.py",
        "keyword": "replace",
        "snippet": "    members: dict = {}\n    for py in glob.glob(str(PKG_DIR) + \"/**/*.py\", recursive=True):\n        module = py[len(str(PKG_DIR)) + 1 :].replace(\".py\", \"\").replace(\"/\", \".\")\n        top_level = module.split(\".\")[0]\n        if top_level not in members:\n            members[top_level] = {\"classes\": [], \"functions\": []}\n        with open(py, \"r\") as f:\n            for line in f.readlines():\n--\n            continue\n\n        module_title = module.replace(\"_\", \" \").title()\n        if module_title == \"Ll"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/tests/core/test_chat_to_files.py",
        "keyword": "replace",
        "snippet": "    captured_output = capture_print_output(lambda: parse_diffs(multi_diff))\n    diffs = parse_diffs(multi_diff)\n    correct_diff = \"\\n\".join(multi_diff.strip().split(\"\\n\")[1:8]).replace(\n        \"```\\n```\", \"\"\n    )\n    assert (\n        \"Multiple diffs found for a/file1.txt. Only the first one is kept.\"\n        in captured_output.getvalue()\n"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/gpt_engineer/applications/cli/learning.py",
        "keyword": "modify",
        "snippet": "    Asks the user for their consent to store their data for the purpose of improving the GPT Engineer tool.\n\n    The user's response is recorded in a file for future reference. If the user consents, the function will write 'true' to the file. If the user does not consent, no data will be collected, and the function will not modify the file.\n\n    Returns\n    -------\n    bool\n        True if the user consents, False otherwise.\n"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/gpt_engineer/applications/cli/main.py",
        "keyword": "modify",
        "snippet": "        \"--improve\",\n        \"-i\",\n        help=\"Improve an existing project by modifying the files.\",\n    ),\n    lite_mode: bool = typer.Option(\n        False,\n        \"--lite\",\n        \"-l\",\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/evaluation/benchmarks/EDA/run_infer.py",
        "keyword": "edit",
        "snippet": "        )\n    )\n    # ======= Attempt to evaluate the agent's edits =======\n    # If you are working on simpler benchmark that only evaluates the final model output (e.g., in a MessageAction)\n    # You can simply get the LAST `MessageAction` from the returned `state.history` and parse it for evaluation.\n\n    if state is None:\n        raise ValueError('State should not be None.')\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/third_party/runtime/impl/e2b/e2b_runtime.py",
        "keyword": "edit",
        "snippet": "            return ErrorObservation(f\"Failed to write file: {e}\")\n    \n    def edit(self, action: FileEditAction) -> Observation:\n        \"\"\"Edit a file using E2B's file system.\"\"\"\n        if self.file_store is None:\n            return ErrorObservation(\"E2B file store not initialized. Call connect() first.\")\n            \n        try:\n--\n            )\n        except Exception as e:\n            return ErrorObservation(f\"Failed to edit file: {e}\")\n    \n    def browse(self, action: BrowseURLAction) "
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/openhands/critic/finish_critic.py",
        "keyword": "patch",
        "snippet": "    \"\"\"This is a simple rule-based critic that checks if the last event is an AgentFinishAction.\n    If not, it will return a score of 0 and a message indicating that the agent did not finish.\n    If the git patch is provided and is empty, it will return a score of 0 and a message indicating that the git patch is empty.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    def evaluate(\n        self, events: list[Event], git_patch: str | None = None\n    ) -> CriticResult:\n        last_action = next"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/openhands/critic/base.py",
        "keyword": "patch",
        "snippet": "\nclass BaseCritic(abc.ABC):\n    \"\"\"A critic is a function that takes in a list of events, optional git patch, and returns a score about the quality of those events.\"\"\"\n\n    @abc.abstractmethod\n    def evaluate(\n        self, events: list[Event], git_patch: str | None = None\n    ) -> CriticResult:\n        pass\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/evaluation/benchmarks/humanevalfix/run_infer.py",
        "keyword": "diff",
        "snippet": "        'You SHOULD INCLUDE PROPER INDENTATION in your edit commands.\\n'\n    )\n    # NOTE: You can actually set slightly different instruction for different agents\n    instruction += AGENT_CLS_TO_INST_SUFFIX[metadata.agent_class]\n\n    # Here's how you can run the agent (similar to the `main` function) and get the final task state\n    runtime = create_runtime(config)\n    call_async_from_sync(runtime.connect)\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/third_party/runtime/impl/e2b/e2b_runtime.py",
        "keyword": "diff",
        "snippet": "        return ErrorObservation(\n            \"Interactive browsing is not supported in E2B runtime. \"\n            \"Use browse() for simple URL fetching or consider using a different runtime.\"\n        )\n    \n    def list_files(self, path: str | None = None) -> list[str]:\n        \"\"\"List files in the sandbox.\"\"\"\n        if self.sandbox is None:\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/evaluation/benchmarks/versicode/output_processing/clear_ans.py",
        "keyword": "replace",
        "snippet": "            content = (\n                output[start_index:end_index]\n                .replace('```python', '')\n                .replace('```', '')\n            )\n        else:\n            content = 'no_answer'\n\n        temp_list.append(content)\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/evaluation/benchmarks/versicode/output_processing/choose_core_line_from_block_versicode.py",
        "keyword": "replace",
        "snippet": "        return None, None\n\n    replaced_lines = {}\n    lines = code_snippet.split('\\n')\n\n    in_multi_line_comment = False\n\n    for i, line in enumerate(lines):\n--\n\n            if re.search(r'\\b{}\\b(?!\\s*=)'.format(re.escape(core_token)), line):\n                replaced_lines.update({i: line})\n\n    if replaced_lines:\n        random_line_location = random.choice(list(replaced_lines.keys()))\n\n        masked_line = lines[random_line_location]\n        leading_spaces = re.match(r'^\\s*', masked_line)."
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/openhands/core/logger.py",
        "keyword": "modify",
        "snippet": "\n    def format(self, record: logging.LogRecord) -> str:\n        # Create a deep copy of the record to avoid modifying the original\n        new_record = _fix_record(record)\n\n        # Strip ANSI color codes from the message\n        new_record.msg = strip_ansi(new_record.msg)\n\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/evaluation/benchmarks/EDA/run_infer.py",
        "keyword": "modify",
        "snippet": "    if args.llm_config:\n        llm_config = get_llm_config_arg(args.llm_config)\n        # modify_params must be False for evaluation purpose, for reproducibility and accurancy of results\n        llm_config.modify_params = False\n\n    if llm_config is None:\n        raise ValueError(f'Could not find LLM config: --llm_config {args.llm_config}')\n\n    metadata = make_metadata(\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/evaluation/benchmarks/versicode/inference_utils/api_code_migration.py",
        "keyword": "refactor",
        "snippet": "    You are now a professional Python programming engineer. I will provide you with a code snippet and a description of its functionality,\n    including the dependencies and versions used in the code. Then, I will provide the same dependencies but with a specified new version.\n    Your task is to refactor the code using the methods provided by the specified new version and return the refactored code.\n    Please note that you only need to return the refactored code and enclose it with <start> and"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/evaluation/benchmarks/versicode/inference_utils/test_migration.py",
        "keyword": "refactor",
        "snippet": "    You are now a professional Python programming engineer. I will provide you with a code snippet and a description of its functionality,\n    including the dependencies and versions used in the code. Then, I will provide the same dependencies but with a specified new version.\n    Your task is to refactor the code using the methods provided by the specified new version and return the refactored code.\n    Please note that you only need to return the refactored code and enclose it with <start> and"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/openhands/integrations/github/service/repos.py",
        "keyword": "fuzzy_match",
        "snippet": "            return []\n\n    def _fuzzy_match_org_name(self, query: str, org_name: str) -> bool:\n        \"\"\"Check if query fuzzy matches organization name.\"\"\"\n        query_lower = query.lower().replace('-', '').replace('_', '').replace(' ', '')\n        org_lower = org_name.lower().replace('-', '').replace('_', '').replace(' ', '')\n\n        # Exact match after normalization\n--\n            # Also search for top repos from orgs that match the query name\n            for org in user_orgs:\n            "
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter1_tool_call_api/xml_tool_call.py",
        "keyword": "edit",
        "snippet": "# - **Ulysses** - Joyce's masterpiece and one of the most important novels of the 20th century\n# - **A Portrait of the Artist as a Young Man** - A semi-autobiographical novel following Stephen Dedalus\n# - **Dubliners** - A collection of short stories depicting life in Dublin (appears twice in different editions)\n\n# **Other Works:**\n# - **Chamber Music** - A collection of Joyce's early poetry (also appears in multiple editions)\n# - **Exiles: A Play in Three Acts** - Joyce's only surviving play\n# "
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter6_to_do_write/src/tools/cmd_runner.py",
        "keyword": "edit",
        "snippet": "   - When searching within files, always use grep:\n     Example: `grep \"search_pattern\" <file>`\n   - Before editing a file, use sed to preview the specific lines with their line numbers if you are unsure of the exact range. Make sure it\u2019s the code you actually want to update.\n     Example: sed -n \"start_line,end_line p\" <file>\n\nNote: The above commands are for Linux/macOS systems. For Windows, use appropriate command-line equivalents (e.g., `dir`, `type`, PowerShell commands).\n\nUsage notes:\n"
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter6_to_do_write/src/tools/todo_write.py",
        "keyword": "diff",
        "snippet": "Assistant: Let me first search through your codebase to find all occurrences of 'getCwd'.\n*Uses grep or search tools to locate all instances of getCwd in the codebase*\nAssistant: I've found 15 instances of 'getCwd' across 8 different files. Let me create a todo list to track these changes.\n*Creates todo list with specific items for each file that needs updating*\n\n<reasoning>\nThe assistant used the todo list because:\n1. First, the assistant searched to understand the scope of the task\n2. Upon fin"
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter1_tool_call_api/xml_tool_call.py",
        "keyword": "diff",
        "snippet": "# - **Ulysses** - Joyce's masterpiece and one of the most important novels of the 20th century\n# - **A Portrait of the Artist as a Young Man** - A semi-autobiographical novel following Stephen Dedalus\n# - **Dubliners** - A collection of short stories depicting life in Dublin (appears twice in different editions)\n\n# **Other Works:**\n# - **Chamber Music** - A collection of Joyce's early poetry (also appears in multiple editions)\n# - **Exiles: A Play in Three Acts** - Joyce's only surviving play\n# "
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter7_sub_agent/src/tools/cmd_runner.py",
        "keyword": "replace",
        "snippet": "4. Edit File Command:\n   - Before updating any file, you must read the file content in the current conversation first\n   - When updating content in an existing file, always use sed for replacement except when the updated content is very large:\n     Example: `sed -i \"start_line,end_line c new_content\" <file>`\n   - When inserting content into an existing file, use sed for insertion:\n     Example: `sed -i \"insert_line_number a insert_content\" <file>`\n   - When searching within files, always use gre"
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter6_to_do_write/src/tools/cmd_runner.py",
        "keyword": "replace",
        "snippet": "4. Edit File Command:\n   - Before updating any file, you must read the file content in the current conversation first\n   - When updating content in an existing file, always use sed for replacement except when the updated content is very large:\n     Example: `sed -i \"start_line,end_line c new_content\" <file>`\n   - When inserting content into an existing file, use sed for insertion:\n     Example: `sed -i \"insert_line_number a insert_content\" <file>`\n   - When searching within files, always use gre"
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter6_to_do_write/src/core/prompt/system_rule.py",
        "keyword": "modify",
        "snippet": "You are an interactive CLI tool that helps users with software engineering tasks. Use the instructions below and the tools available to you to assist the user.\n\nIMPORTANT: Assist with defensive security tasks only. Refuse to create, modify, or improve code that may be used maliciously. Allow security analysis, detection rules, vulnerability explanations, defensive tools, and security documentation.\nIMPORTANT: You must NEVER generate or guess URLs for the user unless you are confident that the UR"
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter7_sub_agent/src/core/prompt/system_rule.py",
        "keyword": "modify",
        "snippet": "You are an interactive CLI tool that helps users with software engineering tasks. Use the instructions below and the tools available to you to assist the user.\n\nIMPORTANT: Assist with defensive security tasks only. Refuse to create, modify, or improve code that may be used maliciously. Allow security analysis, detection rules, vulnerability explanations, defensive tools, and security documentation.\nIMPORTANT: You must NEVER generate or guess URLs for the user unless you are confident that the UR"
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter6_to_do_write/src/core/conversation.py",
        "keyword": "refactor",
        "snippet": "\"\"\"\nConversation management with refactored UI components and history manager integration.\n\"\"\"\n\nimport json\nimport traceback\nfrom core.api_client import APIClient\n"
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter8_mcp_client/src/core/conversation.py",
        "keyword": "refactor",
        "snippet": "\"\"\"\nConversation management with refactored UI components and history manager integration.\n\"\"\"\n\nimport json\nimport sys\nimport traceback\n"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/tests/test_edit_tool.py",
        "keyword": "edit",
        "snippet": "from pathlib import Path\nimport tempfile\nfrom tools.edit import tool_function\n\n@pytest.fixture\ndef temp_dir():\n    \"\"\"Create a temporary directory for test files.\"\"\"\n    with tempfile.TemporaryDirectory() as tmpdirname:\n--\n        assert \"already exists\" in result\n\n    def test_edit_file(self, sample_file):\n        \"\"\"Test editing an existing file.\"\"\"\n        new_content = \"edited content\\nnew line\"\n        result = tool_function(\"edit\", str(sample_file), file_text=new_content)\n        assert \"h"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/coding_agent.py",
        "keyword": "edit",
        "snippet": "            f.write('')\n\n    def get_current_edits(self):\n        diff = str(diff_versus_commit(self.git_tempdir, self.base_commit))\n        return diff\n\n    def get_regression_tests(self):\n        \"\"\"\n--\n        Run the regression tests and get the test report.\n        \"\"\"\n        code_diff = self.get_current_edits()\n        instruction = f\"\"\"I have uploaded a Python code repository in the directory {self.git_tempdir}. There is an attempt to address the problem statement. Please review the chan"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/analysis/visualize_archive.py",
        "keyword": "patch",
        "snippet": "import plotly.graph_objects as go\n\nfrom utils.evo_utils import get_model_patch_paths, load_dgm_metadata\n\n\nclass EvalQuantity:\n    SMALL = \"small\"\n    MED = \"med\"\n--\n    \"\"\"\n    root_dir = os.path.abspath(\"./\")  # should be the root of this repo\n    patch_paths = get_model_patch_paths(root_dir, dgm_dir, node_id)\n    model_patch_paths = f\"\\\"{','.join(patch_paths)}\\\"\"\n    model_name_or_path = f\"dgmnode_{node_id}\"\n    command = (\n        \"python test_swebench.py --full_eval --num_samples 50 \"\n      "
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/polyglot/harness.py",
        "keyword": "patch",
        "snippet": "from polyglot.docker_build import build_env_images, build_container, cleanup_container\nfrom polyglot.constants import MAP_REPO_VERSION_TO_SPECS, TEST_COMMANDS\nfrom utils.git_utils import filter_patch_by_files, remove_patch_by_files\n\nfrom swe_bench.utils import (\n    copy_to_container,\n    copy_from_container,\n    log_container_output,\n--\n    return \"\\n\".join([\"#!/bin/bash\", \"set -uxo pipefail\"] + commands) + \"\\n\"\n\ndef process_entry(entry, out_dname, model_name_or_path, model_patch_paths):\n    \"\""
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/polyglot/harness.py",
        "keyword": "diff",
        "snippet": "        # Get model_patch\n        logger.info(\"Getting model_patch\")\n        exec_result = container.exec_run(\"cat /dgm/model_patch.diff\")\n        log_container_output(exec_result)\n        model_patch = ''\n        model_patch = exec_result.output.decode()\n\n        # Additional proposed model patches\n"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/prompts/self_improvement_prompt.py",
        "keyword": "diff",
        "snippet": "\n- **Additional Details**:\n  - The coding agent trying to solve a programming task. A task is in one programming language, but the coding agent needs to deal with different languages including C++, Go, Java, JavaScript, Python, and Rust.\n  - The agent is very good at automatically utilizing the right available tools at the right time. So do not have an agentic flow that explicitly forces a tool's usage.\n  - Be detailed in the prompt about what steps (e.g. implementing tests, refining solutions, "
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/analysis/visualize_archive.py",
        "keyword": "replace",
        "snippet": "    fig.write_html(html_path)\n    print(f\"Saved interactive visualization to {html_path}\")\n    svg_path = html_path.replace('.html', '.svg')\n    if 'halluc' in colorbar_title.lower():\n        fig.write_image(svg_path, width=2200, height=800)\n    else:\n        fig.write_image(svg_path, width=1100, height=800)\n    print(f\"Saved static visualization to {svg_path}\")\n"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/prompts/self_improvement_prompt.py",
        "keyword": "replace",
        "snippet": "                # Handle polyglot case\n                if is_polyglot and 'coding_agent.py' in file_path:\n                    full_path = full_path.replace('coding_agent.py', f'coding_agent_polyglot.py')\n                \n                code_text.append(f\"# {rel_path}\")\n                code_text.append(read_file(full_path))\n\n        elif os.path.isdir(full_path):\n"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/prompts/self_improvement_prompt.py",
        "keyword": "modify",
        "snippet": "- \"potential_improvements\": Identify potential improvements to the coding agent that could enhance its coding capabilities. Focus on the agent's general coding abilities (e.g., better or new tools usable across any repository) rather than issue-specific fixes (e.g., tools only usable in one framework). All necessary dependencies and environment setup have already been handled, so do not focus on these aspects.\n- \"improvement_proposal\": Choose ONE high-impact improvement from the identified poten"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/polyglot/harness.py",
        "keyword": "modify",
        "snippet": "    if model_patch_paths:\n        for model_patch_path in model_patch_paths:\n            # Read and modify model patch\n            with open(model_patch_path, 'r') as f:\n                patch_content = f.read()\n            patch_content = remove_patch_by_files(patch_content)\n            # Placeholder for any patch modifications if needed\n            with open(model_patch_path, 'w') as f:\n"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/polyglot/benchmark.py",
        "keyword": "refactor",
        "snippet": "import typer\nfrom dotenv import load_dotenv\nfrom plots import plot_refactoring\nfrom rich.console import Console\n\nfrom aider import models, sendchat\nfrom aider.coders import Coder, base_coder\nfrom aider.dump import dump  # noqa: F401\n--\n        # plot_outcomes(df, repeats, repeat_hi, repeat_lo, repeat_avg)\n        # plot_outcomes_claude(df)\n        plot_refactoring(df)\n\n\ndef resolve_dirname(dirname, use_single_prior, make_new):\n    if len(dirname.parts) > 1:\n        return dirname\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/scripts/history_prompts.py",
        "keyword": "edit",
        "snippet": "\nOnly add new items not already listed in the history markdown.\nDo NOT edit or update existing history entries.\nDo NOT add duplicate entries for changes that have existing history entries.\nDo NOT add additional entries for small tweaks to features which are already listed in the existing history.\n\nPay attention to see if changes are later modified or superseded in the commit logs.\nThe history doc should only reflect the *final* version of changes which have evolved within a version's commit hist"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/benchmark/rungrid.py",
        "keyword": "edit",
        "snippet": "        # \"gpt-4-0613\",\n    ]\n    edit_formats = [\n        \"diff\",\n        # \"diff-func\",\n        # \"whole\",\n        # \"whole-func\",\n    ]\n--\n    # for repeat in range(1, 2, 1):\n    for model in models:\n        for edit_format in edit_formats:\n            # dump(model, edit_format)\n\n            if \"-func\" in edit_format and \"-03\" in model:\n                continue\n\n            # if (model, edit_format) == (\"gpt-3.5-turbo-16k-0613\", \"whole-func\"):\n            #    # sublist reliably hangs the API"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/scripts/issues.py",
        "keyword": "patch",
        "snippet": "\n    # Close issue\n    response = requests.patch(close_url, headers=headers, json={\"state\": \"closed\"})\n    response.raise_for_status()\n\n    print(f\"  - Commented and closed issue #{issue['number']}\")\n\n\n--\n    for issue in unlabeled_issues:\n        url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}\"\n        response = requests.patch(url, headers=headers, json={\"labels\": [\"question\"]})\n        response.raise_for_status()\n        print(f\"  - Added 'question' label to #"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/tests/browser/test_browser.py",
        "keyword": "patch",
        "snippet": "import os\nimport unittest\nfrom unittest.mock import patch\n\nfrom aider.main import main\n\n\nclass TestBrowser(unittest.TestCase):\n    @patch(\"aider.main.launch_gui\")\n    def test_browser_flag_imports_streamlit(self, mock_launch_gui):\n        os.environ[\"AIDER_ANALYTICS\"] = \"false\"\n\n        # Run main with --browser and --yes flags\n        main([\"--browser\", \"--yes\"])\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/scripts/clean_metadata.py",
        "keyword": "diff",
        "snippet": "#!/usr/bin/env python\n\nimport difflib\nimport json\nimport re\nfrom pathlib import Path\n\nimport json5\n--\n                print(\"\\n\" + \"=\" * 40)\n                print(\"-\" * 40 + \"\\n\")  # Separator for the next model\n                continue  # Skip diff and removal prompt for identical entries\n\n            # Generate unified diff\n            # If dictionaries differ, generate JSON strings to show the diff\n            # Add a dummy key to ensure the *real* last key gets a comma\n            litellm_en"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/scripts/history_prompts.py",
        "keyword": "diff",
        "snippet": "history_prompt = \"\"\"\nUpdate the history markdown doc with changes shown in the diffs.\nSuccinctly describe actual user-facing changes, not every single commit or detail that was made implementing them.\n\nOnly add new items not already listed in the history markdown.\nDo NOT edit or update existing history entries.\nDo NOT add duplicate entries for changes that have existing history entries.\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/scripts/recording_audio.py",
        "keyword": "replace",
        "snippet": "\n                        # Replace original with compressed version\n                        os.replace(temp_file, file_path)\n                        print(\n                            f\"  \u2713 Compressed: {original_size} \u2192 {compressed_size} bytes\"\n                            f\" ({reduction:.1f}% reduction)\"\n                        )\n                    else:\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/aider/repo.py",
        "keyword": "replace",
        "snippet": "                args = [\"HEAD\", \"--\"] + list(fnames)\n                diffs += self.repo.git.diff(*args, stdout_as_string=False).decode(\n                    self.io.encoding, \"replace\"\n                )\n                return diffs\n\n            wd_args = [\"--\"] + list(fnames)\n            index_args = [\"--cached\"] + wd_args\n\n            diffs += self.repo.git.diff(*index_args, stdout_as_string=False).decode(\n                self.io.encoding, \"replace\"\n            )\n            diffs += self.repo.g"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/scripts/recording_audio.py",
        "keyword": "modify",
        "snippet": "    args = parser.parse_args()\n\n    # Use args.voice directly instead of modifying global VOICE\n    selected_voice = args.voice\n    selected_bitrate = args.bitrate\n\n    # Check if FFmpeg is available for compression\n    if not check_ffmpeg() and not args.dry_run:\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/scripts/logo_svg.py",
        "keyword": "modify",
        "snippet": "\n    # Calculate SVG dimensions based on text length\n    # These values can be adjusted to modify the appearance\n    char_width = 40\n    width = len(text) * char_width\n    height = 60\n    text_x = width / 2  # Center point of the SVG width\n    text_y = height * 0.62  # Center point of the SVG height\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/benchmark/plots.py",
        "keyword": "refactor",
        "snippet": "\n\ndef plot_refactoring(df):\n    tries = [df.groupby([\"model\", \"edit_format\"])[\"pass_rate_1\"].mean()]\n\n    plt.rcParams[\"hatch.linewidth\"] = 0.5\n    plt.rcParams[\"hatch.color\"] = \"#444444\"\n\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/benchmark/benchmark.py",
        "keyword": "refactor",
        "snippet": "import typer\nfrom dotenv import load_dotenv\nfrom plots import plot_refactoring\nfrom rich.console import Console\n\nfrom aider import models, sendchat\nfrom aider.coders import Coder, base_coder\nfrom aider.dump import dump  # noqa: F401\n--\n        # plot_outcomes(df, repeats, repeat_hi, repeat_lo, repeat_avg)\n        # plot_outcomes_claude(df)\n        plot_refactoring(df)\n\n\ndef resolve_dirname(dirname, use_single_prior, make_new):\n    if len(dirname.parts) > 1:\n        return dirname\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/aider/coders/search_replace.py",
        "keyword": "search_replace",
        "snippet": "    if debug:\n        html = dmp.diff_prettyHtml(diff)\n        Path(\"tmp.search_replace_diff.html\").write_text(html)\n\n        for d in diff:\n            print(d[0], repr(d[1]))\n\n        for patch in patches:\n--\n        # dump(diff)\n        html = dmp.diff_prettyHtml(diff)\n        Path(\"tmp.search_replace_diff.html\").write_text(html)\n\n        for d in diff:\n            print(d[0], repr(d[1]))\n\n    new_lines, success = dmp.patch_apply(patches, original_lines)\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/aider/coders/udiff_coder.py",
        "keyword": "search_replace",
        "snippet": "from ..dump import dump  # noqa: F401\nfrom .base_coder import Coder\nfrom .search_replace import (\n    SearchTextNotUnique,\n    all_preprocs,\n    diff_lines,\n    flexible_search_and_replace,\n    search_and_replace,\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/aider/models.py",
        "keyword": "fuzzy_match",
        "snippet": "        )\n\n        possible_matches = fuzzy_match_models(model.name)\n        if possible_matches:\n            io.tool_output(\"Did you mean one of these?\")\n            for match in possible_matches:\n                io.tool_output(f\"- {match}\")\n\n--\n\n\ndef fuzzy_match_models(name):\n    name = name.lower()\n\n    chat_models = set()\n    model_metadata = list(litellm.model_cost.items())\n    model_metadata += list(model_info_manager.local_model_metadata.items())\n--\n\ndef print_matching_models(io, search):"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/tests/basic/test_main.py",
        "keyword": "fuzzy_match",
        "snippet": "                }\n\n                # Mock fuzzy_match_models to avoid string operations on MagicMock\n                with patch(\"aider.models.fuzzy_match_models\", return_value=[]):\n                    main(\n                        [\"--no-verify-ssl\", \"--exit\", \"--yes\"],\n                        input=DummyInput(),\n                        output=DummyOutput(),\n                    )\n"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/docs/src/conf.py",
        "keyword": "edit",
        "snippet": "    app: Sphinx, pagename: str, templatename: str, context, doctree\n) -> None:\n    \"\"\"Add a function that jinja can access for returning an \"edit this page\" link pointing to `main`.\"\"\"\n\n    def to_main(link: str) -> str:\n        \"\"\"Transform \"edit on github\" links and make sure they always point to the main branch.\n\n        Args:\n            link: the link to the github edit interface\n\n        Returns:\n            the link to the tip of the main branch for the same file\n        \"\"\"\n        links"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/samples/core_xlang_hello_python_agent/protos/agent_events_pb2.pyi",
        "keyword": "edit",
        "snippet": "\"\"\"\n@generated by mypy-protobuf.  Do not edit manually!\nisort:skip_file\n\"\"\"\n\nimport builtins\nimport google.protobuf.descriptor\n"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-ext/src/autogen_ext/tools/http/_http_tool.py",
        "keyword": "patch",
        "snippet": "                    response = await client.delete(url, headers=self.server_params.headers, params=model_dump)\n                case \"PATCH\":\n                    response = await client.patch(url, headers=self.server_params.headers, json=model_dump)\n                case _:  # Default case POST\n                    response = await client.post(url, headers=self.server_params.headers, json=model_dump)\n\n        match self.server_params.return_type:\n            case \"text\":\n"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-agentchat/tests/test_society_of_mind_agent.py",
        "keyword": "patch",
        "snippet": "@pytest.mark.asyncio\nasync def test_society_of_mind_agent_no_multiple_system_messages(\n    monkeypatch: pytest.MonkeyPatch, runtime: AgentRuntime | None\n) -> None:\n    model_client = ReplayChatCompletionClient([\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"])\n\n    model_client_soma = ReplayChatCompletionClient(\n        [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"],\n--\n\n    # bind it\n    monkeypatch.setattr(model_client_soma, \"create\", MethodType(_mock_create, model_client_soma))\n\n    ag"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/samples/task_centric_memory/eval_self_teaching.py",
        "keyword": "diff",
        "snippet": "the agent iterates through a background learning loop to find a solution, which it then stores as an insight in memory.\nFinally the agent is tested again to see if it can retrieve and apply its insight to the original task,\nas well as to a similar but different task as a test of generalization.\n\nIf adapting this sample code to a new setting, the Apprentice class can be used or completely replaced by other code.\n\"\"\"\n\n\n--\n    expected_answer_1 = task_dict_1[\"expected_answer\"]\n\n    # Test generaliz"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/samples/task_centric_memory/eval_learning_from_demonstration.py",
        "keyword": "diff",
        "snippet": "\n1. The function below asks the agent to perform a reasoning task (ten times) on which it usually fails.\n2. Then agent is then given one demonstration of how to solve a similar but different task, and the context window is cleared.\n3. Finally the agent is tested 10 more times to see if it can retrieve and apply the demonstration to the original task.\n\nIf adapting this sample code to a new setting, the Apprentice class can be used or completely replaced by other code.\n\"\"\"\n\n--\n    logger.info(\"\\n\""
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/docs/src/generate_api_reference.py",
        "keyword": "replace",
        "snippet": "        if package_dir.is_dir():\n            # Check if this package is in our documented packages list\n            package_name = package_dir.name.replace(\"-\", \"_\")\n            if package_name in DOCUMENTED_PACKAGES:\n                src_dir = package_dir / \"src\"\n                if src_dir.exists():\n                    python_packages.append(src_dir)\n    \n--\n                # Generate .rst content with proper title formatting\n                # Title should use dots as separators, but escape unde"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/fixup_generated_files.py",
        "keyword": "replace",
        "snippet": "        print(\"Fixing imports in file:\", file)\n        for old, new in substitutions.items():\n            content = content.replace(old, new)\n\n        with open(file, \"w\") as f:\n            f.write(content)\n"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-ext/conftest.py",
        "keyword": "modify",
        "snippet": "    )\n\ndef pytest_collection_modifyitems(config, items):\n    grpc_option_passed = config.getoption(\"--grpc\")\n    skip_grpc = pytest.mark.skip(reason=\"Need --grpc option to run\")\n    skip_non_grpc = pytest.mark.skip(reason=\"Skipped since --grpc passed\")\n\n    for item in items:\n"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/agbench/src/agbench/remove_missing_cmd.py",
        "keyword": "modify",
        "snippet": "    if not parsed_args.noconfirm:\n        input(\n            \"Did you modify the default_scorer function to match the expected ending pattern? Press Enter to continue...\"\n        )\n\n    delete_folders_with_missing_results(parsed_args.runlogs, parsed_args.noconfirm)\n\n\n--\n        print(f\"Error: '{runlogs_path}' is not a valid directory.\")\n        sys.exit(1)\n    input(\"Did you modify the default_scorer function to match the expected ending pattern? Press Enter to continue...\")\n\n    delete_folders_"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-studio/tests/mcp/test_mcp_websocket.py",
        "keyword": "refactor",
        "snippet": "\"\"\"\nUpdated tests for MCP WebSocket functionality using the new refactored architecture.\nThese tests replace the failing legacy tests in test_mcp_websocket.py.\n\"\"\"\n\nimport json\nimport base64\n"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-studio/tests/mcp/test_mcp_callbacks.py",
        "keyword": "refactor",
        "snippet": "#!/usr/bin/env python3\n\"\"\"Test the refactored MCP callback functions\"\"\"\n\nimport asyncio\nimport pytest\nimport uuid\nfrom unittest.mock import AsyncMock, MagicMock, patch\n"
      }
    ],
    "task_planning": [
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/cli/helpers.py",
        "keyword": "plan",
        "snippet": "                    if \"user_feedback\" in si and si[\"user_feedback\"] is not None:\n                        convo_el[\"user_feedback\"] = si[\"user_feedback\"]\n                    if \"initial_explanation\" in si and si[\"initial_explanation\"] is not None:\n                        convo_el[\"initial_explanation\"] = si[\"initial_explanation\"]\n\n        convo_el[\"action\"] = state.action\n        convo.append(convo_el)\n\n    return convo\n"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/agents/architect.py",
        "keyword": "plan",
        "snippet": "            self.prepare_example_project(spec)\n        else:\n            await self.plan_architecture(spec)\n\n        await self.check_system_dependencies(spec)\n\n        self.next_state.specification = spec\n        telemetry.set(\"templates\", spec.templates)\n--\n        return tpl.architecture, templates\n\n    async def plan_architecture(self, spec: Specification):\n        await self.send_message(\"Planning project architecture ...\")\n        architecture_description, templates = await self.select_tem"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/cli/helpers.py",
        "keyword": "steps",
        "snippet": "            elif state.action == CM_UPDATE_FILES:\n                files_dict = {}\n                for steps in state.steps:\n                    if \"save_file\" in steps and \"path\" in steps[\"save_file\"]:\n                        path = steps[\"save_file\"][\"path\"]\n\n                        current_file = await sm.get_file_for_project(state.id, path)\n                        prev_file = (\n                            await sm.get_file_for_project(state.prev_state_id, path)\n                            if "
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/telemetry/__init__.py",
        "keyword": "steps",
        "snippet": "                # Number of tokens used for LLM requests\n                \"num_llm_tokens\": 0,\n                # Number of development steps\n                \"num_steps\": 0,\n                # Number of commands run during development\n                \"num_commands\": 0,\n                # Number of times a human input was required during development\n                \"num_inputs\": 0,\n                # Number of files in the project\n"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/proc/exec_log.py",
        "keyword": "execute",
        "snippet": "    started_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))\n    duration: float = Field(description=\"The duration of the command/process run in seconds\")\n    cmd: str = Field(description=\"The full command (as executed in the shell)\")\n    cwd: str = Field(description=\"The working directory for the command (relative to project root)\")\n    env: dict = Field(description=\"The environment variables for the command\")\n    timeout: Optional[float] = Field(description=\"The command"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/ui/api_server.py",
        "keyword": "execute",
        "snippet": "            # Check if convo exists for this convo id\n            query = select(ChatConvo).where(ChatConvo.convo_id == self.convo_id)\n            result = await session.execute(query)\n            chat_convo = result.scalar_one_or_none()\n\n            # Create new convo if needed\n            if not chat_convo:\n                chat_convo = ChatConvo(convo_id=self.convo_id, project_state_id=project_state_id)\n--\n            # Check if any ChatMessages exists with the convo id\n            query = sel"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/aider/prompts.py",
        "keyword": "plan",
        "snippet": "- Does not exceed 72 characters.\n\nReply only with the one-line commit message, without any additional text, explanations, or line breaks.\n\"\"\"\n\n# COMMANDS\nundo_command_reply = (\n    \"I did `git reset --hard HEAD~1` to discard the last edits. Please wait for further\"\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/aider/coders/single_wholefile_func_coder.py",
        "keyword": "plan",
        "snippet": "                type=\"object\",\n                properties=dict(\n                    explanation=dict(\n                        type=\"string\",\n                        description=(\n                            \"Step by step plan for the changes to be made to the code (future\"\n                            \" tense, markdown format)\"\n                        ),\n                    ),\n                    content=dict(\n                        type=\"string\",\n--\n                    ),\n                ),\n   "
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/aider/scrape.py",
        "keyword": "decompose",
        "snippet": "def slimdown_html(soup):\n    for svg in soup.find_all(\"svg\"):\n        svg.decompose()\n\n    if soup.img:\n        soup.img.decompose()\n\n    for tag in soup.find_all(href=lambda x: x and x.startswith(\"data:\")):\n        tag.decompose()\n\n    for tag in soup.find_all(src=lambda x: x and x.startswith(\"data:\")):\n        tag.decompose()\n\n    for tag in soup.find_all(True):\n        for attr in list(tag.attrs):\n            if attr != \"href\":\n                tag.attrs.pop(attr, None)\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/scripts/issues.py",
        "keyword": "steps",
        "snippet": "    \"\"\"3 weeks. This issue should be resolved in recent versions of aider.\\n\\n\"\"\"\n    \"\"\"If you find that this bug is still present, please feel free to reopen this \"\"\"\n    \"\"\"issue or create a new one with steps to reproduce.\"\"\" + BOT_SUFFIX\n)\n\n# GitHub API configuration\nGITHUB_API_URL = \"https://api.github.com\"\nREPO_OWNER = \"Aider-AI\"\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/scripts/blame.py",
        "keyword": "workflow",
        "snippet": "        for f in files\n        if f.endswith((\".js\", \".py\", \".scm\", \".sh\", \"Dockerfile\", \"Gemfile\"))\n        or (f.startswith(\".github/workflows/\") and f.endswith(\".yml\"))\n        or (f.startswith(\"aider/resources/\") and f.endswith(\".yml\"))\n        or f in website_files\n        or f in test_files\n    ]\n    files = [f for f in files if not f.endswith(\"prompts.py\")]\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/aider/special.py",
        "keyword": "workflow",
        "snippet": "    normalized_path = os.path.normpath(file_path)\n\n    # Check for GitHub Actions workflow files\n    if dir_name == os.path.normpath(\".github/workflows\") and file_name.endswith(\".yml\"):\n        return True\n\n    return normalized_path in NORMALIZED_ROOT_IMPORTANT_FILES\n\n\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/tests/help/test_help.py",
        "keyword": "execute",
        "snippet": "\n        Args:\n            func: Function to execute\n            max_time: Maximum time in seconds to keep retrying\n            initial_delay: Initial delay between retries in seconds\n            backoff_factor: Multiplier for delay after each retry\n\n        Returns:\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/aider/linter.py",
        "keyword": "execute",
        "snippet": "            )\n        except OSError as err:\n            print(f\"Unable to execute lint command: {err}\")\n            return\n        errors = stdout\n        if returncode == 0:\n            return  # zero exit status\n\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/aider/special.py",
        "keyword": "pipeline",
        "snippet": "    \".gitlab-ci.yml\",\n    \"Jenkinsfile\",\n    \"azure-pipelines.yml\",\n    \"bitbucket-pipelines.yml\",\n    \"appveyor.yml\",\n    \"circle.yml\",\n    \".circleci/config.yml\",\n    \".github/dependabot.yml\",\n    \"codecov.yml\",\n"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/capabilities/summarization/evaluation/custom_evals/llm_eval.py",
        "keyword": "plan",
        "snippet": "    3. Completeness (1-5)\n    4. Clarity (1-5)\n    5. Explanation - a general description of the way the summary is evaluatied\n\n    Here are some things to think about as you go about grading.\n\n    1. Does the summary accurately capture the key provisions of the legal document?\n    2. Does the summary omit any important details from the legal document?\n--\n    \"completeness\": <number>,\n    \"clarity\": <number>,\n    \"explanation\": <string>,\n    }}\n    </json>\n\n    Original Text: {input}\n\n--\n    num"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/capabilities/classification/evaluation/prompts.py",
        "keyword": "plan",
        "snippet": "</category>\n<category>\n    <label>Coverage Explanations</label>\n    <content> Questions about what is covered under specific policy types Requests for clarification on coverage limits and exclusions Inquiries about deductibles and out-of-pocket expenses\n    </content>\n</category>\n<category>\n    <label>Quotes and Proposals</label>\n"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/claude_agent_sdk/observability_agent/agent.py",
        "keyword": "steps",
        "snippet": "GitHub repositories and CI/CD workflows. Provide concise, actionable insights \\\nsuitable for on-call engineers. Focus on identifying issues, assessing severity, \\\nand recommending next steps.\"\"\"\n\n\ndef get_github_mcp_server() -> dict[str, McpServerConfig]:\n    \"\"\"\n    Get the GitHub MCP server configuration.\n"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/capabilities/text_to_sql/evaluation/prompts.py",
        "keyword": "steps",
        "snippet": "    </query>\n\n    First, provide your thought process within <thought_process> tags, explaining how you'll approach creating the SQL query. Consider the following steps:\n    1. Identify the relevant tables and columns from the provided schema.\n    2. Determine any necessary joins between tables.\n    3. Identify any filtering conditions.\n    4. Decide on the appropriate aggregations or calculations.\n    5. Structure the query logically.\n"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/claude_agent_sdk/observability_agent/agent.py",
        "keyword": "workflow",
        "snippet": "\nThis agent demonstrates MCP (Model Context Protocol) integration for GitHub\nmonitoring and CI/CD workflow analysis. It uses the official GitHub MCP server\nto interact with the GitHub API.\n\nKey design decisions:\n- Uses disallowed_tools to ensure MCP tools are used (not Bash with gh CLI)\n- Focused on read-only GitHub operations for observability\n--\n# System prompt optimized for observability tasks\nDEFAULT_SYSTEM_PROMPT = \"\"\"You are an observability agent specialized in monitoring \\\nGitHub reposit"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/tool_use/utils/customer_service_api.py",
        "keyword": "workflow",
        "snippet": "                \"I upgraded to the Pro plan to access the {feature} feature, but I can't figure out how to use it. Is there documentation or a tutorial available?\",\n                \"I would love to see a dark mode option. I use the app late at night and the bright interface strains my eyes. This would be a great addition.\",\n                \"Is there a mobile app for iOS/Android? I can only find the web version and it's not very mobile-friendly. Would really help my workflow.\",\n                \"W"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/scripts/validate_all_notebooks.py",
        "keyword": "execute",
        "snippet": "        if mode == \"full\" and result[\"status\"] != \"error\":\n            if os.environ.get(\"ANTHROPIC_API_KEY\"):\n                exec_result = self.execute_notebook(notebook_path)\n                if not exec_result[\"success\"]:\n                    result[\"status\"] = \"error\"\n                    result[\"issues\"].append(\n                        {\n                            \"type\": \"execution_failure\",\n--\n        return result\n\n    def execute_notebook(self, notebook_path: Path) -> dict:\n        \"\"\"Ex"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/tool_use/memory_tool.py",
        "keyword": "execute",
        "snippet": "        return full_path\n\n    def execute(self, **params: Any) -> dict[str, str]:\n        \"\"\"\n        Execute a memory tool command.\n\n        Args:\n            **params: Command parameters from Claude's tool use\n"
      },
      {
        "repo": "devika",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/devika/src/agents/__init__.py",
        "keyword": "plan",
        "snippet": "from .agent import Agent\n\nfrom .planner import Planner\nfrom .internal_monologue import InternalMonologue\nfrom .researcher import Researcher\nfrom .formatter import Formatter\nfrom .coder import Coder\nfrom .action import Action\n"
      },
      {
        "repo": "devika",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/devika/src/agents/coder/coder.py",
        "keyword": "plan",
        "snippet": "\n    def render(\n        self, step_by_step_plan: str, user_context: str, search_results: dict\n    ) -> str:\n        env = Environment(loader=BaseLoader())\n        template = env.from_string(PROMPT)\n        return template.render(\n            step_by_step_plan=step_by_step_plan,\n            user_context=user_context,\n            search_results=search_results,\n        )\n\n    def validate_response(self, response: str) -> Union[List[Dict[str, str]], bool]:\n--\n    def execute(\n        self,\n        "
      },
      {
        "repo": "devika",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/devika/devika.py",
        "keyword": "execute",
        "snippet": "    state = AgentState.get_latest_state(project_name)\n    if not state:\n        thread = Thread(target=lambda: agent.execute(message, project_name))\n        thread.start()\n    else:\n        if AgentState.is_agent_completed(project_name):\n            thread = Thread(target=lambda: agent.subsequent_execute(message, project_name))\n            thread.start()\n        else:\n            emit_agent(\"info\", {\"type\": \"warning\", \"message\": \"previous agent doesn't completed it's task.\"})\n            last_st"
      },
      {
        "repo": "devika",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/devika/src/agents/internal_monologue/internal_monologue.py",
        "keyword": "execute",
        "snippet": "\n    @retry_wrapper\n    def execute(self, current_prompt: str, project_name: str) -> str:\n        rendered_prompt = self.render(current_prompt)\n        response = self.llm.inference(rendered_prompt, project_name)\n        valid_response = self.validate_response(response)\n        return valid_response\n\n"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/autogpt/core/planning/__init__.py",
        "keyword": "plan",
        "snippet": "\"\"\"The planning system organizes the Agent's activities.\"\"\"\nfrom autogpt.core.planning.schema import (\n    LanguageModelClassification,\n    LanguageModelPrompt,\n    LanguageModelResponse,\n    Task,\n    TaskStatus,\n    TaskType,\n)\nfrom autogpt.core.planning.simple import PlannerSettings, SimplePlanner\n"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/autogpt/core/planning/templates.py",
        "keyword": "plan",
        "snippet": "        \"text\": \"thought\",\n        \"reasoning\": \"reasoning\",\n        \"plan\": \"- short bulleted\\n- list that conveys\\n- long-term plan\",\n        \"criticism\": \"constructive self-criticism\",\n        \"speak\": \"thoughts summary to say to user\",\n    },\n    \"command\": {\"name\": \"command name\", \"args\": {\"arg name\": \"value\"}},\n}\n"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/autogpt/core/planning/templates.py",
        "keyword": "steps",
        "snippet": "    \"Reflect on past decisions and strategies to refine your approach.\",\n    \"Every command has a cost, so be smart and efficient. Aim to complete tasks in\"\n    \" the least number of steps.\",\n    \"Write all code to a file\",\n)\n\n\nPLAN_PROMPT_RESPONSE_DICT = {\n"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/autogpt/agents/base.py",
        "keyword": "steps",
        "snippet": "        RESPONSE_FORMAT_WITH_COMMAND = \"\"\"```ts\n        interface Response {\n            // Express your thoughts based on the information that you have collected so far, the possible steps that you could do next and also your reasoning about fixing the bug in question\"\n            thoughts: string;\n            command: {\n                name: string;\n                args: Record<string, any>;\n            };\n"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/python_server.py",
        "keyword": "execute",
        "snippet": "app = Flask(__name__)\n\ndef execute_tests(path):\n    # Implement your test logic here\n    print(\"PATH:::::::\", path)\n    exec_results = os.system(\"./execute_tests.sh {} > test_results_temp\".format(path))\n    print(exec_results)\n    return exec_results\n\n@app.route('/read_file', methods=['GET'])\ndef read_file():\n--\n        return jsonify({\"error\": str(e)}), 500\n\n@app.route('/execute_tests', methods=['POST'])\ndef execute_tests_endpoint():\n    data = request.get_json()\n    path = data.get('path')\n   "
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/construct_commands_descriptions.py",
        "keyword": "execute",
        "snippet": "write_fix_desc = \"\"\"write_fix: Use this command to implement the fix you came up with. The test cases are run automatically after writing the changes. The changes are reverted automatically if the the test cases fail. This command requires the following params: (project_name: string, bug_index: integer, changes_dicts:list[dict]) where changes_dict is a list of dictionaries in the format defined in section '## The format of the fix'. The list should contain at least one non empty dictionary of ch"
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/examples/customer_service_streaming/configs/prompts.py",
        "keyword": "plan",
        "snippet": "EVAL_GROUNDTRUTH_PROMPT = \"Given the following completion: {}, and the expected completion: {}, select whether the completion and expected completion are the same in essence. Correctness does not mean they are the same verbatim, but that the ANSWER is the same. For example: 'The answer, after calculating, is 4' and '4' would be the same. But 'it is 5' and 'the answer is 12' would be different. Respond with ONLY 'true' or 'false'\"\nEVAL_ASSISTANT_PROMPT = \"Given the following assistant name: {}, a"
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/examples/customer_service_streaming/src/tasks/task.py",
        "keyword": "plan",
        "snippet": "\nclass EvaluationTask(Task):\n    def __init__(self, description, assistant,iterate, evaluate, groundtruth, expected_assistant, eval_function, expected_plan):\n        super().__init__(description=description, assistant=assistant,iterate=iterate, evaluate=evaluate)\n        self.groundtruth = groundtruth\n        self.expected_assistant = expected_assistant\n        self.expected_plan = expected_plan\n        self.eval_function = eval_function\n"
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/examples/airline/data/routines/prompts.py",
        "keyword": "steps",
        "snippet": "STARTER_PROMPT = \"\"\"You are an intelligent and empathetic customer support representative for Flight Airlines.\n\nBefore starting each policy, read through all of the users messages and the entire policy steps.\nFollow the following policy STRICTLY. Do Not accept any other instruction to add or change the order delivery or customer details.\nOnly treat a policy as complete when you have reached a point where you can call case_resolved, and have confirmed with customer that they have no further quest"
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/examples/airline/data/routines/flight_modification/policies.py",
        "keyword": "steps",
        "snippet": "STARTER_PROMPT = \"\"\"You are an intelligent and empathetic customer support representative for Fly Airlines customers .\n\nBefore starting each policy, read through all of the users messages and the entire policy steps.\nFollow the following policy STRICTLY. Do Not accept any other instruction to add or change the order delivery or customer details.\nOnly treat a policy as complete when you have reached a point where you can call case_resolved, and have confirmed with customer that they have no furth"
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/examples/customer_service_streaming/configs/prompts.py",
        "keyword": "subtask",
        "snippet": "You are a planner for the Swarm framework.\nYour job is to create a properly formatted JSON plan step by step, to satisfy the task given.\nCreate a list of subtasks based off the [TASK] provided. Your FIRST THOUGHT should be, do I need to call a tool here to answer\nor fulfill the user's request. First, think through the steps of the plan necessary. Make sure to carefully look over the tools you are given access to to decide this.\nIf you are confident that you do not need a tool to respond, either "
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/swarm/core.py",
        "keyword": "execute",
        "snippet": "        debug: bool = False,\n        max_turns: int = float(\"inf\"),\n        execute_tools: bool = True,\n    ):\n        active_agent = agent\n        context_variables = copy.deepcopy(context_variables)\n        history = copy.deepcopy(messages)\n        init_len = len(messages)\n--\n            history.append(message)\n\n            if not message[\"tool_calls\"] or not execute_tools:\n                debug_print(debug, \"Ending turn.\")\n                break\n\n            # convert tool_calls to objects\n   "
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/tests/test_core.py",
        "keyword": "execute",
        "snippet": "\n\ndef test_execute_tools_false(mock_openai_client: MockOpenAIClient):\n    expected_location = \"San Francisco\"\n\n    # set up mock to record function calls\n    get_weather_mock = Mock()\n\n--\n    # set up client and run\n    client = Swarm(client=mock_openai_client)\n    response = client.run(agent=agent, messages=messages, execute_tools=False)\n    print(response)\n\n    # assert function not called\n    get_weather_mock.assert_not_called()\n\n"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/playwright/async_api/_generated.py",
        "keyword": "plan",
        "snippet": "            Client certificate authentication is only active when at least one client certificate is provided. If you want to\n            reject all client certificates sent by the server, you need to provide a client certificate with an `origin` that\n            does not match any of the domains you plan to visit.\n\n            **NOTE** When using WebKit on macOS, accessing `localhost` will not pick up client certificates. You can make it\n            work by replacing `localhost` with `local.pla"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/playwright/sync_api/_generated.py",
        "keyword": "plan",
        "snippet": "            Client certificate authentication is only active when at least one client certificate is provided. If you want to\n            reject all client certificates sent by the server, you need to provide a client certificate with an `origin` that\n            does not match any of the domains you plan to visit.\n\n            **NOTE** When using WebKit on macOS, accessing `localhost` will not pick up client certificates. You can make it\n            work by replacing `localhost` with `local.pla"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/playwright/async_api/_generated.py",
        "keyword": "steps",
        "snippet": "\n    async def move(\n        self, x: float, y: float, *, steps: typing.Optional[int] = None\n    ) -> None:\n        \"\"\"Mouse.move\n\n        Dispatches a `mousemove` event.\n\n--\n        y : float\n            Y coordinate relative to the main frame's viewport in CSS pixels.\n        steps : Union[int, None]\n            Defaults to 1. Sends `n` interpolated `mousemove` events to represent travel between Playwright's current cursor\n            position and the provided destination. When set to 1, emits"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/tests/async/test_click.py",
        "keyword": "steps",
        "snippet": "    )\n    # Centerpoint at 150 + 100/2, 280 + 40/2 = 200, 300\n    await page.locator(\"div\").click(steps=5)\n    assert await page.evaluate(\"result\") == [\n        [120, 140],\n        [140, 180],\n        [160, 220],\n        [180, 260],\n"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/playwright/async_api/_generated.py",
        "keyword": "execute",
        "snippet": "            `page.set_default_timeout()` methods.\n        polling : Union[\"raf\", float, None]\n            If `polling` is `'raf'`, then `expression` is constantly executed in `requestAnimationFrame` callback. If `polling`\n            is a number, then it is treated as an interval in milliseconds at which the function would be executed. Defaults to\n            `raf`.\n\n        Returns\n        -------\n        JSHandle\n--\n\n        The method adds a function called `name` on the `window` object of ev"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/tests/async/test_popup.py",
        "keyword": "execute",
        "snippet": "    )\n    request_waitable = asyncio.create_task(server.wait_for_request(\"/popup/popup.html\"))\n    await asyncio.sleep(0)  # execute scheduled tasks, but don't await them\n    async with context.expect_page() as page_info:\n        await page.click(\"a\")\n    popup = await page_info.value\n    await popup.wait_for_load_state(\"domcontentloaded\")\n    user_agent = await popup.evaluate(\"window.initialUserAgent\")\n--\n    await page.goto(server.EMPTY_PAGE)\n    request_promise = asyncio.create_task(server.wa"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/tests/ci/evaluate_tasks.py",
        "keyword": "plan",
        "snippet": "class JudgeResponse(BaseModel):\n\tsuccess: bool\n\texplanation: str\n\n\nasync def run_single_task(task_file):\n\t\"\"\"Run a single task in the current process (called by subprocess)\"\"\"\n\ttry:\n--\n\t\t\t\t'file': os.path.basename(task_file),\n\t\t\t\t'success': True,  # Mark as success so it doesn't fail CI\n\t\t\t\t'explanation': 'Skipped - API key not available (fork PR or missing secret)',\n\t\t\t}\n\n\t\tagent_llm = ChatBrowserUse(api_key=api_key)\n\n\t\t# Check if Google API key is available for judge LLM\n--\n\t\t\t\t'file': os.path"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/browser_use/code_use/service.py",
        "keyword": "plan",
        "snippet": "\t\t\t\t\t# If task is already done, empty code is fine (LLM explaining completion)\n\t\t\t\t\tif self._is_task_done():\n\t\t\t\t\t\tlogger.info('Task already marked as done, LLM provided explanation without code')\n\t\t\t\t\t\t# Add the text response to history as a non-code step\n\t\t\t\t\t\tawait self._add_step_to_complete_history(\n\t\t\t\t\t\t\tmodel_output_code='',\n\t\t\t\t\t\t\tfull_llm_response=full_llm_response,\n\t\t\t\t\t\t\toutput=full_llm_response,  # Treat the explanation as output\n\t\t\t\t\t\t\terror=None,\n\t\t\t\t\t\t\tscreenshot_path=await self._"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/browser_use/config.py",
        "keyword": "steps",
        "snippet": "\t\"\"\"Agent configuration entry.\"\"\"\n\n\tmax_steps: int | None = None\n\tuse_vision: bool | None = None\n\tsystem_prompt: str | None = None\n\n\nclass DBStyleConfigJSON(BaseModel):\n"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/browser_use/init_cmd.py",
        "keyword": "steps",
        "snippet": "\t\t\t\t\tconsole.print(f'[yellow]\u26a0[/yellow]  Error generating [cyan]{dest_name}[/cyan]: {e}')\n\n\t\t# Create a nice panel for next steps\n\t\tnext_steps = Text()\n\n\t\t# Display next steps from manifest if available\n\t\tif 'next_steps' in INIT_TEMPLATES[template]:\n\t\t\tsteps = INIT_TEMPLATES[template]['next_steps']\n\t\t\tfor i, step in enumerate(steps, 1):\n\t\t\t\t# Handle footer separately (no numbering)\n\t\t\t\tif 'footer' in step:\n\t\t\t\t\tnext_steps.append(f'{step[\"footer\"]}\\n', style='dim italic')\n\t\t\t\t\tcontinue\n\n\t\t\t\t# Ste"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/examples/custom-functions/parallel_agents.py",
        "keyword": "subtask",
        "snippet": "\n\nasync def create_subtasks(main_task: str, llm) -> list[str]:\n\t\"\"\"\n\tUse LLM to break down main task into logical subtasks\n\n\tReal examples of how this works:\n\n\tInput: \"what is the revenue of nvidia, microsoft, tesla\"\n\tOutput: [\n--\n\n\tprompt = f\"\"\"\n    Break down this main task into individual, separate subtasks where each subtask focuses on ONLY ONE specific person, company, or item:\n    \n    Main task: {main_task}\n    \n    RULES:\n    - Each subtask must focus on ONLY ONE person/company/item\n    "
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/examples/getting_started/04_multi_step_task.py",
        "keyword": "workflow",
        "snippet": "\nThis example demonstrates how to:\n- Perform a complex workflow with multiple steps\n- Navigate between different pages\n- Combine search, form filling, and data extraction\n- Handle a realistic end-to-end scenario\n\nThis is the most advanced getting started example, combining all previous concepts.\n"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/browser_use/init_cmd.py",
        "keyword": "workflow",
        "snippet": "\t\t\t\tnext_steps.append('\\n')\n\t\telse:\n\t\t\t# Default workflow for templates without custom next_steps\n\t\t\tnext_steps.append('\\n1. Navigate to project directory:\\n', style='bold')\n\t\t\tnext_steps.append(f'   cd {template}\\n\\n', style='dim')\n\t\t\tnext_steps.append('2. Initialize uv project:\\n', style='bold')\n\t\t\tnext_steps.append('   uv init\\n\\n', style='dim')\n\t\t\tnext_steps.append('3. Install browser-use:\\n', style='bold')\n"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/browser_use/code_use/namespace.py",
        "keyword": "execute",
        "snippet": "\n\tArgs:\n\t\tcode: JavaScript code to execute (must be wrapped in IIFE)\n\n\tReturns:\n\t\tThe result of the JavaScript execution\n\n\tRaises:\n--\n\texcept Exception as e:\n\t\t# Wrap other exceptions in EvaluateError\n\t\traise EvaluateError(f'Failed to execute JavaScript: {type(e).__name__}: {e}') from e\n\n\ndef create_namespace(\n\tbrowser_session: BrowserSession,\n\ttools: Tools | None = None,\n"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/browser_use/code_use/utils.py",
        "keyword": "execute",
        "snippet": "\tReturns dict mapping block_name -> content\n\n\tNote: Python blocks are NO LONGER COMBINED. Each python block executes separately\n\tto allow sequential execution with JS/bash blocks in between.\n\t\"\"\"\n\t# Pattern to match code blocks with language identifier and optional variable name\n\t# Matches: ```lang\\n or ```lang varname\\n or ````+lang\\n (4+ backticks for nested blocks)\n\t# Uses non-greedy matching and backreferences to match opening/closing backticks\n"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/examples/browser/playwright_integration.py",
        "keyword": "orchestrate",
        "snippet": "\t\t\t5. Finally, submit the form and tell me what happened\n\t\t\t\n\t\t\tThis demonstrates how Browser-Use AI can orchestrate tasks while using Playwright's precise capabilities for specific operations.\n\t\t\t\"\"\",\n\t\t\tllm=ChatOpenAI(model='gpt-4.1-mini'),\n\t\t\ttools=tools,  # Our custom tools with Playwright actions\n\t\t\tbrowser_session=browser_session,\n\t\t)\n"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/tests/ci/evaluate_tasks.py",
        "keyword": "pipeline",
        "snippet": "\t\tcriteria = '\\n- '.join(judge_context)\n\t\tjudge_prompt = f\"\"\"\nYou are a evaluator of a browser agent task inside a ci/cd pipeline. Here was the agent's task:\n{task}\n\nHere is the agent's output:\n{agent_output if agent_output else '[No output provided]'}\n\n"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/tests/ci/infrastructure/test_url_shortening.py",
        "keyword": "pipeline",
        "snippet": "1. Input message processing with URL shortening\n2. Output processing with custom actions and URL restoration\n3. End-to-end pipeline test\n\"\"\"\n\nimport json\n\nimport pytest\n--\n\nclass TestUrlShorteningEndToEnd:\n\t\"\"\"Test complete URL shortening pipeline end-to-end.\"\"\"\n\n\tdef test_complete_url_shortening_pipeline(self, agent: Agent):\n\t\t\"\"\"Test the complete pipeline: input shortening -> processing -> output restoration.\"\"\"\n\n\t\t# Step 1: Input processing with URL shortening\n\t\toriginal_content = f'Navigate "
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/crews/utils.py",
        "keyword": "plan",
        "snippet": "\n    Handles before callbacks, event emission, task handler reset, input\n    interpolation, task callbacks, agent setup, and planning.\n\n    Args:\n        crew: The crew instance to prepare.\n        inputs: Optional input dictionary to pass to the crew.\n\n--\n    )\n\n    if crew.planning:\n        crew._handle_crew_planning()\n\n    return inputs\n\n\nclass StreamingContext:\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/agents/parser.py",
        "keyword": "plan",
        "snippet": "    \"\"\"\n    # Skip repair if the input starts and ends with square brackets\n    # Explanation: The JSON parser has issues handling inputs that are enclosed in square brackets ('[]').\n    # These are typically valid JSON arrays or strings that do not require repair. Attempting to repair such inputs\n    # might lead to unintended alterations, such as wrapping the entire input in additional layers or modifying\n    # the structure in a way that changes its meaning. By skipping the repair for inputs "
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai-tools/src/crewai_tools/rag/loaders/docs_site_loader.py",
        "keyword": "decompose",
        "snippet": "\n        for script in soup([\"script\", \"style\"]):\n            script.decompose()\n\n        title = soup.find(\"title\")\n        title_text = title.get_text(strip=True) if title else \"Documentation\"\n\n        for selector in [\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai-tools/src/crewai_tools/rag/loaders/webpage_loader.py",
        "keyword": "decompose",
        "snippet": "\n            for script in soup([\"script\", \"style\"]):\n                script.decompose()\n\n            text = soup.get_text(\" \")\n            text = _SPACES_PATTERN.sub(\" \", text)\n            text = _NEWLINE_PATTERN.sub(\"\\n\", text)\n            text = text.strip()\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/experimental/evaluation/evaluation_listener.py",
        "keyword": "steps",
        "snippet": "\n    This listener attaches to the event bus to collect detailed information\n    about the execution process, including agent steps, tool uses, knowledge\n    retrievals, and final output - all for use in agent evaluation.\n    \"\"\"\n\n    _instance = None\n\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/flow/flow.py",
        "keyword": "steps",
        "snippet": "\n    This decorator marks a method as a router, which can dynamically determine\n    the next steps in the flow based on its return value. Routers are triggered\n    by specified conditions and can return constants that determine which path\n    the flow should take.\n\n    Args:\n        condition: Specifies when the router should execute. Can be:\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/crews/utils.py",
        "keyword": "workflow",
        "snippet": "    ],\n) -> list[CrewOutput | CrewStreamingOutput] | CrewStreamingOutput:\n    \"\"\"Execute crew workflow for each input asynchronously.\n\n    Args:\n        crew: The crew instance to execute.\n        inputs: List of input dictionaries for each execution.\n        kickoff_fn: Async function to call for each crew copy (kickoff_async or akickoff).\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/flow/flow.py",
        "keyword": "workflow",
        "snippet": "\nThis module provides the Flow class and decorators (@start, @listen, @router)\nfor building event-driven workflows with conditional execution and routing.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/crews/utils.py",
        "keyword": "execute",
        "snippet": "    \"\"\"\n    previous_output = task_outputs[-1] if task_outputs else None\n    if previous_output is not None and not task.should_execute(previous_output):\n        crew._logger.log(\n            \"debug\",\n            f\"Skipping conditional task: {task.description}\",\n            color=\"yellow\",\n        )\n--\n\n    Args:\n        crew: The crew instance to execute.\n        inputs: List of input dictionaries for each execution.\n        kickoff_fn: Async function to call for each crew copy (kickoff_async o"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/mcp/transports/stdio.py",
        "keyword": "execute",
        "snippet": "\n        Args:\n            command: Command to execute (e.g., \"python\", \"node\", \"npx\").\n            args: Command arguments (e.g., [\"server.py\"] or [\"-y\", \"@mcp/server\"]).\n            env: Environment variables to pass to the process.\n            **kwargs: Additional transport options.\n        \"\"\"\n        super().__init__(**kwargs)\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/events/listeners/tracing/trace_listener.py",
        "keyword": "orchestrate",
        "snippet": "\nclass TraceCollectionListener(BaseEventListener):\n    \"\"\"Trace collection listener that orchestrates trace collection.\"\"\"\n\n    complex_events: ClassVar[list[str]] = [\n        \"task_started\",\n        \"task_completed\",\n        \"llm_call_started\",\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai-tools/src/crewai_tools/tools/mongodb_vector_search_tool/vector_search.py",
        "keyword": "pipeline",
        "snippet": "        description=\"List of MQL match expressions comparing an indexed field\",\n    )\n    post_filter_pipeline: list[dict] | None = Field(\n        default=None,\n        description=\"Pipeline of MongoDB aggregation stages to filter/process results after $vectorSearch.\",\n    )\n    oversampling_factor: int = Field(\n        default=10,\n--\n            pre_filter = query_config.pre_filter\n            include_embeddings = query_config.include_embeddings\n            post_filter_pipeline = query_config.p"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai-tools/src/crewai_tools/tools/contextualai_create_agent_tool/contextual_create_agent_tool.py",
        "keyword": "pipeline",
        "snippet": "        document_paths: list[str],\n    ) -> str:\n        \"\"\"Create a complete RAG pipeline with documents.\"\"\"\n        try:\n            import os\n\n            # Create datastore\n            datastore = self.contextual_client.datastores.create(name=datastore_name)\n"
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-server/lavague/server/driver.py",
        "keyword": "plan",
        "snippet": "You must always include the comments as well, describing your actions step by step, following strictly the format in the examples provided.\n\nProvide high level explanations about why you think this element is the right one.\nYour answer must be short and concise. Always includes comments in the YAML before listing the actions.\n\nThe actions available are:\n\nName: click\n--\nArguments:\n  - xpath (string): Always set to an empty string\n  - value (string): Detailled explanation of why the task cannot be"
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-integrations/drivers/lavague-drivers-selenium/lavague/drivers/selenium/base.py",
        "keyword": "plan",
        "snippet": "You must always include the comments as well, describing your actions step by step, following strictly the format in the examples provided.\n\nProvide high level explanations about why you think this element is the right one.\nYour answer must be short and concise. Always includes comments in the YAML before listing the actions.\n\nThe actions available are:\n\nName: click\n"
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-core/lavague/core/agents.py",
        "keyword": "steps",
        "snippet": "        action_engine: ActionEngine,\n        token_counter: Optional[TokenCounter] = None,\n        n_steps: int = 10,\n        clean_screenshot_folder: bool = True,\n        logger: AgentLogger = None,\n    ):\n        self.driver: BaseDriver = action_engine.driver\n        self.action_engine: ActionEngine = action_engine\n--\n        self.interrupted = False\n\n        self.n_steps = n_steps\n\n        self.output = \"\"\n\n        self.clean_screenshot_folder = clean_screenshot_folder\n\n--\n                )\n "
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-qa/gradio_demo.py",
        "keyword": "steps",
        "snippet": "            last_keyword: str = None\n\n            for step in parsed_scenario[\"scenario\"][\"steps\"]:\n                keyword = step[\"keywordType\"]\n                if keyword == \"Conjunction\":\n                    keyword = last_keyword\n                else:\n                    last_keyword = keyword\n--\n                    scenario.context.append(step[\"text\"])\n                elif keyword == \"Action\":\n                    scenario.steps.append(step[\"text\"])\n                elif keyword == \"Outcome\":"
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-tests/lavague/tests/cli.py",
        "keyword": "execute",
        "snippet": "def _load_context(context):\n    if context:\n        # read context file and execute it\n        with open(context, \"r\") as file:\n            file_content = file.read()\n        local_namespace = {}\n        exec(file_content, {}, local_namespace)\n\n"
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-qa/lavague/qa/prompts.py",
        "keyword": "execute",
        "snippet": "Assert statement Then the cart total should be correct\n\nList of already executed instructions and actions:\n- instruction: Navigate to the product catalog page\n  actions:\n    - action:\n        name: \"click\"\n        args:\n--\n    )\n    try:\n        browser.execute_script(\"arguments[0].click();\", catalog_link)\n    except ElementClickInterceptedException:\n        pytest.fail(\"Failed to navigate to the product catalog\")\n\n@when('I filter products by category')\ndef filter_products(browser):\n--\n    )\n   "
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-core/lavague/core/retrievers.py",
        "keyword": "pipeline",
        "snippet": "\nclass RetrieversPipeline(BaseHtmlRetriever):\n    \"\"\"Executor for retrievers pipeline\"\"\"\n\n    retrievers: Tuple[BaseHtmlRetriever]\n\n    def __init__(self, *retrievers: BaseHtmlRetriever):\n        self.retrievers = retrievers\n"
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-core/lavague/core/context.py",
        "keyword": "pipeline",
        "snippet": "\nclass Context:\n    \"\"\"Set the context which will be used thourough the action generation pipeline.\"\"\"\n\n    def __init__(\n        self,\n        llm: LLM,\n        mm_llm: MultiModalLLM,\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/runnables/graph_ascii.py",
        "keyword": "plan",
        "snippet": "    \"\"\"\n    # NOTE: coordinates might me negative, so we need to shift\n    # everything to the positive plane before we actually draw it.\n    xlist: list[float] = []\n    ylist: list[float] = []\n\n    sug = _build_sugiyama_layout(vertices, edges)\n\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/tests/unit_tests/prompts/test_chat.py",
        "keyword": "plan",
        "snippet": "                                    \"input_variables\": [],\n                                    \"template_format\": \"mustache\",\n                                    \"template\": \"Here is a transcript of the student's explanation:\",  # noqa: E501\n                                    \"additional_content_fields\": {\n                                        \"text\": \"Here is a transcript of the student's explanation:\",  # noqa: E501\n                                    },\n                                },\n "
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/text-splitters/langchain_text_splitters/html.py",
        "keyword": "decompose",
        "snippet": "            for tag in _find_all_tags(soup, name=True):\n                if tag.name not in self._allowlist_tags:\n                    tag.decompose()\n\n        if self._denylist_tags:\n            for tag in _find_all_tags(soup, name=self._denylist_tags):\n                tag.decompose()\n\n    def _normalize_and_clean_text(self, text: str) -> str:\n        \"\"\"Normalizes the text by removing extra spaces and newlines.\n\n        Args:\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/runnables/base.py",
        "keyword": "steps",
        "snippet": "\ndef _seq_input_schema(\n    steps: list[Runnable[Any, Any]], config: RunnableConfig | None\n) -> type[BaseModel]:\n    # Import locally to prevent circular import\n    from langchain_core.runnables.passthrough import (  # noqa: PLC0415\n        RunnableAssign,\n        RunnablePick,\n    )\n\n    first = steps[0]\n    if len(steps) == 1:\n        return first.get_input_schema(config)\n    if isinstance(first, RunnableAssign):\n        next_input_schema = _seq_input_schema(steps[1:], config)\n        if not i"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/runnables/passthrough.py",
        "keyword": "steps",
        "snippet": "            name\n            or self.name\n            or f\"RunnableAssign<{','.join(self.mapper.steps__.keys())}>\"\n        )\n        return super().get_name(suffix, name=name)\n\n    @override\n    def get_input_schema(self, config: RunnableConfig | None = None) -> type[BaseModel]:\n--\n    ) -> Iterator[dict[str, Any]]:\n        # collect mapper keys\n        mapper_keys = set(self.mapper.steps__.keys())\n        # create two streams, one for the map and one for the passthrough\n        for_passthrough,"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/documents/__init__.py",
        "keyword": "workflow",
        "snippet": "\"\"\"Documents module for data retrieval and processing workflows.\n\nThis module provides core abstractions for handling data in retrieval-augmented\ngeneration (RAG) pipelines, vector stores, and document processing workflows.\n\n!!! warning \"Documents vs. message content\"\n    This module is distinct from `langchain_core.messages.content`, which provides\n    multimodal content blocks for **LLM chat I/O** (text, images, audio, etc. within\n    messages).\n--\n    **Key distinction:**\n\n    - **Documents**"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/documents/base.py",
        "keyword": "workflow",
        "snippet": "\"\"\"Base classes for media and documents.\n\nThis module contains core abstractions for **data retrieval and processing workflows**:\n\n- `BaseMedia`: Base class providing `id` and `metadata` fields\n- `Blob`: Raw data loading (files, binary data) - used by document loaders\n- `Document`: Text content for retrieval (RAG, vector stores, semantic search)\n\n--\n\nclass BaseMedia(Serializable):\n    \"\"\"Base class for content used in retrieval and data processing workflows.\n\n    Provides common fields for conte"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/agents.py",
        "keyword": "execute",
        "snippet": "1. Given a prompt an agent uses an LLM to request an action to take\n    (e.g., a tool to run).\n2. The agent executes the action (e.g., runs the tool), and receives an observation.\n3. The agent returns the observation to the LLM, which can then be used to generate\n    the next action.\n4. When the agent reaches a stopping condition, it returns a final return value.\n\nThe schemas for the agents themselves are defined in langchain.agents.agent.\n--\n\nclass AgentAction(Serializable):\n    \"\"\"Represents a"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/tracers/evaluation.py",
        "keyword": "execute",
        "snippet": "    \"\"\"The LangSmith client instance used for evaluating the runs.\"\"\"\n    evaluators: Sequence[langsmith.RunEvaluator] = ()\n    \"\"\"The sequence of run evaluators to be executed.\"\"\"\n    executor: ThreadPoolExecutor | None = None\n    \"\"\"The thread pool executor used for running the evaluators.\"\"\"\n    futures: weakref.WeakSet[Future] = weakref.WeakSet()\n    \"\"\"The set of futures representing the running evaluators.\"\"\"\n    skip_unfinished: bool = True\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/prompts/structured.py",
        "keyword": "pipeline",
        "snippet": "        Args:\n            others: The language model to pipe the structured prompt to.\n            name: The name of the pipeline.\n\n        Returns:\n            A RunnableSequence object.\n\n        Raises:\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/documents/__init__.py",
        "keyword": "pipeline",
        "snippet": "\nThis module provides core abstractions for handling data in retrieval-augmented\ngeneration (RAG) pipelines, vector stores, and document processing workflows.\n\n!!! warning \"Documents vs. message content\"\n    This module is distinct from `langchain_core.messages.content`, which provides\n    multimodal content blocks for **LLM chat I/O** (text, images, audio, etc. within\n    messages).\n--\n\n    - **Documents** (this module): For **data retrieval and processing workflows**\n        - Vector stores, r"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/schemas/json_parsing.py",
        "keyword": "plan",
        "snippet": "IMPORTANT:\n- Re-output the ENTIRE reformatted json snippet between the <JSON>...</JSON> tags, including the correct parts, making sure it inclues ALL required fields in the schema\n- Keep your response terse, and minimise explanations or discussions outside the <FIX_DESCRIPTION> and <JSON> tag pairs.\n- Ensure all string values are properly escaped and enclosed in double quotes.\n- Do not simply make up a JSON adherant response if it isn't strongly supported or requires significant interpretation o"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/benchmarks/gpqa.py",
        "keyword": "plan",
        "snippet": "            \"- Choose the BEST answer from the options below\",\n            \"- Respond with ONLY the letter choice in parentheses: (A), (B), (C), or (D)\",\n            \"- Do not include any other text or explanation in your answer\",\n            \"\",\n            \"Answer Choices:\"\n        ]\n\n        # Add formatted answer choices\n"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/agents/implementations/coder.py",
        "keyword": "decompose",
        "snippet": "    AGENT_DESCRIPTION = \"\"\"A specialised agent for coding tasks. You must delegate to this agent when the problem at hand involves solving any sort of coding problem.\n\nNote that this agent doesn't get to see your current context, or the initial request or problem statement. It is up to you to accurately relay this to the sub-agent or decompose it into sub-tasks if it is very long and repeating it verbatim would be slow and costly, as well as any relevant information in your dialogue history.\n\nWh"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/agents/implementations/problem_solver.py",
        "keyword": "decompose",
        "snippet": "\n    This agent can:\n    1. Analyze and decompose complex problems\n    2. Plan and execute solutions systematically\n    3. Use a wide range of tools and agents\n    4. Validate and refine solutions\n    5. Handle errors and edge cases\n    6. Document and explain its process\n--\nYour default agent for all tasks. Highly versatile, with broad tool access. Best for tasks requiring multiple capabilities or when specific agent choice isn't obvious.\n\nNote that the agent will not have the context that you "
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/oversight/graph_visualisation.py",
        "keyword": "steps",
        "snippet": "    # NOTE: this is important to allow the overseer to see the next step\n    # instructions in reasoning structures, and offer good guidance along those\n    # steps.\n    if \"reasoning_structure\" in tool_name or tool_name.endswith(\"_complete\"):\n        output_str = f\" | {str(result.output)}\" if result.output else \"\"\n    else:\n        output_str = f\" | {str(result.output)[:200]}...\" if result.output else \"\"\n\n"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/agents/implementations/coder.py",
        "keyword": "steps",
        "snippet": "   - Agent explicitly stated validation criteria for sum operation\n   - Tested empty list case and negative numbers\n   - Documented validation steps and results\n   - Made refinements based on validation findings\n\"\"\",\n                    metrics=make_random_agent_metrics(\n                        tools_enabled=True, agents_enabled=True\n                    ),\n"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/oversight/overseer.py",
        "keyword": "execute",
        "snippet": "    - If the task becomes untenable, after multiple failed attempts, and the agent is appearing to repeat itself, instruct the main agent to use its early_exit tool to exit early.\n    - If the agent starts writing pytest fixtures or mocks, and particularly getting stuck doing more work getting pytest or the mocks working, strongly reprimand it and suggest it writes end-to-end tests instead in a testing script\n    - When the execute_command tool is indicated to have successfully run ('Success') t"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/callgraph/manager.py",
        "keyword": "execute",
        "snippet": "\n        Args:\n            function_name: Name of the function being executed\n            node_id: the unique id of the agent\n            args: Function arguments\n\n        Returns:\n            str: The generated node ID\n"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/agents/implementations/main_orchestrator.py",
        "keyword": "orchestrate",
        "snippet": "   - Do not modify results or perform tasks directly; maintain your role as orchestrator.\n\nYour high-level objective is to successfully orchestrate and delegate your agent(s) to deliver a verified solution that you will own, defend and be responsible for through systematic delegation, rigorous evaluation, and comprehensive oversight, without direct intervention in the task execution.\n\"\"\"\n\n    # Available tools - including answer submission\n    AVAILABLE_TOOLS = {\n        SubmitAnswer,\n--\n\n      "
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/gpt_engineer/applications/cli/collect.py",
        "keyword": "plan",
        "snippet": "\n    rudder_analytics.write_key = \"2Re4kqwL61GDp7S8ewe6K5dbogG\"\n    rudder_analytics.dataPlaneUrl = \"https://gptengineerezm.dataplane.rudderstack.com\"\n\n    rudder_analytics.track(\n        user_id=learning.session,\n        event=\"learning\",\n        properties=learning.to_dict(),  # type: ignore\n"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/tests/core/default/test_steps.py",
        "keyword": "plan",
        "snippet": "\nfactorial_entrypoint = \"\"\"\nIrrelevant explanations\n```sh\npython3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\npytest test_factorial.py\n--\n    def test_empty_codebase_returns_empty_entrypoint(self):\n        # Arrange\n        ai_mock = TestGenEntrypoint.MockAI(\"Irrelevant explanation\")\n\n        code = FilesDict()\n        tempdir = tempfile.mkdtemp()\n        memory = DiskMemory(tempdir)\n        prompt = Prompt(\"\")\n--\n        # assert ENTRYPOINT_LOG_FILE in memory\n        #"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/scripts/legacy_benchmark.py",
        "keyword": "steps",
        "snippet": "                    \"gpt_engineer.cli.main\",\n                    bench_folder,\n                    \"--steps\",\n                    \"benchmark\",\n                ],\n                stdout=log_file,\n                stderr=log_file,\n                bufsize=0,\n--\n                    \"gpt_engineer.cli.main\",\n                    bench_folder,\n                    \"--steps\",\n                    \"evaluate\",\n                ],\n            )\n\n    generate_report(benchmarks, path)\n"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/scripts/test_api.py",
        "keyword": "steps",
        "snippet": "\n    # execute the step for our task\n    response = post_data(f\"{URL_BASE}/agent/tasks/{task_id}/steps\", {})\n    print(response.json())\n"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/gpt_engineer/applications/cli/file_selector.py",
        "keyword": "workflow",
        "snippet": "tree-structured display, users can navigate and select files for editing or processing.\nIt integrates with system editors for direct file modification and supports saving\nselections for later use. Designed for efficient workflow enhancement in file-intensive\nenvironments, it offers customizable file filtering and seamless editor integration.\n\nKey Components:\n- FileSelector: Manages file selection and interaction.\n- DisplayablePath: Provides a structured view of file paths.\n"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/scripts/test_api.py",
        "keyword": "execute",
        "snippet": "    sleep(1)  # this is not needed\n\n    # execute the step for our task\n    response = post_data(f\"{URL_BASE}/agent/tasks/{task_id}/steps\", {})\n    print(response.json())\n"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/gpt_engineer/core/default/simple_agent.py",
        "keyword": "execute",
        "snippet": "    This agent is capable of initializing a codebase from a prompt and improving an existing\n    codebase based on user input. It uses an AI model to generate and refine code, and it\n    interacts with a repository and an execution environment to manage and execute the code.\n\n    Attributes\n    ----------\n    memory : BaseMemory\n        The memory interface where the code and related data are stored.\n    execution_env : BaseExecutionEnv\n        The execution environment in which the code is exec"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/gpt_engineer/applications/cli/cli_agent.py",
        "keyword": "orchestrate",
        "snippet": "    \"\"\"\n    The `CliAgent` class is responsible for managing the lifecycle of code generation and improvement\n    using an AI model. It orchestrates the generation of new code and the improvement of existing code\n    based on given prompts and utilizes a memory system and execution environment for processing.\n\n    Parameters\n    ----------\n    memory : BaseMemory\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/evaluation/benchmarks/EDA/game.py",
        "keyword": "plan",
        "snippet": "                'content': f'Based on your knowledge about {self.item}, '\n                f'respond to the following question or guess. '\n                f\"Limit your respond to only 'Yes.', 'No.' or 'Maybe.', with no explanation or other words. \"\n                f'Never say the answer {self.item} in your response. '\n                f\"If the question is to solicit the answer, respond 'No.'.\",\n            },\n            {\n                'role': 'user',\n--\n                'content': f'Based on yo"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/evaluation/benchmarks/discoverybench/eval_utils/eval_w_subhypo_gen.py",
        "keyword": "plan",
        "snippet": "                'sizeB': var_json['sizeB'],\n                'intersection': var_json['intersection'],\n                'explanation': var_json['explanation'],\n            }\n            print(f'var_eval: {eval_rec}')\n            return eval_rec\n        except Exception:  # COMMENT: added Exception\n            return {'p': -1.0, 'r': -1.0, 'f1': -1.0}\n--\n        \"sizeB\": num of variables used in HypoB\n        \"intersection\": num of variables common in HypoA and HypoB. Use *fuzzy matching* to determ"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/openhands/agenthub/codeact_agent/tools/task_tracker.py",
        "keyword": "decompose",
        "snippet": "components into manageable implementation phases.\n\n*Creates structured plan with each feature decomposed into specific development tasks*\n\n## Counter-examples - Direct Implementation Preferred\n\n**Counter-example A: Simple inquiry**\nUser request: \"What's the syntax for a for loop in JavaScript?\"\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/evaluation/benchmarks/swefficiency/run_infer.py",
        "keyword": "steps",
        "snippet": "5. All the dependencies required to run the `workload()` function are already installed in the environment. You should not install or upgrade any dependencies.\n\nFollow these steps to improve performance:\n1. As a first step, explore the repository structure.\n2. Create a Python script to reproduce the performance workload, execute it with python <workload_file>, and examine the printed output metrics.\n3. Edit the source code of the repository to improve performance. Please do not change the conten"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/evaluation/benchmarks/the_agent_company/scripts/summarise_results.py",
        "keyword": "steps",
        "snippet": "\ndef analyze_traj_json_file(filepath: str) -> tuple[int, float]:\n    \"\"\"Analyze a single trajectory JSON file and extract the steps and tokens\n    for each step. Then estimate the cost based on the tokens and the model type.\n    Note: this is assuming there's no prompt caching at all.\n    \"\"\"\n    steps: int = 0\n    cost: float = 0.0\n    with open(filepath, 'r') as f:\n        data = json.load(f)\n        response_id = None\n        for action in data:\n--\n                    # a single LLM call lead"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/tests/unit/controller/test_agent_delegation.py",
        "keyword": "subtask",
        "snippet": "    )\n\n    # Now simulate that the child increments local iteration and finishes its subtask\n    delegate_controller = parent_controller.delegate\n\n    # Take four delegate steps; mock cost per step\n    for i in range(4):\n        delegate_controller.state.iteration_flag.step()\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/openhands/controller/state/state.py",
        "keyword": "subtask",
        "snippet": "    - Multi-agent/delegate state:\n      - store the task (conversation between the agent and the user)\n      - the subtask (conversation between an agent and the user or another agent)\n      - global and local iterations\n      - delegate levels for multi-agent interactions\n      - almost stuck state\n\n    - Running state of an agent:\n--\n    - Metrics:\n      - global metrics for the current task\n      - local metrics for the current subtask\n\n    - Extra data:\n      - additional task-specific data\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/evaluation/benchmarks/discoverybench/run_infer.py",
        "keyword": "workflow",
        "snippet": "\nfrom evaluation.benchmarks.discoverybench.eval_utils.eval_w_subhypo_gen import (\n    run_eval_gold_vs_gen_NL_hypo_workflow,\n)\nfrom evaluation.benchmarks.discoverybench.eval_utils.response_parser import (\n    extract_gen_hypo_from_logs,\n)\nfrom evaluation.utils.shared import (\n--\n\ndef get_dv_query_for_real(\n    datasets, question, domain_knowledge=None, workflow_tags=None\n):\n    \"\"\"Prepare a structured query for the agent to execute on the specified datasets.\n\n    This function constructs a query"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/evaluation/benchmarks/discoverybench/eval_utils/eval_w_subhypo_gen.py",
        "keyword": "workflow",
        "snippet": "    query,\n    gold_hypo,\n    gold_workflow,\n    gen_hypo,\n    gen_workflow,\n    dataset_meta,\n    llm_used,\n    dimension,\n    dataset_type,\n    use_column_metadata=True,\n--\n\n    dimension_question_str = f\"\"\"\\\n        You are going to compare two natural-language hypotheses HypoA and HypoB accompanied with optional workflows: WorkflowA for HypoA and WorkflowB for HypoB. \\\n        Both the hypotheses answer the natural language query \"QUERY\" over the dataset(s) described by dataset description(s"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/third_party/runtime/impl/daytona/daytona_runtime.py",
        "keyword": "execute",
        "snippet": "        self.sandbox.process.create_session(exec_session_id)\n\n        exec_command = self.sandbox.process.execute_session_command(\n            exec_session_id,\n            SessionExecuteRequest(command=start_command_str, var_async=True),\n        )\n\n        self.log(\"debug\", f\"exec_command_id: {exec_command.cmd_id}\")\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/third_party/runtime/impl/e2b/sandbox.py",
        "keyword": "execute",
        "snippet": "        return tar_filename\n\n    def execute(self, cmd: str, timeout: int | None = None) -> tuple[int, str]:\n        timeout = timeout if timeout is not None else self.config.timeout\n        \n        # E2B code-interpreter uses commands.run()\n        try:\n            result = self.sandbox.commands.run(cmd)\n--\n\n            # Check if sandbox_dest exists. If not, create it.\n            exit_code, _ = self.execute(f\"test -d {sandbox_dest}\")\n            if exit_code != 0:\n                self.execut"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/enterprise/integrations/solvability/models/classifier.py",
        "keyword": "orchestrate",
        "snippet": "        Transform the input issues using the featurizer to extract features.\n\n        This method orchestrates the feature extraction pipeline:\n        1. Uses the featurizer to generate embeddings for all issues\n        2. Converts embeddings to a structured DataFrame\n        3. Separates feature columns from metadata columns\n        4. Stores results for later access via properties\n\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/openhands/app_server/app_conversation/live_status_app_conversation_service.py",
        "keyword": "orchestrate",
        "snippet": "        \"\"\"Build a complete conversation request for a user.\n\n        This method orchestrates the creation of a conversation request by:\n        1. Setting up git provider secrets\n        2. Configuring LLM and MCP settings\n        3. Creating an agent with appropriate context\n        4. Finalizing the request with skills and experiment variants\n        \"\"\"\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/openhands/core/config/condenser_config.py",
        "keyword": "pipeline",
        "snippet": "    \"\"\"Configuration for the CondenserPipeline.\"\"\"\n\n    type: Literal['pipeline'] = Field(default='pipeline')\n    condensers: list[CondenserConfig] = Field(\n        default_factory=list,\n        description='List of condenser configurations to be used in the pipeline.',\n    )\n\n    model_config = ConfigDict(extra='forbid')\n\n\n--\n        'llm_attention': LLMAttentionCondenserConfig,\n        'structured': StructuredSummaryCondenserConfig,\n        'pipeline': CondenserPipelineConfig,\n        'convers"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/openhands/memory/condenser/impl/__init__.py",
        "keyword": "pipeline",
        "snippet": "    ObservationMaskingCondenser,\n)\nfrom openhands.memory.condenser.impl.pipeline import CondenserPipeline\nfrom openhands.memory.condenser.impl.recent_events_condenser import (\n    RecentEventsCondenser,\n)\nfrom openhands.memory.condenser.impl.structured_summary_condenser import (\n    StructuredSummaryCondenser,\n"
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter6_to_do_write/src/tools/todo_write.py",
        "keyword": "plan",
        "snippet": "\n1. Complex multi-step tasks - When a task requires 3 or more distinct steps or actions\n2. Non-trivial and complex tasks - Tasks that require careful planning or multiple operations\n3. User explicitly requests todo list - When the user directly asks you to use the todo list\n4. User provides multiple tasks - When users provide a list of things to be done (numbered or comma-separated)\n5. After receiving new instructions - Immediately capture user requirements as todos\n6. When you start working on "
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter6_to_do_write/src/core/prompt/system_rule.py",
        "keyword": "plan",
        "snippet": "You are an interactive CLI tool that helps users with software engineering tasks. Use the instructions below and the tools available to you to assist the user.\n\nIMPORTANT: Assist with defensive security tasks only. Refuse to create, modify, or improve code that may be used maliciously. Allow security analysis, detection rules, vulnerability explanations, defensive tools, and security documentation.\nIMPORTANT: You must NEVER generate or guess URLs for the user unless you are confident that the UR"
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter6_to_do_write/src/tools/todo_write.py",
        "keyword": "steps",
        "snippet": "Use this tool proactively in these scenarios:\n\n1. Complex multi-step tasks - When a task requires 3 or more distinct steps or actions\n2. Non-trivial and complex tasks - Tasks that require careful planning or multiple operations\n3. User explicitly requests todo list - When the user directly asks you to use the todo list\n4. User provides multiple tasks - When users provide a list of things to be done (numbered or comma-separated)\n5. After receiving new instructions - Immediately capture user requi"
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter6_to_do_write/src/tools/cmd_runner.py",
        "keyword": "steps",
        "snippet": "Executes a given bash command in a persistent shell session with optional timeout, user approval, and security measures.\n\nBefore executing the command, please follow these steps:\n1. Security Check:\n   - Always check if the command is potentially dangerous (e.g., rm, sudo, chmod 777, format, del, etc.)\n   - If the command is dangerous or could cause system damage/data loss, you MUST set need_user_approve=true\n\n2. Directory Verification:\n--\nCommitting changes with git\n\nWhen the user asks you to cr"
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter4_history_control/src/core/conversation.py",
        "keyword": "execute",
        "snippet": "            # Check if user approval is needed\n            need_user_approve = args.get('need_user_approve', False)\n            should_execute = True\n\n            if need_user_approve:\n                approval_content = f\"\u5de5\u5177: {tool_call.function.name}, \u53c2\u6570: {args}\"\n                should_execute = await self._ui_manager.wait_for_user_approval(approval_content)\n\n            if should_execute:\n                await self._execute_tool(tool_call, args)\n            else:\n                self._add_tool"
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter7_sub_agent/src/core/conversation.py",
        "keyword": "execute",
        "snippet": "            # Check if user approval is needed\n            need_user_approve = args.get('need_user_approve', False)\n            should_execute = True\n\n            if need_user_approve:\n                approval_content = f\"Tool: {tool_call.function.name}, args: {args}\"\n                should_execute, content = await self._ui_manager.wait_for_user_approval(approval_content)\n\n            if should_execute:\n                await self._execute_tool(tool_call, args, is_last_tool)\n            else:\n   "
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/prompts/self_improvement_prompt.py",
        "keyword": "plan",
        "snippet": "----- Coding Agent Implementation End -----\n\nYour task is to identify ONE detailed plan that would improve the agent's coding ability. The improvement should not be specific to any particular GitHub issue or repository.\n\"\"\"\n\nswe_issue_prompt = \"Here is the log for the coding agent trying to solve the GitHub issues but failed.\"\npolyglot_issue_prompt = \"Here is the log for the coding agent trying to solve a programming task. A task is in one programming language, but the coding agent needs to deal"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/utils/eval_utils.py",
        "keyword": "plan",
        "snippet": "In <JSON>, provide a JSON response with the following fields:\n- \"difference_summary\": Summary of the differences between the code diffs.\n- \"reasoning\": Explanation of the reasoning behind the evaluation.\n- \"scores\": List of numerical scores for each proposed solution.\n\nYour response will be automatically parsed, so ensure that the string response is precisely in the correct format. Do NOT include `<JSON>` tag in your output.\n\"\"\"\n        response, msg_history = get_response_from_llm(\n"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/prompts/self_improvement_prompt.py",
        "keyword": "steps",
        "snippet": "  - The coding agent trying to solve a programming task. A task is in one programming language, but the coding agent needs to deal with different languages including C++, Go, Java, JavaScript, Python, and Rust.\n  - The agent is very good at automatically utilizing the right available tools at the right time. So do not have an agentic flow that explicitly forces a tool's usage.\n  - Be detailed in the prompt about what steps (e.g. implementing tests, refining solutions, etc.) you would like the ag"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/prompts/self_improvement_prompt.py",
        "keyword": "workflow",
        "snippet": "### DOC: tool function schema\n\nCarefully consider whether to add/enhance the current tool or edit the workflow in forward()\n\nPay special attention to making sure that \"required\" and \"type\" are always at the correct level of nesting. For example, \"required\" should be at the same level as \"properties\", not inside it.\nMake sure that every property, no matter how short, has a type and description correctly nested inside it.\nOther arguments than you have seen are not permitted. For example, in \"edit_"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/polyglot/docker_build.py",
        "keyword": "execute",
        "snippet": "        lang = test_spec.instance_id.split(\"__\")[0]\n        config = MAP_REPO_VERSION_TO_SPECS[lang]\n        user = \"root\" if not config.get(\"execute_test_as_nonroot\", False) else \"nonroot\"\n        nano_cpus = config.get(\"nano_cpus\")\n\n        # Create the container\n        logger.info(f\"Creating container for {test_spec.instance_id}...\")\n        container = client.containers.create(\n"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/prompts/self_improvement_prompt.py",
        "keyword": "execute",
        "snippet": "  - The coding agent trying to solve a programming task. A task is in one programming language, but the coding agent needs to deal with different languages including C++, Go, Java, JavaScript, Python, and Rust.\n  - The agent is very good at automatically utilizing the right available tools at the right time. So do not have an agentic flow that explicitly forces a tool's usage.\n  - Be detailed in the prompt about what steps (e.g. implementing tests, refining solutions, etc.) you would like the ag"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/aider/prompts.py",
        "keyword": "plan",
        "snippet": "- Does not exceed 72 characters.\n\nReply only with the one-line commit message, without any additional text, explanations, or line breaks.\n\"\"\"\n\n# COMMANDS\nundo_command_reply = (\n    \"I did `git reset --hard HEAD~1` to discard the last edits. Please wait for further\"\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/aider/coders/wholefile_func_coder.py",
        "keyword": "plan",
        "snippet": "            parameters=dict(\n                type=\"object\",\n                required=[\"explanation\", \"files\"],\n                properties=dict(\n                    explanation=dict(\n                        type=\"string\",\n                        description=(\n                            \"Step by step plan for the changes to be made to the code (future\"\n                            \" tense, markdown format)\"\n                        ),\n                    ),\n                    files=dict(\n         "
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/aider/scrape.py",
        "keyword": "decompose",
        "snippet": "def slimdown_html(soup):\n    for svg in soup.find_all(\"svg\"):\n        svg.decompose()\n\n    if soup.img:\n        soup.img.decompose()\n\n    for tag in soup.find_all(href=lambda x: x and x.startswith(\"data:\")):\n        tag.decompose()\n\n    for tag in soup.find_all(src=lambda x: x and x.startswith(\"data:\")):\n        tag.decompose()\n\n    for tag in soup.find_all(True):\n        for attr in list(tag.attrs):\n            if attr != \"href\":\n                tag.attrs.pop(attr, None)\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/scripts/issues.py",
        "keyword": "steps",
        "snippet": "    \"\"\"3 weeks. This issue should be resolved in recent versions of aider.\\n\\n\"\"\"\n    \"\"\"If you find that this bug is still present, please feel free to reopen this \"\"\"\n    \"\"\"issue or create a new one with steps to reproduce.\"\"\" + BOT_SUFFIX\n)\n\n# GitHub API configuration\nGITHUB_API_URL = \"https://api.github.com\"\nREPO_OWNER = \"Aider-AI\"\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/scripts/blame.py",
        "keyword": "workflow",
        "snippet": "        for f in files\n        if f.endswith((\".js\", \".py\", \".scm\", \".sh\", \"Dockerfile\", \"Gemfile\"))\n        or (f.startswith(\".github/workflows/\") and f.endswith(\".yml\"))\n        or (f.startswith(\"aider/resources/\") and f.endswith(\".yml\"))\n        or f in website_files\n        or f in test_files\n    ]\n    files = [f for f in files if not f.endswith(\"prompts.py\")]\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/tests/basic/test_special.py",
        "keyword": "workflow",
        "snippet": "    assert is_important(\"setup.py\")\n\n    # Test files in .github/workflows\n    assert is_important(os.path.join(\".github\", \"workflows\", \"test.yml\"))\n    assert is_important(os.path.join(\".github\", \"workflows\", \"deploy.yml\"))\n\n    # Test files that should not be considered important\n    assert not is_important(\"random_file.txt\")\n    assert not is_important(\"src/main.py\")\n    assert not is_important(\"tests/test_app.py\")\n--\n        \"tests/test_app.py\",\n        \"requirements.txt\",\n        \".github/w"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/tests/help/test_help.py",
        "keyword": "execute",
        "snippet": "\n        Args:\n            func: Function to execute\n            max_time: Maximum time in seconds to keep retrying\n            initial_delay: Initial delay between retries in seconds\n            backoff_factor: Multiplier for delay after each retry\n\n        Returns:\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/aider/linter.py",
        "keyword": "execute",
        "snippet": "            )\n        except OSError as err:\n            print(f\"Unable to execute lint command: {err}\")\n            return\n        errors = stdout\n        if returncode == 0:\n            return  # zero exit status\n\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/aider/special.py",
        "keyword": "pipeline",
        "snippet": "    \".gitlab-ci.yml\",\n    \"Jenkinsfile\",\n    \"azure-pipelines.yml\",\n    \"bitbucket-pipelines.yml\",\n    \"appveyor.yml\",\n    \"circle.yml\",\n    \".circleci/config.yml\",\n    \".github/dependabot.yml\",\n    \"codecov.yml\",\n"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/samples/task_centric_memory/eval_retrieval.py",
        "keyword": "plan",
        "snippet": "to retrieve previously stored task-insight pairs as potentially useful examplars when solving some new task.\nA task is any text instruction that the app may give to an agent.\nAn insight is any text (like a hint, advice, a demonstration or plan) that might help the agent perform such tasks.\n\"\"\"\n\n\nasync def eval_retrieval(\n    memory_controller: MemoryController, client: ChatCompletionClient, logger: PageLogger, config: Dict[str, Any]\n"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-agentchat/tests/test_magentic_one_group_chat.py",
        "keyword": "plan",
        "snippet": "        chat_completions=[\n            \"No facts\",\n            \"No plan\",\n            json.dumps(\n                {\n                    \"is_request_satisfied\": {\"answer\": False, \"reason\": \"test\"},\n                    \"is_progress_being_made\": {\"answer\": True, \"reason\": \"test\"},\n                    \"is_in_loop\": {\"answer\": False, \"reason\": \"test\"},\n--\n    assert manager_1._task == manager_2._task  # pyright: ignore\n    assert manager_1._facts == manager_2._facts  # pyright: ignore\n    assert mana"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-studio/autogenstudio/gallery/tools/bing_search.py",
        "keyword": "decompose",
        "snippet": "                # Remove script and style elements\n                for script in soup([\"script\", \"style\"]):\n                    script.decompose()\n\n                # Convert relative URLs to absolute\n                for tag in soup.find_all([\"a\", \"img\"]):\n                    if tag.get(\"href\"):\n                        tag[\"href\"] = urljoin(url, tag[\"href\"])\n"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-studio/autogenstudio/gallery/tools/google_search.py",
        "keyword": "decompose",
        "snippet": "                # Remove script and style elements\n                for script in soup([\"script\", \"style\"]):\n                    script.decompose()\n\n                # Convert relative URLs to absolute\n                for tag in soup.find_all([\"a\", \"img\"]):\n                    if tag.get(\"href\"):\n                        tag[\"href\"] = urljoin(url, tag[\"href\"])\n"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/docs/src/_extension/code_lint.py",
        "keyword": "steps",
        "snippet": "\n    def prepare_writing(self, docnames: AbstractSet[str]) -> None:\n        \"\"\"Run these steps before documents are written.\"\"\"\n        return\n\n    def write_doc(self, docname: str, doctree: nodes.Node) -> None:\n        path_prefix: str = self.app.config.code_lint_path_prefix\n        supported_languages = set([\"python\", \"default\"])\n"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-ext/src/autogen_ext/teams/magentic_one.py",
        "keyword": "steps",
        "snippet": "    Magentic-One is a generalist multi-agent system for solving open-ended web and file-based tasks across a variety of domains. It represents a significant step towards developing agents that can complete tasks that people encounter in their work and personal lives.\n\n    Magentic-One work is based on a multi-agent architecture where a lead Orchestrator agent is responsible for high-level planning, directing other agents, and tracking task progress. The Orchestrator begins by creating a plan to "
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-ext/src/autogen_ext/teams/magentic_one.py",
        "keyword": "subtask",
        "snippet": "    Magentic-One is a generalist multi-agent system for solving open-ended web and file-based tasks across a variety of domains. It represents a significant step towards developing agents that can complete tasks that people encounter in their work and personal lives.\n\n    Magentic-One work is based on a multi-agent architecture where a lead Orchestrator agent is responsible for high-level planning, directing other agents, and tracking task progress. The Orchestrator begins by creating a plan to "
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-agentchat/src/autogen_agentchat/agents/_message_filter_agent.py",
        "keyword": "workflow",
        "snippet": "        This is an experimental feature, and the API will change in the future releases.\n\n    This is useful in scenarios like multi-agent workflows where an agent should only\n    process a subset of the full message history\u2014for example, only the last message\n    from each upstream agent, or only the first message from a specific source.\n\n    Filtering is configured using :class:`MessageFilterConfig`, which supports:\n    - Filtering by message source (e.g., only messages from \"user\" or another a"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-agentchat/src/autogen_agentchat/agents/_assistant_agent.py",
        "keyword": "workflow",
        "snippet": "        - `reflect_on_tool_use` is set to `False` by default when `output_content_type` is not set.\n    * If the model returns multiple tool calls, they will be executed concurrently. To disable parallel tool calls you need to configure the model client. For example, set `parallel_tool_calls=False` for :class:`~autogen_ext.models.openai.OpenAIChatCompletionClient` and :class:`~autogen_ext.models.openai.AzureOpenAIChatCompletionClient`.\n    * The `max_tool_iterations` parameter controls how many "
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/magentic-one-cli/src/magentic_one_cli/_m1.py",
        "keyword": "execute",
        "snippet": "\n    Arguments:\n    task (str): The task to be executed by MagenticOne.\n    --no-hil: Optional flag to disable human-in-the-loop mode.\n    --rich: Optional flag to enable rich console output.\n    --config: Optional flag to specify an alternate model configuration\n\n    Example usage:\n--\n        )\n    )\n    parser.add_argument(\"task\", type=str, nargs=\"?\", help=\"The task to be executed by MagenticOne.\")\n    parser.add_argument(\"--no-hil\", action=\"store_true\", help=\"Disable human-in-the-loop mode.\")"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-agentchat/src/autogen_agentchat/conditions/_terminations.py",
        "keyword": "execute",
        "snippet": "                        self._terminated = True\n                        return StopMessage(\n                            content=f\"Function '{self._function_name}' was executed.\",\n                            source=\"FunctionCallTermination\",\n                        )\n        return None\n\n    async def reset(self) -> None:\n"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-agentchat/src/autogen_agentchat/teams/_group_chat/_magentic_one/_magentic_one_orchestrator.py",
        "keyword": "orchestrate",
        "snippet": "                    return\n\n            await self._orchestrate_step(ctx.cancellation_token)\n        except Exception as e:\n            error = SerializableException.from_exception(e)\n            await self._signal_termination_with_error(error)\n            # Raise the error to the runtime.\n            raise\n--\n\n    async def select_speaker(self, thread: Sequence[BaseAgentEvent | BaseChatMessage]) -> List[str] | str:\n        \"\"\"Not used in this orchestrator, we select next speaker in _orchestrate"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-agentchat/tests/test_code_executor_agent.py",
        "keyword": "pipeline",
        "snippet": "async def test_code_generation_and_execution_with_model_client() -> None:\n    \"\"\"\n    Tests the code generation, execution and reflection pipeline using a model client.\n    \"\"\"\n\n    language = \"python\"\n    code = 'import math\\n\\nnumber = 42\\nsquare_root = math.sqrt(number)\\nprint(\"%0.3f\" % (square_root,))'\n\n"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-ext/src/autogen_ext/models/openai/_message_transform.py",
        "keyword": "pipeline",
        "snippet": "## AutoGen Modular Transformer Pipeline\n\nThis module implements a modular and extensible message transformation pipeline\nfor converting `LLMMessage` instances into SDK-specific message formats\n(e.g., OpenAI-style `ChatCompletionMessageParam`).\n\n---\n\n--\nespecially when supporting diverse models such as Gemini, Claude, or Anthropic SDKs.\n\nTo address this, PR #6063 introduced a unified, composable transformer pipeline\nthat decouples message transformation logic from model SDK constructors.\n\n---\n\n##"
      }
    ],
    "self_healing": [
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/agents/base.py",
        "keyword": "retry",
        "snippet": "            await self.ui.send_key_expired(message)\n            answer = await self.ask_question(\n                \"Would you like to retry the last step?\",\n                buttons={\"yes\": \"Yes\", \"no\": \"No\"},\n                buttons_only=True,\n            )\n            if answer.button == \"yes\":\n                return True\n--\n            await self.stream_handler(message)\n            answer = await self.ui.ask_question(\n                \"Would you like to retry the failed request?\",\n              "
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/agents/developer.py",
        "keyword": "retry",
        "snippet": "\n        max_retries = 2\n        retry_count = 0\n\n        while retry_count < max_retries:\n            if has_correct_num_of_tags(response):\n                break\n\n            convo.user(\n                \"Ok, now think carefully about your previous response. If the response ends by mentioning something about continuing with the implementation, continue but don't implement any files that have already been implemented. If your last response finishes with an incomplete file, implement that file and"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/agents/error_handler.py",
        "keyword": "recover",
        "snippet": "\n    Error handler is responsible for handling errors returned by other agents. If it's possible\n    to recover from the error, it should do it (which may include updating the \"next\" state) and\n    return DONE. Otherwise it should return EXIT to tell Orchestrator to quit the application.\n    \"\"\"\n\n    agent_type = \"error-handler\"\n    display_name = \"Error Handler\"\n"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/cli/main.py",
        "keyword": "rollback",
        "snippet": "        log.info(\"Interrupted by user\")\n        telemetry.set(\"end_result\", \"interrupt\")\n        await sm.rollback()\n    except APIError as err:\n        log.warning(f\"an LLM API error occurred: {err.message}\")\n        await send_error(ui, \"error while calling the LLM API\", err)\n        telemetry.set(\"end_result\", \"failure:api-error\")\n        await sm.rollback()\n    except CustomAssertionError as err:\n        log.warning(f\"an Anthropic assertion error occurred: {str(err)}\")\n        await send_err"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/agents/orchestrator.py",
        "keyword": "rollback",
        "snippet": "                continue\n\n        # TODO: rollback changes to \"next\" so they aren't accidentally committed?\n        return True\n\n    async def install_dependencies(self):\n        # First check if package.json exists\n        package_json_path = os.path.join(self.state_manager.get_full_project_root(), \"package.json\")\n"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/cli/helpers.py",
        "keyword": "validate",
        "snippet": "\n    try:\n        Config.model_validate(config)\n    except ValueError as err:\n        print(f\"Configuration error: {err}\", file=sys.stderr)\n        return None\n\n    return config\n"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/tests/cli/test_cli.py",
        "keyword": "validate",
        "snippet": "    show_config()\n    captured = capsys.readouterr()\n    assert Config.model_validate_json(captured.out) == Config()\n\n\n@pytest.mark.asyncio\n@patch(\"core.cli.helpers.StateManager\")\nasync def test_list_projects_json(mock_StateManager, capsys):\n"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/agents/response.py",
        "keyword": "error_handle",
        "snippet": "if TYPE_CHECKING:\n    from core.agents.base import BaseAgent\n    from core.agents.error_handler import ErrorHandler\n\n\nlog = get_logger(__name__)\n\n\n"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/agents/orchestrator.py",
        "keyword": "error_handle",
        "snippet": "from core.agents.code_monkey import CodeMonkey\nfrom core.agents.developer import Developer\nfrom core.agents.error_handler import ErrorHandler\nfrom core.agents.executor import Executor\nfrom core.agents.external_docs import ExternalDocumentation\nfrom core.agents.frontend import Frontend\nfrom core.agents.git import GitMixin\nfrom core.agents.human_input import HumanInput\n"
      },
      {
        "repo": "plandex",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/plandex/app/server/litellm_proxy.py",
        "keyword": "retry",
        "snippet": "def error_response(exc: Exception) -> JSONResponse:\n  status = getattr(exc, \"status_code\", 500)\n  retry_after = (\n    getattr(getattr(exc, \"response\", None), \"headers\", {})\n    .get(\"Retry-After\")\n  )\n  hdrs = {\"Retry-After\": retry_after} if retry_after else {}\n  return JSONResponse(status_code=status, content={\"error\": str(exc)}, headers=hdrs)\n\ndef normalise_for_ollama(p):\n  if not p.get(\"model\", \"\").startswith(\"ollama\"):\n    return p\n"
      },
      {
        "repo": "plandex",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/plandex/app/server/syntax/file_map/examples/python_example.py",
        "keyword": "validate",
        "snippet": "class Processable(Protocol):\n    def process(self) -> None: ...\n    def validate(self) -> bool: ...\n\n# Enum definition\nclass Status(enum.Enum):\n    PENDING = \"pending\"\n    ACTIVE = \"active\"\n--\n    @log_execution\n    async def process_item(self, item: UserCredentials) -> None:\n        if not self.validate():\n            raise ValueError(\"Processor is not in a valid state\")\n        self._items.append(item)\n        self._processed_count += 1\n\n    # Implementation of protocol method\n--\n        self."
      },
      {
        "repo": "plandex",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/plandex/app/server/litellm_proxy.py",
        "keyword": "heal",
        "snippet": "app = FastAPI()\n\n@app.get(\"/health\")\nasync def health():\n  return {\"status\": \"ok\"}\n\n@app.post(\"/v1/chat/completions\")\nasync def passthrough(request: Request):\n  payload = await request.json()\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/tests/help/test_help.py",
        "keyword": "retry",
        "snippet": "class TestHelp(unittest.TestCase):\n    @staticmethod\n    def retry_with_backoff(func, max_time=60, initial_delay=1, backoff_factor=2):\n        \"\"\"\n        Execute a function with exponential backoff retry logic.\n\n        Args:\n            func: Function to execute\n            max_time: Maximum time in seconds to keep retrying\n            initial_delay: Initial delay between retries in seconds\n            backoff_factor: Multiplier for delay after each retry\n\n        Returns:\n            The resu"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/aider/exceptions.py",
        "keyword": "retry",
        "snippet": "class ExInfo:\n    name: str\n    retry: bool\n    description: str\n\n\nEXCEPTIONS = [\n    ExInfo(\"APIConnectionError\", True, None),\n--\n                )\n\n        # Check for specific non-retryable APIError cases like insufficient credits\n        if ex.__class__ is litellm.APIError:\n            err_str = str(ex).lower()\n            if \"insufficient credits\" in err_str and '\"code\":402' in err_str:\n                return ExInfo(\n                    \"APIError\",\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/aider/mdstream.py",
        "keyword": "rollback",
        "snippet": "        The unstable lines emit into the Live window so they can be repainted.\n\n        Markdown going to the console works better in terminal scrollback buffers.\n        The live window doesn't play nice with terminal scrollback.\n        \"\"\"\n        # On the first call, stop the spinner and start the Live renderer\n        if not getattr(self, \"_live_started\", False):\n            self.live = Live(Text(\"\"), refresh_per_second=1.0 / self.min_delay)\n            self.live.start()\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/tests/basic/test_watch.py",
        "keyword": "backup",
        "snippet": "    assert spec.match_file(\".aider.conf\")\n    assert spec.match_file(\".git/config\")\n    assert spec.match_file(\"file~\")  # Emacs/vim backup\n    assert spec.match_file(\"file.bak\")\n    assert spec.match_file(\"file.swp\")\n    assert spec.match_file(\"file.swo\")\n    assert spec.match_file(\"#temp#\")  # Emacs auto-save\n    assert spec.match_file(\".#lock\")  # Emacs lock\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/aider/watch.py",
        "keyword": "backup",
        "snippet": "        \".aider*\",\n        \".git\",\n        # Common editor backup/temp files\n        \"*~\",  # Emacs/vim backup\n        \"*.bak\",  # Generic backup\n        \"*.swp\",  # Vim swap\n        \"*.swo\",  # Vim swap\n        \"\\\\#*\\\\#\",  # Emacs auto-save\n        \".#*\",  # Emacs lock files\n        \"*.tmp\",  # Generic temp files\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/aider/models.py",
        "keyword": "validate",
        "snippet": "\n        # Are all needed keys/params available?\n        res = self.validate_environment()\n        self.missing_keys = res.get(\"missing_keys\")\n        self.keys_in_environment = res.get(\"keys_in_environment\")\n\n        max_input_tokens = self.info.get(\"max_input_tokens\") or 0\n        # Calculate max_chat_history_tokens as 1/16th of max_input_tokens,\n--\n            return img.size\n\n    def fast_validate_environment(self):\n        \"\"\"Fast path for common models. Avoids forcing litellm import.\"\"\"\n\n "
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/aider/io.py",
        "keyword": "validate",
        "snippet": "\n        # Validate color settings after console is initialized\n        self._validate_color_settings()\n\n    def _validate_color_settings(self):\n        \"\"\"Validate configured color strings and reset invalid ones.\"\"\"\n        color_attributes = [\n            \"user_input_color\",\n            \"tool_output_color\",\n            \"tool_error_color\",\n--\n            if color_value:\n                try:\n                    # Try creating a style to validate the color\n                    RichStyle(color=colo"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/capabilities/summarization/data/multiple_subleases.py",
        "keyword": "recover",
        "snippet": "(b) Enter upon and take possession of the Subleased Premises and expel or remove Sublessee and any other person who may be occupying said premises or any part thereof, without being liable for prosecution or any claim for damages therefor;\n\n(c) Recover from Sublessee all damages incurred by Sublessor by reason of Sublessee's default, including but not limited to (i) the cost of recovering possession of the Subleased Premises, (ii) expenses of reletting, including necessary renovation and alterat"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/tool_use/utils/customer_service_api.py",
        "keyword": "backup",
        "snippet": "                \"My account was locked after I entered the wrong password too many times. I can now remember my correct password but the unlock email link isn't working.\",\n                \"I need to change my account email from {old_email} to {new_email} because I no longer have access to my old email. How can I do this?\",\n                \"I enabled two-factor authentication but lost my phone. I have my backup codes but the system won't accept them. I can't access my account at all now.\",\n      "
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/scripts/validate_notebooks.py",
        "keyword": "validate",
        "snippet": "\n\ndef validate_notebook(path: Path) -> list:\n    \"\"\"Validate a single notebook.\"\"\"\n    issues = []\n\n    with open(path, encoding=\"utf-8\") as f:\n        nb = json.load(f)\n--\n\n    if not notebooks:\n        print(\"\u26a0\ufe0f No notebooks to validate\")\n        sys.exit(0)\n\n    for notebook in notebooks:\n        issues = validate_notebook(notebook)\n        if issues:\n            has_issues = True\n            print(f\"\\n\u274c {notebook}:\")\n            for issue in issues:\n"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/scripts/validate_all_notebooks.py",
        "keyword": "validate",
        "snippet": "            json.dump(self.state, f, indent=2, default=str)\n\n    def validate_notebook(self, notebook_path: Path, mode: str = \"full\") -> dict:\n        \"\"\"Validate a single notebook.\"\"\"\n        result = {\"status\": \"pass\", \"issues\": [], \"last_validated\": datetime.now().isoformat()}\n\n        # Quick structure check\n        try:\n            with open(notebook_path) as f:\n                nb = json.load(f)\n--\n        \"\"\"Generate dashboard view of validation results.\"\"\"\n        if not self.state[\"noteb"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/skills/custom_skills/analyzing-financial-statements/interpret_ratios.py",
        "keyword": "heal",
        "snippet": "            \"pe_ratio\": {\"undervalued\": 14, \"fair\": 20, \"growth\": 28, \"expensive\": 40},\n        },\n        \"healthcare\": {\n            \"current_ratio\": {\"excellent\": 2.3, \"good\": 1.8, \"acceptable\": 1.4, \"poor\": 1.0},\n            \"debt_to_equity\": {\"excellent\": 0.3, \"good\": 0.6, \"acceptable\": 1.0, \"poor\": 1.8},\n            \"roe\": {\"excellent\": 0.22, \"good\": 0.16, \"acceptable\": 0.11, \"poor\": 0.07},\n            \"gross_margin\": {\"excellent\": 0.65, \"good\": 0.45, \"acceptable\": 0.30, \"poor\": 0.20},\n   "
      },
      {
        "repo": "devika",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/devika/src/services/utils.py",
        "keyword": "retry",
        "snippet": "# create wrapper function that will has retry logic of 5 times\nimport sys\nimport time\nfrom functools import wraps\nimport json\n\nfrom src.socket_instance import emit_agent\n\ndef retry_wrapper(func):\n    def wrapper(*args, **kwargs):\n        max_tries = 5\n        tries = 0\n        while tries < max_tries:\n            result = func(*args, **kwargs)\n"
      },
      {
        "repo": "devika",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/devika/src/agents/internal_monologue/internal_monologue.py",
        "keyword": "retry",
        "snippet": "\nfrom src.llm import LLM\nfrom src.services.utils import retry_wrapper, validate_responses\n\nPROMPT = open(\"src/agents/internal_monologue/prompt.jinja2\").read().strip()\n\nclass InternalMonologue:\n    def __init__(self, base_model: str):\n--\n            return response[\"internal_monologue\"]\n\n    @retry_wrapper\n    def execute(self, current_prompt: str, project_name: str) -> str:\n        rendered_prompt = self.render(current_prompt)\n        response = self.llm.inference(rendered_prompt, project_name)\n"
      },
      {
        "repo": "devika",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/devika/src/project.py",
        "keyword": "validate",
        "snippet": "            return None\n\n    def validate_last_message_is_from_user(self, project: str):\n        with Session(self.engine) as session:\n            project_state = session.query(Projects).filter(Projects.project == project).first()\n            if project_state:\n                message_stack = json.loads(project_state.message_stack_json)\n                if message_stack:\n"
      },
      {
        "repo": "devika",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/devika/src/agents/internal_monologue/internal_monologue.py",
        "keyword": "validate",
        "snippet": "\nfrom src.llm import LLM\nfrom src.services.utils import retry_wrapper, validate_responses\n\nPROMPT = open(\"src/agents/internal_monologue/prompt.jinja2\").read().strip()\n\nclass InternalMonologue:\n    def __init__(self, base_model: str):\n--\n        return template.render(current_prompt=current_prompt)\n\n    @validate_responses\n    def validate_response(self, response: str):\n        print('-------------------> ', response)\n        print(\"####\", type(response))\n        if \"internal_monologue\" not in re"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/autogpt/llm/providers/openai.py",
        "keyword": "retry",
        "snippet": "\n\ndef retry_api(\n    max_retries: int = 10,\n    backoff_base: float = 2.0,\n    warn_user: bool = True,\n):\n    \"\"\"Retry an OpenAI API call.\n--\n\n@meter_api\n@retry_api()\ndef create_chat_completion(\n    messages: List[MessageDict],\n    *_,\n    **kwargs,\n) -> OpenAIObject:\n--\n\n@meter_api\n"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/autogpt/commands/image_gen.py",
        "keyword": "retry",
        "snippet": "    }\n\n    retry_count = 0\n    while retry_count < 10:\n        response = requests.post(\n            API_URL,\n            headers=headers,\n            json={\n                \"inputs\": prompt,\n--\n                break\n\n        retry_count += 1\n\n    return f\"Error creating image.\"\n\n\ndef generate_image_with_dalle(\n"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/JavaParser.py",
        "keyword": "recover",
        "snippet": "            localctx.exception = re\n            self._errHandler.reportError(self, re)\n            self._errHandler.recover(self, re)\n        finally:\n            self.exitRule()\n        return localctx\n\n\n--\n            localctx.exception = re\n            self._errHandler.reportError(self, re)\n            self._errHandler.recover(self, re)\n        finally:\n            self.exitRule()\n        return localctx\n\n\n--\n            localctx.exception = re\n            self._errHandler.reportError(self, r"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/prepare_ai_settings.py",
        "keyword": "validate",
        "snippet": "   * 'collect information to understand the bug', where you gather information to understand a bug;\n   * 'collect information to fix the bug', where you gather information to fix the bug;\n   * 'trying out candidate fixes', where you suggest bug fixes that will be validated by a test suite.\napi_budget: 0.0\n\"\"\"\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"name\")\n"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/autogpt/utils.py",
        "keyword": "validate",
        "snippet": "\n\ndef validate_yaml_file(file: str):\n    try:\n        with open(file, encoding=\"utf-8\") as fp:\n            yaml.load(fp.read(), Loader=yaml.FullLoader)\n    except FileNotFoundError:\n        return (False, f\"The file {Fore.CYAN}`{file}`{Fore.RESET} wasn't found\")\n--\n        )\n\n    return (True, f\"Successfully validated {Fore.CYAN}`{file}`{Fore.RESET}!\")\n"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/autogpt/logs/logger.py",
        "keyword": "error_handle",
        "snippet": "\n        # Error handler error.log\n        error_handler = logging.FileHandler(self.log_dir / error_file, \"a\", \"utf-8\")\n        error_handler.setLevel(logging.ERROR)\n        error_formatter = AutoGptFormatter(\n            \"%(asctime)s %(levelname)s %(module)s:%(funcName)s:%(lineno)d %(title)s\"\n            \" %(message_no_color)s\"\n        )\n        error_handler.setFormatter(error_formatter)\n\n        self.typing_logger = logging.getLogger(\"TYPER\")\n        self.typing_logger.addHandler(self.typing_"
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/examples/customer_service_streaming/main.py",
        "keyword": "validate",
        "snippet": "from src.tasks.task import Task\nfrom configs.general import test_root, test_file, engine_name, persist\nfrom src.validator import validate_all_tools, validate_all_assistants\nfrom src.arg_parser import parse_args\n\n\ndef main():\n    args = parse_args()\n    try:\n        validate_all_tools(engine_name)\n        validate_all_assistants()\n    except:\n        raise Exception(\"Validation failed\")\n\n    swarm = Swarm(\n        engine_name=engine_name, persist=persist)\n"
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/examples/customer_service_streaming/src/validator.py",
        "keyword": "validate",
        "snippet": "from src.swarm.assistants import Assistant\n\ndef validate_tool(tool_definition):\n    # Validate the tool using its schema\n    Tool(**tool_definition)  # Uncomment if you have a schema to validate tools\n    print(f\"Validating tool: {tool_definition['function']['name']}\")\n\ndef validate_all_tools(engine):\n    tools_path = os.path.join(os.getcwd(), 'configs/tools')\n    for tool_dir in os.listdir(tools_path):\n        if '__pycache__' in tool_dir:\n            continue\n        tool_dir_path = os.path.jo"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/playwright/async_api/_generated.py",
        "keyword": "retry",
        "snippet": "        max_retries : Union[int, None]\n            Maximum number of times network errors should be retried. Currently only `ECONNRESET` error is retried. Does not\n            retry based on HTTP response codes. An error will be thrown if the limit is exceeded. Defaults to `0` - no retries.\n        timeout : Union[float, None]\n            Request timeout in milliseconds. Defaults to `30000` (30 seconds). Pass `0` to disable timeout.\n\n        Returns\n        -------\n--\n        - When an overlay i"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/tests/async/test_click.py",
        "keyword": "retry",
        "snippet": "    assert \"waiting for element to be visible, enabled and stable\" in error.message\n    assert \"element is not visible\" in error.message\n    assert \"retrying click action\" in error.message\n\n\nasync def test_timeout_waiting_for_visbility_hidden_to_be_gone(\n    page: Page, server: Server\n) -> None:\n--\n    assert \"waiting for element to be visible, enabled and stable\" in error.message\n    assert \"element is not visible\" in error.message\n    assert \"retrying click action\" in error.message\n\n\nasync def"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/tests/async/test_page.py",
        "keyword": "validate",
        "snippet": "@pytest.mark.skip_browser(\"webkit\")\nasync def test_fill_should_throw_on_incorrect_date(page: Page, server: Server) -> None:\n    # Disabled as in upstream, we should validate time in the Playwright lib\n    await page.set_content(\"<input type=date>\")\n    with pytest.raises(Error) as exc_info:\n        await page.fill(\"input\", \"2020-13-05\")\n    assert \"Malformed value\" in exc_info.value.message\n\n--\n@pytest.mark.skip_browser(\"webkit\")\nasync def test_fill_should_throw_on_incorrect_time(page: Page, ser"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/tests/async/test_frames.py",
        "keyword": "validate",
        "snippet": ") -> None:\n    await page.goto(server.EMPTY_PAGE)\n    # validate frameattached events\n    attached_frames = []\n    page.on(\"frameattached\", lambda frame: attached_frames.append(frame))\n    await utils.attach_frame(page, \"frame1\", \"./assets/frame.html\")\n    assert len(attached_frames) == 1\n    assert \"/assets/frame.html\" in attached_frames[0].url\n\n    # validate framenavigated events\n    navigated_frames = []\n    page.on(\"framenavigated\", lambda frame: navigated_frames.append(frame))\n    await pa"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/playwright/_impl/_connection.py",
        "keyword": "error_handle",
        "snippet": "                            self._on_event_listener_error(exc)\n\n                    def _listener_with_error_handler_attached(params: Any) -> None:\n                        potential_future = listener(params)\n                        if asyncio.isfuture(potential_future):\n                            potential_future.add_done_callback(_done_callback)\n\n                    # Each event handler is a potentilly blocking context, create a fiber for each\n                    # and switch to them in order,"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/tests/ci/test_fallback_llm.py",
        "keyword": "retry",
        "snippet": "\n\tdef test_no_switch_on_400_error(self):\n\t\t\"\"\"Test that agent does NOT switch on 400 Bad Request (not retryable).\"\"\"\n\t\tfrom browser_use import Agent\n\n\t\tprimary = create_mock_llm('primary-model')\n\t\tfallback = create_mock_llm('fallback-model')\n\n"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/browser_use/code_use/service.py",
        "keyword": "retry",
        "snippet": "\t\t\t\t\tcode, full_llm_response = await self._get_code_from_llm(step_number=step + 1)\n\t\t\t\texcept Exception as llm_error:\n\t\t\t\t\t# LLM call failed - count as consecutive error and retry\n\t\t\t\t\tself._consecutive_errors += 1\n\t\t\t\t\tlogger.warning(\n\t\t\t\t\t\tf'LLM call failed (consecutive errors: {self._consecutive_errors}/{self.max_failures}), retrying: {llm_error}'\n\t\t\t\t\t)\n\t\t\t\t\tawait self._demo_mode_log(\n\t\t\t\t\t\tf'LLM call failed: {llm_error}',\n\t\t\t\t\t\t'error',\n\t\t\t\t\t\t{'step': step + 1},\n--\n\t\t\t\t\t\tbreak\n\n\t\t\t\t\tawait a"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/browser_use/code_use/service.py",
        "keyword": "recover",
        "snippet": "\t\t\tlogger.warning(f'Token limit issue detected: {issue_message}')\n\t\t\t# Don't add the bad response to history\n\t\t\t# Instead, inject a system message prompting recovery\n\t\t\trecovery_prompt = (\n\t\t\t\tf'Your previous response hit a token limit or became repetitive: {issue_message}\\n\\n'\n\t\t\t\t'Please write a SHORT plan (2 sentences) for what to do next, then execute ONE simple action.'\n\t\t\t)\n\t\t\tself._llm_messages.append(UserMessage(content=recovery_prompt))\n\t\t\t# Return a controlled error message instead of "
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/browser_use/browser/session_manager.py",
        "keyword": "recover",
        "snippet": "\n\t\tself._lock = asyncio.Lock()\n\t\tself._recovery_lock = asyncio.Lock()\n\n\t\t# Focus recovery coordination - event-driven instead of polling\n\t\tself._recovery_in_progress: bool = False\n\t\tself._recovery_complete_event: asyncio.Event | None = None\n\t\tself._recovery_task: asyncio.Task | None = None\n\n\tasync def start_monitoring(self) -> None:\n\t\t\"\"\"Start monitoring Target attach/detach events.\n\n\t\tRegisters CDP event handlers to keep the session pool synchronized with browser state.\n--\n\n\t\t\u26a0\ufe0f INTERNAL API - "
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/browser_use/tools/service.py",
        "keyword": "backup",
        "snippet": "\t\t\t\tresult_data = result.get('result', {})\n\n\t\t\t\t# Check for wasThrown flag (backup error detection)\n\t\t\t\tif result_data.get('wasThrown'):\n\t\t\t\t\tmsg = f'JavaScript code: {code} execution failed (wasThrown=true)'\n\t\t\t\t\tlogger.debug(msg)\n\t\t\t\t\treturn ActionResult(error=msg)\n\n"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/browser_use/browser/watchdogs/storage_state_watchdog.py",
        "keyword": "backup",
        "snippet": "\t\t\t\t# Backup existing file\n\t\t\t\tif json_path.exists():\n\t\t\t\t\tbackup_path = json_path.with_suffix('.json.bak')\n\t\t\t\t\tjson_path.replace(backup_path)\n\n\t\t\t\t# Move temp to final\n\t\t\t\ttemp_path.replace(json_path)\n\n\t\t\t\t# Emit success event\n"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/browser_use/code_use/namespace.py",
        "keyword": "validate",
        "snippet": "\n\nasync def validate_task_completion(\n\ttask: str,\n\toutput: str | None,\n\tllm: BaseChatModel,\n) -> tuple[bool, str]:\n\t\"\"\"\n--\n\n\texcept Exception as e:\n\t\tlogger.warning(f'Failed to validate task completion: {e}')\n\t\t# On error, assume the agent knows what they're doing\n\t\treturn True, f'Validation failed: {e}'\n\n\nasync def evaluate(code: str, browser_session: BrowserSession) -> Any:\n"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/browser_use/code_use/service.py",
        "keyword": "validate",
        "snippet": "\t\t\t\t\tself._consecutive_errors = 0\n\n\t\t\t\t# Check if task is done - validate completion first if not at limits\n\t\t\t\tif self._is_task_done():\n\t\t\t\t\t# Get the final result from namespace (from done() call)\n\t\t\t\t\tfinal_result: str | None = self.namespace.get('_task_result')  # type: ignore[assignment]\n\n\t\t\t\t\t# Check if we should validate (not at step/error limits and under max validations)\n\t\t\t\t\tsteps_remaining = self.max_steps - step - 1\n\t\t\t\t\tshould_validate = (\n\t\t\t\t\t\tself._validation_count < self.max_val"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/examples/use-cases/apply_to_job.py",
        "keyword": "heal",
        "snippet": "\n\nasync def apply_to_rochester_regional_health(info: dict, resume_path: str):\n\t\"\"\"\n\tjson format:\n\t{\n\t    \"first_name\": \"John\",\n\t    \"last_name\": \"Doe\",\n--\n                    - SELECT NO FOR THIS FIELD\n            5) use input_text action to fill out the following:\n                - \"What drew you to healthcare?\"\n            6) use click action to select the following options:\n                - \"How many years of experience do you have in a related role?\"\n                - \"Gender\"\n             "
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/browser_use/browser/session.py",
        "keyword": "heal",
        "snippet": "\t\t\t# Note: These should be handled by dedicated watchdogs:\n\t\t\t# - Security checks (security_watchdog)\n\t\t\t# - Page health checks (crash_watchdog)\n\t\t\t# - Dialog handling (dialog_watchdog)\n\t\t\t# - Download handling (downloads_watchdog)\n\t\t\t# - DOM rebuilding (dom_watchdog)\n\n\t\texcept Exception as e:\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/cli/tools/main.py",
        "keyword": "retry",
        "snippet": "                \"* [bold]Push[/bold] to sync with the remote.\\n\"\n                \"* [bold]Pull[/bold] the latest changes from the remote.\\n\"\n                \"\\nOnce your repository is up-to-date, retry publishing the tool.\"\n            )\n            raise SystemExit()\n\n        project_name = get_project_name(require=True)\n        assert isinstance(project_name, str)  # noqa: S101\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/tools/mcp_tool_wrapper.py",
        "keyword": "retry",
        "snippet": "\n    async def _run_async(self, **kwargs) -> str:\n        \"\"\"Async implementation of MCP tool execution with timeouts and retry logic.\"\"\"\n        return await self._retry_with_exponential_backoff(\n            self._execute_tool_with_timeout, **kwargs\n        )\n\n    async def _retry_with_exponential_backoff(self, operation_func, **kwargs) -> str:\n        \"\"\"Retry operation with exponential backoff, avoiding try-except in loop for performance.\"\"\"\n        last_error = None\n\n        for attempt in r"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/tests/rag/test_error_handling.py",
        "keyword": "recover",
        "snippet": "    mock_client.search.side_effect = [\n        ConnectionError(\"Network interruption\"),\n        [{\"content\": \"recovered result\", \"score\": 0.8, \"metadata\": {\"source\": \"test\"}}],\n    ]\n\n    storage = KnowledgeStorage(collection_name=\"network_test\")\n\n    first_attempt = storage.search([\"test query\"])\n--\n    mock_client.search.side_effect = None\n    mock_client.search.return_value = [\n        {\"content\": \"recovered result\", \"score\": 0.8, \"metadata\": {\"source\": \"test\"}}\n    ]\n\n    second_attempt = st"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai-tools/src/crewai_tools/tools/nl2sql/nl2sql_tool.py",
        "keyword": "rollback",
        "snippet": "\n        except Exception as e:\n            session.rollback()\n            raise e\n\n        finally:\n            session.close()\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/cli/update_crew.py",
        "keyword": "backup",
        "snippet": "\n    # Backup the old pyproject.toml\n    backup_file = \"pyproject-old.toml\"\n    shutil.copy2(input_file, backup_file)\n\n    # Rename the poetry.lock file\n    lock_file = \"poetry.lock\"\n    lock_backup = \"poetry-old.lock\"\n    if os.path.exists(lock_file):\n        os.rename(lock_file, lock_backup)\n    else:\n        pass\n\n    # Write the new pyproject.toml\n    with open(output_file, \"wb\") as f:\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/tests/test_context.py",
        "keyword": "backup",
        "snippet": "        assert get_platform_integration_token() is None\n\n    @patch.dict(os.environ, {\"CREWAI_PLATFORM_INTEGRATION_TOKEN\": \"env-backup\"})\n    def test_platform_context_with_env_fallback(self):\n        \"\"\"Test platform_context interaction with environment variable fallback.\"\"\"\n        context_token = \"context-token\"\n\n        assert get_platform_integration_token() == \"env-backup\"\n\n        with platform_context(context_token):\n            assert get_platform_integration_token() == context_token\n\n "
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/llm.py",
        "keyword": "validate",
        "snippet": "            3. If \"/\" in model name:\n               - Check if prefix is a native provider (openai/anthropic/azure/bedrock/gemini)\n               - If yes, validate model against constants\n               - If valid, route to native SDK; otherwise route to LiteLLM\n        \"\"\"\n        if not model or not isinstance(model, str):\n            raise ValueError(\"Model must be a non-empty string\")\n\n--\n            canonical_provider = provider_mapping.get(prefix.lower())\n\n            if canonical_provide"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/cli/utils.py",
        "keyword": "validate",
        "snippet": "def _load_tools_from_init(init_file: Path) -> list[dict[str, Any]]:\n    \"\"\"\n    Load and validate tools from a given __init__.py file.\n    \"\"\"\n    spec = importlib.util.spec_from_file_location(\"temp_module\", init_file)\n\n    if not spec or not spec.loader:\n        return []\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai-tools/tests/tools/stagehand_tool_test.py",
        "keyword": "heal",
        "snippet": "    assert tool.headless is False\n    assert tool.dom_settle_timeout_ms == 3000\n    assert tool.self_heal is True\n    assert tool.wait_for_captcha_solves is True\n\n\n@patch(\n    \"crewai_tools.tools.stagehand_tool.stagehand_tool.StagehandTool._run\", autospec=True\n--\n        headless=True,\n        dom_settle_timeout_ms=5000,\n        self_heal=False,\n        wait_for_captcha_solves=False,\n        verbose=3,\n        _testing=True,  # Enable testing mode\n    )\n\n--\n    assert tool.headless is True\n    a"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai-tools/src/crewai_tools/tools/stagehand_tool/stagehand_tool.py",
        "keyword": "heal",
        "snippet": "    headless: bool = False\n    dom_settle_timeout_ms: int = 3000\n    self_heal: bool = True\n    wait_for_captcha_solves: bool = True\n    verbose: int = 1\n\n    # Token management settings\n    max_retries_on_token_limit: int = 3\n--\n        headless: bool | None = None,\n        dom_settle_timeout_ms: int | None = None,\n        self_heal: bool | None = None,\n        wait_for_captcha_solves: bool | None = None,\n        verbose: int | None = None,\n        _testing: bool = False,\n        **kwargs,\n    "
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-core/lavague/core/extractors.py",
        "keyword": "retry",
        "snippet": "            return cleaned_yml\n        except yaml.YAMLError:\n            # retry with extra quote in case of truncated output\n            cleaned_yml += '\"'\n            try:\n                yaml.safe_load(cleaned_yml)\n                return cleaned_yml\n            except yaml.YAMLError:\n"
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-core/lavague/core/evaluator.py",
        "keyword": "validate",
        "snippet": "\n\ndef validate_action(action):\n    try:\n        _ = action[\"args\"][\"xpath\"]\n        _ = action[\"name\"]\n        return action[\"name\"] != \"fail\"\n    except:\n--\n            + \".csv\"\n        )\n        results = dataset.loc[dataset[\"validated\"]].copy()\n        results.insert(len(results.columns), \"result_nodes\", None)\n        results.insert(len(results.columns), \"recall\", None)\n        results.insert(len(results.columns), \"output_size\", None)\n        results.insert(len(results.columns), \"time\", None)"
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-core/lavague/core/extractors.py",
        "keyword": "validate",
        "snippet": "from abc import ABC, abstractmethod\nimport re\nfrom jsonschema import validate, ValidationError\nimport yaml\nimport json\nfrom typing import Any, Dict, Tuple\n\n\n--\n            try:\n                # checks if the json returned from the llm matchs the schema\n                validate(\n                    instance=json.loads(match.group(1).strip()), schema=shape_validator\n                )\n            except json.JSONDecodeError:\n                raise ExtractionError(\"Invalid JSON format\")\n            "
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/language_models/llms.py",
        "keyword": "retry",
        "snippet": "    RetryCallState,\n    before_sleep_log,\n    retry,\n    retry_base,\n    retry_if_exception_type,\n    stop_after_attempt,\n    wait_exponential,\n)\nfrom typing_extensions import override\n\n--\n\n\ndef create_base_retry_decorator(\n    error_types: list[type[BaseException]],\n    max_retries: int = 1,\n    run_manager: AsyncCallbackManagerForLLMRun | CallbackManagerForLLMRun | None = None,\n) -> Callable[[Any], Any]:\n    \"\"\"Create a retry decorator for a given LLM and provided a list of error types.\n\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/language_models/chat_models.py",
        "keyword": "retry",
        "snippet": "        | `bind_tools`                 | Create chat model that can call tools.                                                     |\n        | `with_structured_output`     | Create wrapper that structures model output using schema.                                  |\n        | `with_retry`                 | Create wrapper that retries model calls on failure.                                        |\n        | `with_fallbacks`             | Create wrapper that falls back to other models on failure"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/runnables/base.py",
        "keyword": "recover",
        "snippet": "            cls = self.__class__\n            # Then it's a pydantic sub-class, and we have to check\n            # whether it's a generic, and if so recover the original name.\n            if (\n                hasattr(\n                    cls,\n                    \"__pydantic_generic_metadata__\",\n                )\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/tests/unit_tests/indexing/test_indexing.py",
        "keyword": "recover",
        "snippet": "\n\ndef test_index_delete_full_recovery_after_deletion_failure(\n    record_manager: InMemoryRecordManager, vector_store: InMemoryVectorStore\n) -> None:\n    \"\"\"Indexing some content to confirm it gets added only once.\"\"\"\n    loader = ToyLoader(\n        documents=[\n--\n\n\nasync def test_aindex_delete_full_recovery_after_deletion_failure(\n    arecord_manager: InMemoryRecordManager, vector_store: InMemoryVectorStore\n) -> None:\n    \"\"\"Indexing some content to confirm it gets added only once.\"\"\"\n    loade"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/output_parsers/list.py",
        "keyword": "backup",
        "snippet": "            return [item for sublist in reader for item in sublist]\n        except csv.Error:\n            # Keep old logic for backup\n            return [part.strip() for part in text.split(\",\")]\n\n    @property\n    def _type(self) -> str:\n        return \"comma-separated-list\"\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/vectorstores/base.py",
        "keyword": "validate",
        "snippet": "    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_search_type(cls, values: dict) -> Any:\n        \"\"\"Validate search type.\n\n        Args:\n            values: Values to validate.\n\n        Returns:\n            Validated values.\n\n        Raises:\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/structured_query.py",
        "keyword": "validate",
        "snippet": "    \"\"\"Allowed operators for the visitor.\"\"\"\n\n    def _validate_func(self, func: Operator | Comparator) -> None:\n        if (\n            isinstance(func, Operator)\n            and self.allowed_operators is not None\n            and func not in self.allowed_operators\n        ):\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/langchain_v1/tests/unit_tests/agents/middleware/core/test_composition.py",
        "keyword": "error_handle",
        "snippet": "        \"\"\"Test handler converting error to success response.\"\"\"\n\n        def outer_error_handler(request, handler):\n            try:\n                return handler(request)\n            except Exception:\n                # Middleware can return AIMessage - it will be normalized to ModelResponse\n                return AIMessage(content=\"Fallback response\")\n--\n            return handler(request)\n\n        composed = _chain_model_call_handlers([outer_error_handler, inner_passthrough])\n        assert "
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/tests/unit_tests/language_models/chat_models/test_base.py",
        "keyword": "error_handle",
        "snippet": "\n\ndef test_generate_response_from_error_handles_streaming_response_failure() -> None:\n    # Simulates scenario where accessing response.json() or response.text\n    # raises ResponseNotRead on streaming responses\n    response = MockResponse(\n        status_code=400,\n        headers={\"content-type\": \"application/json\"},\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/langchain/langchain_classic/chains/constitutional_ai/prompts.py",
        "keyword": "heal",
        "snippet": "        \"as arson is considered a serious crime. \"\n        \"Doing so could harm yourself and others. \"\n        \"Please reach out to a mental health professional if you are considering this.\",\n    },\n    {\n        \"input_prompt\": \"is it really necessary to ask for consent during sex every \"\n        \"time? Answer:\",\n        \"output_from_model\": \"While it is ideal to ask for consent every time before \"\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/langchain/langchain_classic/chains/api/news_docs.py",
        "keyword": "heal",
        "snippet": "\n    country | The 2-letter ISO 3166-1 code of the country you want to get headlines for. Possible options: ae ar at au be bg br ca ch cn co cu cz de eg fr gb gr hk hu id ie il in it jp kr lt lv ma mx my ng nl no nz ph pl pt ro rs ru sa se sg si sk th tr tw ua us ve za. Note: you can't mix this param with the sources param.\n    category | The category you want to get headlines for. Possible options: business entertainment general health science sports technology. Note: you can't mix this param w"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/oversight/overseer.py",
        "keyword": "retry",
        "snippet": "            except Exception as e:\n                logger.error(f\"Error in overseer monitoring loop: {e}\")\n                await asyncio.sleep(self._next_check_delay)  # Still wait before retry\n\n    def start(self):\n        \"\"\"Start the overseer monitoring task.\"\"\"\n        if not self._monitoring_task:\n            self._stopping = False\n"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/tools/base_agent_tools.py",
        "keyword": "retry",
        "snippet": "                cls(\n                    calling_agent=DemoAgent(),\n                    exit_reason=\"Required external API service is unavailable after multiple retry attempts. Cannot complete the operation.\",\n                ),\n                ToolResult(tool_name=cls.TOOL_NAME, success=True),\n            ),\n            # Example of invalid usage (empty reason)\n            (\n"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/schemas/xml_parsing.py",
        "keyword": "recover",
        "snippet": "with a focus on:\n1. Content preservation (maintaining exact formatting where needed)\n2. Safe parsing (size limits, error recovery)\n3. Clear error reporting\n4. Performance optimization\n\"\"\"\n\nimport re\n"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/agents/implementations/archive_explorer.py",
        "keyword": "rollback",
        "snippet": "- Add checkpoint system between reasoning steps\n- Include explicit dependency tracking between sub-solutions\n- Implement rollback capability when sub-steps fail validation\n\nThis will build on the successful approaches from Iteration 8 while adding\nstronger guarantees for solution coherence. Implementation should focus on\nmaking the verification steps explicit and programmatically enforced.\"\"\",\n                    metrics=make_random_agent_metrics(\n"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/oversight/overseer.py",
        "keyword": "validate",
        "snippet": "\n        try:\n            judgement = TypeAdapter(OverseerJudgement).validate_python(\n                overseer_data_dict\n            )\n            return judgement\n        except Exception as e:\n            logger.error(f\"overseer exception: {e}\")\n"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/oversight/graph_visualisation.py",
        "keyword": "validate",
        "snippet": "    if isinstance(completion, dict):\n        try:\n            completion = Completion.model_validate(completion)\n        except Exception:\n            completion = None\n\n    if not completion:\n        content = textwrap.shorten(event.content, width=max_length, placeholder=\"...\")\n--\n    result = event.metadata.get(\"tool_result\")\n    try:\n        result = ToolResult.model_validate(result)\n    except Exception:\n        pass\n    if not isinstance(result, ToolResult):\n        return f\"[Tool] [Badly f"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/oversight/overseer.py",
        "keyword": "heal",
        "snippet": "<next_check_type>time</next_check_type>\n<next_check_delay>60</next_check_delay>\n<notes_for_next_iteration>Main agent healthy with ~7k tokens/msg and 4-5s ttft - alert if: no progress for >15s, total time >90s, >20s without tool calls, ttft >10s consistently, or if pattern deviates from explore->solve->submit</notes_for_next_iteration>\n{OVERSEER_STOP_TOKEN}\n\nExample 2 - Agent is looping but can be redirected:\n\n<OVERSEER_JUDGEMENT>\n--\n1. Analyze the current execution state\n2. Determine if you have"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/callgraph/manager.py",
        "keyword": "heal",
        "snippet": "            return\n\n    async def get_token_tracking_health(self) -> Dict[str, Any]:\n        \"\"\"Get health metrics about token tracking operations.\"\"\"\n        return {\n            \"stats\": self._token_tracking_stats.copy(),\n            \"success_rate\": (\n                self._token_tracking_stats[\"successful_calls\"]\n                / self._token_tracking_stats[\"total_calls\"]\n"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/gpt_engineer/core/ai.py",
        "keyword": "retry",
        "snippet": "        Perform inference using the language model while implementing an exponential backoff strategy.\n\n        This function will retry the inference in case of a rate limit error from the OpenAI API.\n        It uses an exponential backoff strategy, meaning the wait time between retries increases\n        exponentially. The function will attempt to retry up to 7 times within a span of 45 seconds.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of chat m"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/gpt_engineer/core/ai.py",
        "keyword": "validate",
        "snippet": "        # Modify implicit is_chunk property to ALWAYS false\n        # since Langchain's Message schema is stricter\n        prevalidated_data = [\n            {**item, \"tools\": {**item.get(\"tools\", {}), \"is_chunk\": False}}\n            for item in data\n        ]\n        return list(messages_from_dict(prevalidated_data))  # type: ignore\n\n    def _create_chat_model(self) -> BaseChatModel:\n        \"\"\"\n        Create a chat model with the specified model name and temperature.\n\n"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/gpt_engineer/core/default/steps.py",
        "keyword": "validate",
        "snippet": "\n    diffs = parse_diffs(ai_response, diff_timeout=diff_timeout)\n    # validate and correct diffs\n\n    for _, diff in diffs.items():\n        # if diff is a new file, validation and correction is unnecessary\n        if not diff.is_new_file():\n            problems = diff.validate_and_correct(\n                file_to_lines_dict(files_dict[diff.filename_pre])\n            )\n            error_messages.extend(problems)\n    files_dict = apply_diffs(diffs, files_dict)\n    memory.log(IMPROVE_LOG_FILE, \"\\n"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/tests/applications/cli/test_main.py",
        "keyword": "heal",
        "snippet": "        args()\n\n    #  Runs gpt-engineer with self-heal mode and generates a project after discussing the specification with the AI and self-healing the code.\n    def test_self_heal_mode_generate_project(self, tmp_path, monkeypatch):\n        p = tmp_path / \"projects/example\"\n        p.mkdir(parents=True)\n        (p / \"prompt\").write_text(prompt_text)\n        args = DefaultArgumentsMain(\n            str(p), self_heal_mode=True, llm_via_clipboard=True, no_execution=True\n        )\n        args()\n\n "
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/gpt_engineer/applications/cli/main.py",
        "keyword": "heal",
        "snippet": "from gpt_engineer.core.preprompts_holder import PrepromptsHolder\nfrom gpt_engineer.core.prompt import Prompt\nfrom gpt_engineer.tools.custom_steps import clarified_gen, lite_gen, self_heal\n\napp = typer.Typer(\n    context_settings={\"help_option_names\": [\"-h\", \"--help\"]}\n)  # creates a CLI app\n\n--\n        help=\"Clarify mode - discuss specification with AI before implementation.\",\n    ),\n    self_heal_mode: bool = typer.Option(\n        False,\n        \"--self-heal\",\n        \"-sh\",\n        help=\"Self-"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/evaluation/benchmarks/EDA/game.py",
        "keyword": "retry",
        "snippet": "import openai\nfrom openai import OpenAI\nfrom retry import retry\n\nLOGGER = logging.getLogger(__name__)\n\n\nclass Q20Game:\n--\n        return 0\n\n    @retry(\n        (\n            openai.Timeout,\n            httpx.TimeoutException,\n            openai.RateLimitError,\n            openai.APIError,\n--\n        )\n\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/evaluation/benchmarks/swe_bench/live_utils.py",
        "keyword": "retry",
        "snippet": "                break\n            else:\n                logger.info('Failed to get git diff, retrying...')\n                sleep_if_should_continue(10)\n        elif isinstance(obs, ErrorObservation):\n            logger.error(f'Error occurred: {obs.content}. Retrying...')\n            sleep_if_should_continue(10)\n        else:\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/enterprise/server/clustered_conversation_manager.py",
        "keyword": "recover",
        "snippet": "    - Cross-server message passing via Redis pub/sub\n    - Tracking of conversations and connections across the cluster\n    - Graceful recovery from server failures\n    - Enforcement of conversation limits across the cluster\n    - Cleanup of stale conversations and connections\n\n    The Redis communication uses several key patterns:\n    - ohcnv:{user_id}:{conversation_id} - Marks a conversation as active\n--\n\n        When a server hosting a conversation crashes or is terminated without proper\n    "
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/enterprise/server/mcp/mcp_config.py",
        "keyword": "recover",
        "snippet": "# The second point is very important - any long lived stateful connections (like SSE) will\n# require bespoke implementation to make sure all subsequent requests hit the same replica. It is\n# also not resistant to replica pod restarts (it will kill the connection and there's no recovering from it)\n# NOTE: these details are specific to the MCP protocol\nclass SaaSOpenHandsMCPConfig(OpenHandsMCPConfig):\n    @staticmethod\n    def create_default_mcp_server_config(\n        host: str, config: 'OpenHands"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/tests/unit/app_server/test_db_session_injector.py",
        "keyword": "rollback",
        "snippet": "\n    @pytest.mark.asyncio\n    async def test_depends_rollback_on_exception(self, basic_db_session_injector):\n        \"\"\"Test that managed sessions are rolled back on exceptions.\"\"\"\n        request = MockRequest()\n\n        # Mock the async session maker and session\n        with patch(\n--\n            session = await session_gen.__anext__()\n\n            # The actual rollback testing would require more complex mocking\n            # For now, just verify the session was created\n            assert sess"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/openhands/app_server/services/db_session_injector.py",
        "keyword": "rollback",
        "snippet": "            except Exception:\n                _logger.exception('Rolling back SQL due to error', stack_info=True)\n                await db_session.rollback()\n                raise\n            finally:\n                # If instructed, do not close the db session at the end of the request.\n                if not getattr(state, DB_SESSION_KEEP_OPEN_ATTR, False):\n                    # Clean up the session from request state\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/openhands/core/logger.py",
        "keyword": "backup",
        "snippet": "    log_level: int = logging.INFO,\n    when: str = 'd',\n    backup_count: int = 7,\n    utc: bool = False,\n) -> TimedRotatingFileHandler:\n    \"\"\"Returns a file handler for logging.\"\"\"\n    os.makedirs(log_dir, exist_ok=True)\n    file_name = 'openhands.log'\n--\n        os.path.join(log_dir, file_name),\n        when=when,\n        backupCount=backup_count,\n        utc=utc,\n    )\n    file_handler.setLevel(log_level)\n    if LOG_JSON:\n        file_handler.setFormatter(json_formatter())\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/scripts/update_openapi.py",
        "keyword": "backup",
        "snippet": "  - /api/conversations/{conversation_id}/vscode-url\n  - /api/conversations/{conversation_id}/web-hosts\n- Creates a backup docs/openapi.json.backup before overwriting.\n\nOutput:\n- Prints OpenAPI and API versions, endpoint count, servers count, and sample endpoints.\n\"\"\"\n\n--\n\n\ndef update_openapi_spec(spec_path, backup=True):\n    \"\"\"Update the OpenAPI specification file.\"\"\"\n    # Generate new spec\n    new_spec = generate_openapi_spec()\n\n    # Load current spec for server information\n--\n\n    # Backup "
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/tests/unit/core/config/test_mcp_config.py",
        "keyword": "validate",
        "snippet": "        ]\n    )\n    config.validate_servers()  # Should not raise any exception\n\n\ndef test_empty_sse_config():\n    \"\"\"Test SSE configuration with empty servers list.\"\"\"\n    config = MCPConfig(sse_servers=[])\n    config.validate_servers()\n\n\ndef test_invalid_sse_url():\n    \"\"\"Test SSE configuration with invalid URL format.\"\"\"\n    with pytest.raises(ValidationError) as exc_info:\n--\n    )\n    with pytest.raises(ValueError) as exc_info:\n        config.validate_servers()\n    assert 'Duplicate MCP serv"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/tests/unit/core/config/test_kubernetes_config.py",
        "keyword": "validate",
        "snippet": "\ndef test_kubernetes_config_validation():\n    \"\"\"Test that KubernetesConfig validates input correctly.\"\"\"\n    # Test that extra fields are not allowed\n    with pytest.raises(ValidationError):\n        KubernetesConfig(extra_field='not allowed')\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/openhands/events/stream.py",
        "keyword": "error_handle",
        "snippet": "                        future = pool.submit(callback, event)\n                        future.add_done_callback(\n                            self._make_error_handler(callback_id, key)\n                        )\n\n    def _make_error_handler(\n        self, callback_id: str, subscriber_id: str\n    ) -> Callable[[Any], None]:\n        def _handle_callback_error(fut: Any) -> None:\n            try:\n                # This will raise any exception that occurred during callback execution\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/openhands/server/app.py",
        "keyword": "error_handle",
        "snippet": "\n@app.exception_handler(AuthenticationError)\nasync def authentication_error_handler(request: Request, exc: AuthenticationError):\n    return JSONResponse(\n        status_code=401,\n        content=str(exc),\n    )\n\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/tests/unit/app_server/test_docker_utils.py",
        "keyword": "heal",
        "snippet": "        # With path\n        result = replace_localhost_hostname_for_docker(\n            'http://localhost:3000/api/health'\n        )\n        assert result == 'http://host.docker.internal:3000/api/health'\n\n        # With query parameters containing localhost\n        result = replace_localhost_hostname_for_docker(\n            'http://localhost:8080/path?param=localhost&other=value'\n        )\n--\n        \"\"\"Test complex URL scenarios.\"\"\"\n        # Multiple query parameters and fragments\n        comp"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/tests/unit/app_server/test_process_sandbox_service.py",
        "keyword": "heal",
        "snippet": "        python_executable='python',\n        agent_server_module='openhands.agent_server',\n        health_check_path='/alive',\n        httpx_client=mock_httpx_client,\n    )\n\n\nclass TestProcessSandboxService:\n--\n        assert injector.base_working_dir == '/tmp/openhands-sandboxes'\n        assert injector.base_port == 8000\n        assert injector.health_check_path == '/alive'\n        assert injector.agent_server_module == 'openhands.agent_server'\n\n    def test_custom_values(self):\n        \"\"\"Test "
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter6_to_do_write/src/tools/cmd_runner.py",
        "keyword": "retry",
        "snippet": "   - Add relevant untracked files to the staging area.\n   - Run git status to make sure the commit succeeded.\n4. If the commit fails due to pre-commit hook changes, retry the commit ONCE to include these automated changes. If it fails again, it usually means a pre-commit hook is preventing the commit. If the commit succeeds but you notice that files were modified by the pre-commit hook, you MUST amend your commit to include them.\n\nImportant notes:\n- NEVER update the git config\n- NEVER run additi"
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter7_sub_agent/src/tools/cmd_runner.py",
        "keyword": "retry",
        "snippet": "   - Add relevant untracked files to the staging area.\n   - Run git status to make sure the commit succeeded.\n4. If the commit fails due to pre-commit hook changes, retry the commit ONCE to include these automated changes. If it fails again, it usually means a pre-commit hook is preventing the commit. If the commit succeeds but you notice that files were modified by the pre-commit hook, you MUST amend your commit to include them.\n\nImportant notes:\n- NEVER update the git config\n- NEVER run additi"
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter6_to_do_write/src/tools/smart_context_cropper.py",
        "keyword": "backup",
        "snippet": "    crop_direction = \"BOTTOM\",\n    crop_amount = 3,\n    deleted_messages_summary = \"Failure summary: (1) GC tuning ineffective despite observed long pauses; (2) DB write-lock hypothesis invalid\u2014adding index did not reduce latency; (3) Batch-job correlation inconclusive\u2014disabling related experiment had no effect. Conclusion: Spikes likely due to overlap of scheduled tasks with DB maintenance. Next steps: inspect cron jobs at 00:00 and DB vacuum/backup windows; add temporary rate-limiting or queue"
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter8_mcp_client/src/tools/smart_context_cropper.py",
        "keyword": "backup",
        "snippet": "    crop_direction = \"BOTTOM\",\n    crop_amount = 3,\n    deleted_messages_summary = \"Failure summary: (1) GC tuning ineffective despite observed long pauses; (2) DB write-lock hypothesis invalid\u2014adding index did not reduce latency; (3) Batch-job correlation inconclusive\u2014disabling related experiment had no effect. Conclusion: Spikes likely due to overlap of scheduled tasks with DB maintenance. Next steps: inspect cron jobs at 00:00 and DB vacuum/backup windows; add temporary rate-limiting or queue"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/llm_withtools.py",
        "keyword": "retry",
        "snippet": "def get_response_withtools(\n    client, model, messages, tools, tool_choice,\n    logging=None, max_retry=3\n):\n    try:\n        if 'claude' in model:\n            response = client.messages.create(\n                model=model,\n--\n    except Exception as e:\n        logging(f\"Error in get_response_withtools: {str(e)}\")\n        if max_retry > 0:\n            return get_response_withtools(client, model, messages, tools, tool_choice, logging, max_retry - 1)\n\n        # Hitting the context window limit\n  "
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/coding_agent_polyglot.py",
        "keyword": "backup",
        "snippet": "    # Create and set up file handler\n    os.makedirs(os.path.dirname(log_file), exist_ok=True)\n    file_handler = RotatingFileHandler(log_file, maxBytes=10*1024*1024, backupCount=5)\n    file_handler.setLevel(level)\n    file_handler.setFormatter(file_formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/coding_agent.py",
        "keyword": "backup",
        "snippet": "    # Create and set up file handler\n    os.makedirs(os.path.dirname(log_file), exist_ok=True)\n    file_handler = RotatingFileHandler(log_file, maxBytes=10*1024*1024, backupCount=5)\n    file_handler.setLevel(level)\n    file_handler.setFormatter(file_formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/prompts/self_improvement_prompt.py",
        "keyword": "validate",
        "snippet": "Giving previous attempts and test results as context to the agent may also help.\nThe tests for tasks are not provided in the repo, and the agent needs workflow design to implement them.\nAgent's own tests that validate the solution may not cover all the cases that will be checked by the private tests during official scoring. So the quality of implemented tests are crucial.\n\nPlease analyze the log below to identify how we can improve the testing process and multiple solution attempts.\n\n# Agent Run"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/tools/edit.py",
        "keyword": "validate",
        "snippet": "    return content\n\ndef validate_path(path: str, command: str) -> Path:\n    \"\"\"\n    Validate the file path for each command:\n      - 'view': path may be a file or directory; must exist.\n      - 'create': path must not exist (for new file creation).\n      - 'edit': path must exist (for overwriting).\n--\n    \"\"\"\n    try:\n        path_obj = validate_path(path, command)\n\n        if command == \"view\":\n            return view_path(path_obj)\n\n        elif command == \"create\":\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/tests/help/test_help.py",
        "keyword": "retry",
        "snippet": "class TestHelp(unittest.TestCase):\n    @staticmethod\n    def retry_with_backoff(func, max_time=60, initial_delay=1, backoff_factor=2):\n        \"\"\"\n        Execute a function with exponential backoff retry logic.\n\n        Args:\n            func: Function to execute\n            max_time: Maximum time in seconds to keep retrying\n            initial_delay: Initial delay between retries in seconds\n            backoff_factor: Multiplier for delay after each retry\n\n        Returns:\n            The resu"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/aider/exceptions.py",
        "keyword": "retry",
        "snippet": "class ExInfo:\n    name: str\n    retry: bool\n    description: str\n\n\nEXCEPTIONS = [\n    ExInfo(\"APIConnectionError\", True, None),\n--\n                )\n\n        # Check for specific non-retryable APIError cases like insufficient credits\n        if ex.__class__ is litellm.APIError:\n            err_str = str(ex).lower()\n            if \"insufficient credits\" in err_str and '\"code\":402' in err_str:\n                return ExInfo(\n                    \"APIError\",\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/aider/mdstream.py",
        "keyword": "rollback",
        "snippet": "        The unstable lines emit into the Live window so they can be repainted.\n\n        Markdown going to the console works better in terminal scrollback buffers.\n        The live window doesn't play nice with terminal scrollback.\n        \"\"\"\n        # On the first call, stop the spinner and start the Live renderer\n        if not getattr(self, \"_live_started\", False):\n            self.live = Live(Text(\"\"), refresh_per_second=1.0 / self.min_delay)\n            self.live.start()\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/tests/basic/test_watch.py",
        "keyword": "backup",
        "snippet": "    assert spec.match_file(\".aider.conf\")\n    assert spec.match_file(\".git/config\")\n    assert spec.match_file(\"file~\")  # Emacs/vim backup\n    assert spec.match_file(\"file.bak\")\n    assert spec.match_file(\"file.swp\")\n    assert spec.match_file(\"file.swo\")\n    assert spec.match_file(\"#temp#\")  # Emacs auto-save\n    assert spec.match_file(\".#lock\")  # Emacs lock\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/aider/watch.py",
        "keyword": "backup",
        "snippet": "        \".aider*\",\n        \".git\",\n        # Common editor backup/temp files\n        \"*~\",  # Emacs/vim backup\n        \"*.bak\",  # Generic backup\n        \"*.swp\",  # Vim swap\n        \"*.swo\",  # Vim swap\n        \"\\\\#*\\\\#\",  # Emacs auto-save\n        \".#*\",  # Emacs lock files\n        \"*.tmp\",  # Generic temp files\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/aider/models.py",
        "keyword": "validate",
        "snippet": "\n        # Are all needed keys/params available?\n        res = self.validate_environment()\n        self.missing_keys = res.get(\"missing_keys\")\n        self.keys_in_environment = res.get(\"keys_in_environment\")\n\n        max_input_tokens = self.info.get(\"max_input_tokens\") or 0\n        # Calculate max_chat_history_tokens as 1/16th of max_input_tokens,\n--\n            return img.size\n\n    def fast_validate_environment(self):\n        \"\"\"Fast path for common models. Avoids forcing litellm import.\"\"\"\n\n "
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/aider/io.py",
        "keyword": "validate",
        "snippet": "\n        # Validate color settings after console is initialized\n        self._validate_color_settings()\n\n    def _validate_color_settings(self):\n        \"\"\"Validate configured color strings and reset invalid ones.\"\"\"\n        color_attributes = [\n            \"user_input_color\",\n            \"tool_output_color\",\n            \"tool_error_color\",\n--\n            if color_value:\n                try:\n                    # Try creating a style to validate the color\n                    RichStyle(color=colo"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-agentchat/src/autogen_agentchat/messages.py",
        "keyword": "retry",
        "snippet": "    \"\"\"An event signaling code generation event.\"\"\"\n\n    retry_attempt: int\n    \"Retry number, 0 means first generation\"\n\n    content: str\n    \"The complete content as string.\"\n\n--\n    \"\"\"An event signaling code execution event.\"\"\"\n\n    retry_attempt: int\n    \"Retry number, 0 means first execution\"\n\n    result: CodeResult\n    \"Code Execution Result\"\n\n"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-agentchat/tests/test_code_executor_agent.py",
        "keyword": "retry",
        "snippet": "```\n\"\"\",\n            \"\"\"{\"retry\": \"true\", \"reason\": \"Retry 1: It is a test environment\"}\"\"\",\n            f\"\"\"\nHere is the updated code to calculate the mean of 10, 20, 30, 40, 50\n\n```{language}\n{correct_code_block}\n--\n    incorrect_code_generation_event: CodeGenerationEvent | None = None\n    correct_code_generation_event: CodeGenerationEvent | None = None\n    retry_decision_event: CodeGenerationEvent | None = None\n    incorrect_code_execution_event: CodeExecutionEvent | None = None\n    correct_c"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-agentchat/src/autogen_agentchat/messages.py",
        "keyword": "recover",
        "snippet": "        message_type = data.get(\"type\")\n        if message_type is None:\n            raise ValueError(\"Field 'type' is required in the message data to recover the message type.\")\n        if message_type not in self._message_types:\n            raise ValueError(f\"Unknown message type: {message_type}\")\n        if not isinstance(message_type, str):\n            raise ValueError(f\"Message type must be a string, got {type(message_type)}\")\n\n"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-agentchat/tests/test_assistant_agent.py",
        "keyword": "recover",
        "snippet": "\n    @pytest.mark.asyncio\n    async def test_error_recovery_in_complex_workflow(self) -> None:\n        \"\"\"Test error recovery in complex workflow with multiple failures.\"\"\"\n\n        def failing_tool(param: str) -> str:\n            if param == \"fail\":\n                raise ValueError(\"Tool failure\")\n            return f\"Success: {param}\"\n--\n\n        agent = AssistantAgent(\n            name=\"error_recovery_agent\",\n            model_client=model_client,\n            tools=[failing_tool],\n           "
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-studio/autogenstudio/database/db_manager.py",
        "keyword": "rollback",
        "snippet": "\n                except Exception as e:\n                    session.rollback()\n                    raise e\n                finally:\n                    session.close()\n                    self._init_lock.release()\n\n--\n                session.refresh(model)\n            except Exception as e:\n                session.rollback()\n                logger.error(\"Error while updating/creating \" + str(model_class.__name__) + \": \" + str(e))\n                status = False\n\n        return Response(\n         "
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-ext/src/autogen_ext/models/openai/_openai_client.py",
        "keyword": "backup",
        "snippet": "        # even when result.usage is not None\n        usage = RequestUsage(\n            # TODO backup token counting\n            prompt_tokens=getattr(result.usage, \"prompt_tokens\", 0) if result.usage is not None else 0,\n            completion_tokens=getattr(result.usage, \"completion_tokens\", 0) if result.usage is not None else 0,\n        )\n\n        logger.info(\n"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-ext/src/autogen_ext/models/ollama/_ollama_client.py",
        "keyword": "backup",
        "snippet": "\n        usage = RequestUsage(\n            # TODO backup token counting\n            prompt_tokens=result.prompt_eval_count if result.prompt_eval_count is not None else 0,\n            completion_tokens=(result.eval_count if result.eval_count is not None else 0),\n        )\n\n        logger.info(\n"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/samples/core_chess_game/main.py",
        "keyword": "validate",
        "snippet": "\n\ndef validate_turn(board: Board, player: Literal[\"white\", \"black\"]) -> None:\n    \"\"\"Validate that it is the player's turn to move.\"\"\"\n    last_move = board.peek() if board.move_stack else None\n    if last_move is not None:\n        if player == \"white\" and board.color_at(last_move.to_square) == WHITE:\n            raise ValueError(\"It is not your turn to move. Wait for black to move.\")\n--\n) -> Annotated[str, \"A list of legal moves in UCI format.\"]:\n    \"\"\"Get legal moves for the given player.\"\"\"\n"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/samples/agentchat_fastapi/app_team.py",
        "keyword": "validate",
        "snippet": "        try:\n            data = await websocket.receive_json()\n            message = TextMessage.model_validate(data)\n            return message.content\n        except WebSocketDisconnect:\n            # Client disconnected while waiting for input - this is the root cause of the issue\n            logger.info(\"Client disconnected while waiting for user input\")\n            raise  # Let WebSocketDisconnect propagate to be handled by outer try/except\n--\n            # Get user message.\n            dat"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-studio/autogenstudio/web/app.py",
        "keyword": "error_handle",
        "snippet": "\n@app.exception_handler(500)\nasync def internal_error_handler(request, exc):\n    logger.error(f\"Internal error: {str(exc)}\")\n    return {\n        \"status\": False,\n        \"message\": \"Internal server error\",\n        \"detail\": str(exc) if settings.API_DOCS else \"Internal server error\",\n"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-studio/autogenstudio/web/app.py",
        "keyword": "heal",
        "snippet": "\n\n@api.get(\"/health\")\nasync def health_check():\n    \"\"\"API health check endpoint\"\"\"\n    return {\n        \"status\": True,\n        \"message\": \"Service is healthy\",\n    }\n\n\n# Mount static file directories\napp.mount(\"/api\", api)\n"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-studio/autogenstudio/web/auth/models.py",
        "keyword": "heal",
        "snippet": "    exclude_paths: List[str] = [\n        \"/\",  # root for serving frontend\n        \"/api/health\",\n        \"/api/version\",\n        \"/api/auth/login-url\",\n        \"/api/auth/callback-handler\",\n        \"/api/auth/callback\",\n        \"/api/auth/type\",\n"
      }
    ],
    "context_building": [
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/templates/vite_react_swagger.py",
        "keyword": "context",
        "snippet": "        \"client/src/components/ui/collapsible.tsx\": \"Shadcn Collapsible component. Exports Collapsible, CollapsibleTrigger, CollapsibleContent.\",\n        \"client/src/components/ui/command.tsx\": \"Shadcn Command component. Exports Command, CommandDialog, CommandInput, CommandList, CommandEmpty, CommandGroup, CommandItem, CommandShortcut, CommandSeparator.\",\n        \"client/src/components/ui/context-menu.tsx\": \"Shadcn Context Menu component. Exports ContextMenu, ContextMenuTrigger, ContextMenuConte"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/tests/conftest.py",
        "keyword": "context",
        "snippet": "    Set up a temporary in-memory database for testing.\n\n    This fixture is an async context manager.\n    \"\"\"\n    db_cfg = DBConfig(url=\"sqlite+aiosqlite:///:memory:\")\n    manager = SessionManager(db_cfg)\n    async with manager.engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n--\n    Set up a temporary in-memory database for testing.\n\n    This fixture is an async context manager that yields\n    a database session.\n    \"\"\"\n    async with testmanager as db:\n        yiel"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/tests/agents/test_base.py",
        "keyword": "chunk",
        "snippet": "    agent = AgentUnderTest(sm, ui)\n\n    await agent.stream_handler(\"chunk\")\n    ui.send_stream_chunk.assert_called_once_with(\n        \"chunk\", source=agent.ui_source, project_state_id=str(agent.current_state.id), route=None\n    )\n\n\n@pytest.mark.asyncio\nasync def test_ask_question():\n"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/tests/ui/test_console.py",
        "keyword": "chunk",
        "snippet": "    await ui.start()\n    for word in [\"Hell\u00f8 \", \"fr\u00f6m \", \"the \", \"other \", \"\u0161ide \", \"\u266b\"]:\n        await ui.send_stream_chunk(word, source=src)\n\n    captured = capsys.readouterr()\n    assert captured.out == \"Hell\u00f8 fr\u00f6m the other \u0161ide \u266b\"\n    await ui.stop()\n\n"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/tests/cli/test_cli.py",
        "keyword": "token",
        "snippet": "        \"--extension-version\",\n        \"--use-git\",\n        \"--access-token\",\n        \"--no-auto-confirm-breakdown\",\n        \"--initial-prompt\",\n    }\n\n    parser.parse_args.assert_called_once_with()\n"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/cli/helpers.py",
        "keyword": "token",
        "snippet": "        --extension-version: Version of the VSCode extension, if used\n        --use-git: Use Git for version control\n        --access-token: Access token\n        --initial-prompt: Initial prompt to automatically start a new project with 'node' stack\n    :return: Parsed arguments object.\n    \"\"\"\n    version = get_version()\n\n--\n        required=False,\n    )\n    parser.add_argument(\"--access-token\", help=\"Access token\", required=False)\n    parser.add_argument(\n        \"--enable-api-server\",\n       "
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/agents/spec_writer.py",
        "keyword": "relevant",
        "snippet": "        summary = await template.apply()\n\n        self.next_state.relevant_files = template.relevant_files\n        self.next_state.modified_files = {}\n        self.next_state.specification.template_summary = summary\n\n    async def initialize_spec_and_project(self) -> AgentResponse:\n        self.next_state.action = SPEC_CREATE_STEP_NAME\n"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/agents/importer.py",
        "keyword": "relevant",
        "snippet": "        convo = AgentConvo(self).template(\"get_entrypoints\")\n        llm_response = await llm(convo, parser=JSONParser())\n        relevant_files = [f for f in self.current_state.files if f.path in llm_response]\n\n        self.send_message(\"Analyzing project ...\")\n\n        convo = AgentConvo(self).template(\n            \"analyze_project\", relevant_files=relevant_files, example_spec=EXAMPLE_PROJECT_DESCRIPTION\n        )\n        llm_response = await llm(convo)\n\n        spec = self.current_state.speci"
      },
      {
        "repo": "gpt-pilot",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-pilot/core/agents/code_monkey.py",
        "keyword": "summarize",
        "snippet": "class FileDescription(BaseModel):\n    summary: str = Field(\n        description=\"Detailed description summarized what the file is about, and what the major classes, functions, elements or other functionality is implemented.\"\n    )\n    references: list[str] = Field(\n        description=\"List of references the file imports or includes (only files local to the project), where each element specifies the project-relative path of the referenced file, including the file extension.\"\n    )\n\n"
      },
      {
        "repo": "plandex",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/plandex/app/server/litellm_proxy.py",
        "keyword": "chunk",
        "snippet": "      def stream_generator():\n        try:  \n          for chunk in response_stream:\n            yield f\"data: {json.dumps(chunk.to_dict())}\\n\\n\"\n          yield \"data: [DONE]\\n\\n\"\n        except Exception as e:\n          # surface the problem to the client _inside_ the SSE stream\n          yield f\"data: {json.dumps({'error': str(e)})}\\n\\n\"\n          return\n"
      },
      {
        "repo": "plandex",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/plandex/app/server/litellm_proxy.py",
        "keyword": "token",
        "snippet": "  )\n\n  # remove x-api-key when we detect an OAuth access-token\n  print(f\"api_key: {api_key}\")\n  if api_key and api_key.startswith((\"sk-ant-oat\", \"sk-ant-oau\")):\n    hdrs[\"anthropic-beta\"] = \"oauth-2025-04-20\"\n    hdrs[\"anthropic-product\"] = \"claude-code\"\n    hdrs.pop(\"x-api-key\", None)\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/scripts/clean_metadata.py",
        "keyword": "context",
        "snippet": "    script_dir = Path(__file__).parent.resolve()\n    # Adjust path relative to the script's location in the aider repo\n    litellm_path = script_dir.parent / \"../litellm/model_prices_and_context_window.json\"\n    aider_path = script_dir / \"../aider/resources/model-metadata.json\"\n\n    if not litellm_path.exists():\n        print(f\"Error: LiteLLM metadata file not found at {litellm_path}\")\n        return\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/benchmark/benchmark.py",
        "keyword": "context",
        "snippet": "    num_tests: int = typer.Option(-1, \"--num-tests\", \"-n\", help=\"Number of tests to run\"),\n    num_ctx: Optional[int] = typer.Option(\n        None, \"--num-ctx\", help=\"Override model context window size\"\n    ),\n    read_model_settings: str = typer.Option(\n        None, \"--read-model-settings\", help=\"Load aider model settings from YAML file\"\n    ),\n    reasoning_effort: Optional[str] = typer.Option(\n--\n    res.user_asks = 0\n    res.test_timeouts = 0\n    res.exhausted_context_windows = 0\n    res.nu"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/tests/scrape/test_playwright_disable.py",
        "keyword": "chunk",
        "snippet": "            return []\n\n        def format_chat_chunks(self):\n            return type(\"Chunks\", (), {\"repo\": [], \"readonly_files\": [], \"chat_files\": []})()\n\n        def event(self, *a, **k):\n            pass\n\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/tests/basic/test_coder.py",
        "keyword": "chunk",
        "snippet": "\n        # Get the formatted messages\n        chunks = coder.format_messages()\n        messages = chunks.all_messages()\n\n        # Check if the system message contains our prefix\n        system_message = next(msg for msg in messages if msg[\"role\"] == \"system\")\n        self.assertTrue(system_message[\"content\"].startswith(test_prefix))\n\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/scripts/issues.py",
        "keyword": "token",
        "snippet": "TOKEN = os.getenv(\"GITHUB_TOKEN\")\n\nheaders = {\"Authorization\": f\"token {TOKEN}\", \"Accept\": \"application/vnd.github.v3+json\"}\n\n\ndef get_issues(state=\"open\"):\n    issues = []\n    page = 1\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/scripts/homepage.py",
        "keyword": "token",
        "snippet": "GITHUB_STARS_TOOLTIP = \"Total number of GitHub stars the Aider project has received\"\nPYPI_DOWNLOADS_TOOLTIP = \"Total number of installations via pip from PyPI\"\nTOKENS_WEEKLY_TOOLTIP = \"Number of tokens processed weekly by Aider users\"\nOPENROUTER_TOOLTIP = \"Aider's ranking among applications on the OpenRouter platform\"\nSINGULARITY_TOOLTIP = \"Percentage of the new code in Aider's last release written by Aider itself\"\n\n# Cache settings\nCACHE_DIR = os.path.expanduser(\"~/.cache/aider-badges\")\n--\n    "
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/scripts/30k-image.py",
        "keyword": "embed",
        "snippet": "\n\ndef embed_font():\n    \"\"\"Returns base64 encoded font data for the GlassTTYVT220 font.\"\"\"\n    # Path to the font file\n    font_path = (\n        Path(__file__).parent.parent / \"aider\" / \"website\" / \"assets\" / \"Glass_TTY_VT220.ttf\"\n    )\n--\n    \"\"\"Generate a celebratory SVG for 30K GitHub stars.\"\"\"\n\n    # Font embedding\n    font_data = embed_font()\n    font_face = f\"\"\"\n    @font-face {{\n        font-family: 'GlassTTYVT220';\n        src: url(data:font/truetype;charset=utf-8;base64,{font_data}) for"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/scripts/logo_svg.py",
        "keyword": "embed",
        "snippet": "#!/usr/bin/env python3\n\"\"\"\nScript to generate an SVG logo for Aider with embedded font.\nReads the Glass_TTY_VT220.ttf font, subsets it to only include the letters needed,\nand creates an SVG with the word \"aider\" in terminal green (#14b014) on a transparent background.\n\"\"\"\n\nimport argparse\n--\n\n\ndef generate_svg_with_embedded_font(font_path, text=\"aider\", color=\"#14b014\", output_path=None):\n    \"\"\"\n    Generate an SVG with embedded TTF font data.\n\n    Args:\n        font_path (str): Path to the TTF"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/scripts/update-history.py",
        "keyword": "relevant",
        "snippet": "    diff_content = run_git_diff()\n\n    # Extract relevant portion of HISTORY.md\n    latest_ver = get_latest_version_from_history()\n    with open(\"HISTORY.md\", \"r\") as f:\n        history_content = f.read()\n\n    # Find the section for this version\n--\n    if next_version_idx == -1:\n        # No next version found, use the rest of the file\n        relevant_history = history_content[start_idx:]\n    else:\n        # Extract just up to the next version\n        relevant_history = history_content[start_id"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/aider/prompts.py",
        "keyword": "relevant",
        "snippet": "undo_command_reply = (\n    \"I did `git reset --hard HEAD~1` to discard the last edits. Please wait for further\"\n    \" instructions before attempting that change again. Feel free to ask relevant questions about\"\n    \" why the changes were reverted.\"\n)\n\nadded_files = (\n    \"I added these files to the chat: {fnames}\\nLet me know if there are others we should add.\"\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/benchmark/benchmark.py",
        "keyword": "summarize",
        "snippet": "    raw_rows = []\n    for dirname in dirnames:\n        row = summarize_results(dirname, stats_languages)\n        raw_rows.append(row)\n\n    # return\n\n    seen = dict()\n--\n\n            all_results.append(results)\n            summarize_results(dirname)\n            if sleep:\n                time.sleep(sleep)\n    else:\n        run_test_threaded = lox.thread(threads)(run_test)\n        for test_path in test_dnames:\n--\n    print()\n    print()\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/tests/basic/test_coder.py",
        "keyword": "summarize",
        "snippet": "                coder.cur_messages = []\n                coder.done_messages = []\n                coder.summarizer = MagicMock()\n                coder.summarizer.too_big.return_value = False\n\n                # Mock editor_coder creation and execution\n                mock_editor = MagicMock()\n                with patch(\"aider.coders.architect_coder.Coder.create\", return_value=mock_editor):\n                    # Set partial response content\n--\n                coder.cur_messages = []\n               "
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/scripts/recording_audio.py",
        "keyword": "compress",
        "snippet": "\n\ndef compress_audio(input_file, output_file, bitrate=MP3_BITRATE):\n    \"\"\"Compress audio file using FFmpeg.\"\"\"\n    if not check_ffmpeg():\n        print(\"Warning: FFmpeg not found, skipping compression\")\n        return False\n\n    try:\n        subprocess.run(\n            [\n--\n        return True\n    except subprocess.SubprocessError as e:\n        print(f\"Error compressing audio: {e}\")\n        return False\n\n\ndef generate_audio_openai(text, output_file, voice=VOICE, bitrate=MP3_BITRATE):\n    \"\"\"Gen"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/scripts/clean_metadata.py",
        "keyword": "window",
        "snippet": "    script_dir = Path(__file__).parent.resolve()\n    # Adjust path relative to the script's location in the aider repo\n    litellm_path = script_dir.parent / \"../litellm/model_prices_and_context_window.json\"\n    aider_path = script_dir / \"../aider/resources/model-metadata.json\"\n\n    if not litellm_path.exists():\n        print(f\"Error: LiteLLM metadata file not found at {litellm_path}\")\n        return\n"
      },
      {
        "repo": "aider",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/aider/tests/help/test_help.py",
        "keyword": "window",
        "snippet": "        self.assertEqual(fname_to_url(\"/home/user/project/website/_includes/header.md\"), \"\")\n\n    def test_fname_to_url_windows(self):\n        # Test relative Windows-style paths\n        self.assertEqual(fname_to_url(r\"website\\docs\\index.md\"), \"https://aider.chat/docs\")\n        self.assertEqual(\n            fname_to_url(r\"website\\docs\\usage.md\"), \"https://aider.chat/docs/usage.html\"\n        )\n"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/tool_use/utils/team_expense_api.py",
        "keyword": "context",
        "snippet": "        - payment_method: How it was paid (corporate_card, personal_reimbursement)\n        - project_code: Project or cost center code\n        - notes: Employee justification or additional context\n    \"\"\"\n\n    time.sleep(DELAY_MULTIPLIER * 0.2)\n\n    # Generate a deterministic but varied number of expenses based on employee_id\n"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/claude_agent_sdk/research_agent/agent.py",
        "keyword": "context",
        "snippet": "    display_agent_response,\n    print_activity,\n    reset_activity_context,\n)\n\nfrom claude_agent_sdk import ClaudeAgentOptions, ClaudeSDKClient\n\nload_dotenv()\n--\n    Note:\n        For the activity_handler - we support both sync and async handlers\n        to make the module work in different contexts:\n            - Sync handlers (like print_activity) for simple console output\n            - Async handlers for web apps that need WebSocket/network I/O\n        In production, you'd typically use just "
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/capabilities/contextual-embeddings/contextual-rag-lambda-function/inference_adapter.py",
        "keyword": "chunk",
        "snippet": "\n            for event in response.get(\"body\"):\n                chunk = json.loads(event[\"chunk\"][\"bytes\"].decode())\n                if chunk[\"type\"] == \"content_block_delta\":\n                    yield chunk[\"delta\"][\"text\"]\n                elif chunk[\"type\"] == \"message_delta\":\n                    if \"stop_reason\" in chunk[\"delta\"]:\n                        break\n\n        except ClientError as e:\n            print(f\"An error occurred: {e}\")\n            yield None\n"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/capabilities/contextual-embeddings/contextual-rag-lambda-function/lambda_function.py",
        "keyword": "chunk",
        "snippet": "\n\n    Here is the chunk we want to situate within the whole document\n    <chunk>\n    {chunk_content}\n    </chunk>\n\n\n    Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.\n    Answer only with the succinct context and nothing else.\n    \"\"\"\n\n\ndef lambda_handler(event, context):\n--\n        processed_batches = []\n        for batch in input_file.get(\"contentBatches\"):\n            # Get chunks from S3\n    "
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/claude_agent_sdk/observability_agent/agent.py",
        "keyword": "token",
        "snippet": "        MCP server config dict, or empty dict if GITHUB_TOKEN not set.\n    \"\"\"\n    token = os.environ.get(\"GITHUB_TOKEN\")\n    if not token:\n        return {}\n\n    return {\n        \"github\": {\n            \"command\": \"docker\",\n--\n                \"ghcr.io/github/github-mcp-server\",\n            ],\n            \"env\": {\"GITHUB_PERSONAL_ACCESS_TOKEN\": token},\n        }\n    }\n\n\nasync def send_query(\n"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/capabilities/summarization/evaluation/custom_evals/llm_eval.py",
        "keyword": "token",
        "snippet": "    response = client.messages.create(\n        model=\"claude-sonnet-4-5\",\n        max_tokens=1000,\n        temperature=0,\n        messages=[{\"role\": \"user\", \"content\": prompt}, {\"role\": \"assistant\", \"content\": \"<json>\"}],\n        stop_sequences=[\"</json>\"],\n    )\n\n"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/capabilities/text_to_sql/evaluation/prompts.py",
        "keyword": "embed",
        "snippet": "    user_query = context[\"vars\"][\"user_query\"]\n\n    if not vectordb.embeddings:\n        with sqlite3.connect(DATABASE_PATH) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n            schema_data = [\n                {\n"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/claude_agent_sdk/utils/html_renderer.py",
        "keyword": "embed",
        "snippet": "\n    Returns:\n        HTML string with embedded base64 image\n    \"\"\"\n    b64 = _image_to_base64(image_path)\n    return (\n        f'<img src=\"data:image/png;base64,{b64}\" '\n        f'alt=\"Image\" style=\"max-width:100%; height:auto; border-radius:8px;\">'\n"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/claude_agent_sdk/utils/agent_visualizer.py",
        "keyword": "relevant",
        "snippet": "\ndef _format_tool_info(tool_name: str, tool_input: dict) -> str:\n    \"\"\"Format tool information with relevant parameters.\"\"\"\n    info_parts = [tool_name]\n\n    if tool_input:\n        if tool_name == \"WebSearch\" and \"query\" in tool_input:\n            info_parts.append(f'\u2192 \"{tool_input[\"query\"]}\"')\n"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/capabilities/retrieval_augmented_generation/evaluation/prompts.py",
        "keyword": "relevant",
        "snippet": "    prompt = f\"\"\"\n    Query: {query}\n    You are about to be given a group of documents, each preceded by its index number in square brackets. Your task is to select the only {k} most relevant documents from the list to help us answer the query.\n\n    {joined_summaries}\n\n    Output only the indices of {k} most relevant documents in order of relevance, separated by commas, enclosed in XML tags here:\n    <relevant_indices>put the numbers of your indices here, seeparted by commas</relevant_indices>\n"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/capabilities/summarization/evaluation/prompts.py",
        "keyword": "summarize",
        "snippet": "def basic_summarize(text):\n    prompt = f\"\"\"\n    You are a legal analyst known for highly accurate and detailed summaries of legal documents.\n    Summarize the following text in bullet points. Focus on the main ideas and key details:\n\n    {text}\n--\n\n\ndef summarize_long_document(text):\n    prompt = f\"\"\"\n    You are a legal analyst specializing in real estate law, known for highly accurate and detailed summaries of sublease agreements.\n\n    Summarize the following sublease agreement. Focus on thes"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/capabilities/summarization/evaluation/custom_evals/llm_eval.py",
        "keyword": "summarize",
        "snippet": "    Args:\n    summary (str): The summary to evaluate.\n    input (str): The original text that was summarized.\n\n    Returns:\n    bool: True if the average score is above the threshold, False otherwise.\n    \"\"\"\n    client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n"
      },
      {
        "repo": "anthropic-cookbook",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/anthropic-cookbook/capabilities/summarization/data/multiple_subleases.py",
        "keyword": "window",
        "snippet": "6. MAINTENANCE AND REPAIRS\n\n6.1 Sublessee's Obligations: Sublessee shall, at its sole cost and expense, keep and maintain the Subleased Premises and every part thereof in good order, condition, and repair, ordinary wear and tear excepted. Sublessee's obligations shall include, without limitation, the maintenance, repair, and replacement of all interior walls, floors, ceilings, doors, windows, and fixtures within the Subleased Premises.\n\n6.2 Sublessor's Obligations: Sublessor shall have no obliga"
      },
      {
        "repo": "devika",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/devika/src/browser/interaction.py",
        "keyword": "context",
        "snippet": "\tdef go_to_page(self, url):\n\t\tself.page.goto(url=url if \"://\" in url else \"http://\" + url)\n\t\tself.client = self.page.context.new_cdp_session(self.page)\n\t\tself.page_element_buffer = {}\n\n\tdef scroll(self, direction):\n\t\tif direction == \"up\":\n\t\t\tself.page.evaluate(\n"
      },
      {
        "repo": "devika",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/devika/src/agents/agent.py",
        "keyword": "context",
        "snippet": "\n        \"\"\"\n        Accumulate contextual keywords from chained prompts of all preparation agents\n        \"\"\"\n        self.collected_context_keywords = []\n\n        \"\"\"\n        Agents\n        \"\"\"\n        self.planner = Planner(base_model=base_model)\n--\n        return results\n\n    def update_contextual_keywords(self, sentence: str):\n        \"\"\"\n            Update the context keywords with the latest sentence/prompt\n        \"\"\"\n        keywords = SentenceBert(sentence).extract_keywords()\n        f"
      },
      {
        "repo": "devika",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/devika/devika.py",
        "keyword": "token",
        "snippet": "import logging\nfrom threading import Thread\nimport tiktoken\n\nfrom src.apis.project import project_bp\nfrom src.config import Config\nfrom src.logger import Logger, route_logger\nfrom src.project import ProjectManager\n--\n\n\nTIKTOKEN_ENC = tiktoken.get_encoding(\"cl100k_base\")\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nmanager = ProjectManager()\nAgentState = AgentState()\n--\n\n\n"
      },
      {
        "repo": "devika",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/devika/src/state.py",
        "keyword": "token",
        "snippet": "            \"completed\": False,\n            \"agent_is_active\": True,\n            \"token_usage\": 0,\n            \"timestamp\": timestamp\n        }\n\n    def create_state(self, project: str):\n        with Session(self.engine) as session:\n--\n            return None\n            \n    def update_token_usage(self, project: str, token_usage: int):\n        with Session(self.engine) as session:\n            agent_state = session.query(AgentStateModel).filter(AgentStateModel.project == project).first()\n       "
      },
      {
        "repo": "devika",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/devika/src/browser/interaction.py",
        "keyword": "window",
        "snippet": "\t(1) an objective that you are trying to achieve\n\t(2) the URL of your current web page\n\t(3) a simplified text description of what's visible in the browser window (more on that below)\n\nYou can issue these commands:\n\tSCROLL UP - scroll up one page\n\tSCROLL DOWN - scroll down one page\n\tCLICK X - click on a given element. You can only click on links, buttons, and inputs!\n--\n\t\tif direction == \"up\":\n\t\t\tself.page.evaluate(\n\t\t\t\t\"(document.scrollingElement || document.body).scrollTop = (document.scrolling"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/construct_commands_descriptions.py",
        "keyword": "context",
        "snippet": "extract_method_desc = \"\"\"extract_method_code: This command allows you to extract possible implementations of a given method name inside a file. The required params to call this command are: (project_name: string, bug_index: integer, filepath: string, method_name: string)\"\"\"\n\ngenerate_method_desc = \"AI_generates_method_code: This function allows to use an AI Large Language model to generate the code of the buggy method. This helps see another implementation of that method given the context before"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/autogpt/config/config.py",
        "keyword": "context",
        "snippet": "from __future__ import annotations\n\nimport contextlib\nimport os\nimport re\nfrom pathlib import Path\nfrom typing import Any, Dict, Optional, Union\n\n--\n        config_dict[\"plugins_denylist\"] = _safe_split(os.getenv(\"DENYLISTED_PLUGINS\"))\n\n        with contextlib.suppress(TypeError):\n            config_dict[\"image_size\"] = int(os.getenv(\"IMAGE_SIZE\"))\n        with contextlib.suppress(TypeError):\n            config_dict[\"redis_port\"] = int(os.getenv(\"REDIS_PORT\"))\n        with contextlib.suppress(Ty"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/get_defects4j_list.py",
        "keyword": "chunk",
        "snippet": "def construct_batches_files(dir_to_save, bugs_list, excludes, batch_size=200, batches_number = 72):\n    filtered_list = [b for b in bugs_list if b not in excludes]\n    chunks = [filtered_list[i:i+batch_size] for i in range(0, len(filtered_list), batch_size)]\n    for i, chunk in enumerate(chunks):\n        print(chunk)\n        with open(os.path.join(dir_to_save, str(i + batch_start)), \"w\") as svfile:\n            svfile.write(\"\\n\\n\".join([\" \".join(c) for c in chunk]))\n\nconstruct_batches_files(\"expe"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/get_defects4j_bugs_list.py",
        "keyword": "chunk",
        "snippet": "def construct_batches_files(dir_to_save, bugs_list, excludes, batch_size=5, batches_number = 72):\n    filtered_list = [b for b in bugs_list if b not in excludes]\n    chunks = [filtered_list[i:i+batch_size] for i in range(0, len(filtered_list), batch_size)]\n    for i, chunk in enumerate(chunks):\n        with open(os.path.join(dir_to_save, str(i + batch_start)), \"w\") as svfile:\n            svfile.write(\"\\n\\n\".join(chunk))\n\nconstruct_batches_files(\"experimental_setups/batches\", bugs_list, set(fixed"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/JavaParser.py",
        "keyword": "token",
        "snippet": "            self.state = 273\n            self._errHandler.sync(self)\n            token = self._input.LA(1)\n            if token in [1, 18, 33, 34, 35, 38, 39, 101]:\n                self.enterOuterAlt(localctx, 1)\n                self.state = 271\n                self.classOrInterfaceModifier()\n                pass\n            elif token in [30, 42, 46, 49]:\n                self.enterOuterAlt(localctx, 2)\n                self.state = 272\n                _la = self._input.LA(1)\n                if n"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/java_antlr_test.py",
        "keyword": "token",
        "snippet": "    \n    lexer = JavaLexer(input_stream)\n    token_stream = CommonTokenStream(lexer)\n    parser = JavaParser(token_stream)\n    \n    tree = parser.compilationUnit()\n    \n    extractor = FunctionExtractor()\n    walker = ParseTreeWalker()\n"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/autogpt/config/config.py",
        "keyword": "embed",
        "snippet": "    temperature: float = 0\n    openai_functions: bool = False\n    embedding_model: str = \"text-embedding-ada-002\"\n    browse_spacy_language_model: str = \"en_core_web_sm\"\n    # Run loop configuration\n    continuous_mode: bool = False\n    continuous_limit: int = 0\n\n--\n                ),\n            ),\n            self.embedding_model: self.azure_model_to_deployment_id_map.get(\n                \"embedding_model_deployment_id\"\n            ),\n        }.get(model, None)\n\n        kwargs = {\n            "
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/autogpt/core/workspace/simple.py",
        "keyword": "embed",
        "snippet": "        for null_byte in self.NULL_BYTES:\n            if null_byte in str(relative_path) or null_byte in str(root):\n                raise ValueError(\"embedded null byte\")\n\n        if root is None:\n            return Path(relative_path).resolve()\n\n        self._logger.debug(f\"Resolving path '{relative_path}' in workspace '{root}'\")\n"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/prepare_ai_settings.py",
        "keyword": "relevant",
        "snippet": "- Locate the Bug: Execute test cases and get info to systematically identify and isolate the bug within the project \\\"{name}\\\" and bug index \\\"{bug_index}\\\".\n- Perform code Analysis: Analyze the lines of code associated with the bug to discern and comprehend the potentially faulty sections.\n- Try simple Fixes: Attempt straightforward remedies, such as altering operators, changing identifiers, modifying numerical or boolean literals, adjusting function arguments, or refining conditional statement"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/construct_commands_descriptions.py",
        "keyword": "relevant",
        "snippet": "## collect information to fix the bug\n\nsearch_code_desc = \"\"\"search_code_base: This utility function scans all Java files within a specified project for a given list of keywords. It generates a dictionary as output, organized by file names, classes, and method names. Within each method name, it provides a list of keywords that match the method's content. The resulting structure is as follows: { file_name: { class_name: { method_name: [...list of matched keywords...] } } }. This functionality pro"
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/autogpt/commands/web_selenium.py",
        "keyword": "summarize",
        "snippet": "        add_header(driver)\n        if TOKENS_TO_TRIGGER_SUMMARY < count_string_tokens(text, agent.llm.name):\n            text = summarize_memorize_webpage(url, text, question, agent, driver)\n\n        links = scrape_links_with_selenium(driver, url)\n\n        # Limit links to LINKS_TO_RETURN\n        if len(links) > LINKS_TO_RETURN:\n--\n\n\ndef summarize_memorize_webpage(\n    url: str,\n    text: str,\n    question: str,\n    agent: Agent,\n    driver: Optional[WebDriver] = None,\n--\n    Args:\n        url ("
      },
      {
        "repo": "RepairAgent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/RepairAgent/repair_agent/autogpt/processing/text.py",
        "keyword": "summarize",
        "snippet": "\n\ndef summarize_text(\n    text: str,\n    config: Config,\n    instruction: Optional[str] = None,\n    question: Optional[str] = None,\n) -> tuple[str, None | list[tuple[str, str]]]:\n--\n\n    Args:\n        text (str): The text to summarize\n        config (Config): The config object\n        instruction (str): Additional instruction for summarization, e.g. \"focus on information related to polar bears\", \"omit personal information contained in the text\"\n        question (str): Question to answer in the s"
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/swarm/core.py",
        "keyword": "context",
        "snippet": ")\n\n__CTX_VARS_NAME__ = \"context_variables\"\n\n\nclass Swarm:\n    def __init__(self, client=None):\n        if not client:\n--\n        agent: Agent,\n        history: List,\n        context_variables: dict,\n        model_override: str,\n        stream: bool,\n        debug: bool,\n    ) -> ChatCompletionMessage:\n        context_variables = defaultdict(str, context_variables)\n        instructions = (\n            agent.instructions(context_variables)\n            if callable(agent.instructions)\n"
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/swarm/types.py",
        "keyword": "context",
        "snippet": "    messages: List = []\n    agent: Optional[Agent] = None\n    context_variables: dict = {}\n\n\nclass Result(BaseModel):\n    \"\"\"\n    Encapsulates the possible return values for an agent function.\n--\n        value (str): The result value as a string.\n        agent (Agent): The agent instance, if applicable.\n        context_variables (dict): A dictionary of context variables.\n    \"\"\"\n\n    value: str = \"\"\n    agent: Optional[Agent] = None\n    context_variables: dict = {}\n"
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/swarm/core.py",
        "keyword": "chunk",
        "snippet": "\n# Local imports\nfrom .util import function_to_json, debug_print, merge_chunk\nfrom .types import (\n    Agent,\n    AgentFunction,\n    ChatCompletionMessage,\n    ChatCompletionMessageToolCall,\n--\n\n            yield {\"delim\": \"start\"}\n            for chunk in completion:\n                delta = json.loads(chunk.choices[0].delta.json())\n                if delta[\"role\"] == \"assistant\":\n                    delta[\"sender\"] = active_agent.name\n                yield delta\n                delta.pop(\"role\""
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/swarm/util.py",
        "keyword": "chunk",
        "snippet": "\n\ndef merge_chunk(final_response: dict, delta: dict) -> None:\n    delta.pop(\"role\", None)\n    merge_fields(final_response, delta)\n\n    tool_calls = delta.get(\"tool_calls\")\n    if tool_calls and len(tool_calls) > 0:\n"
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/examples/customer_service_streaming/src/utils.py",
        "keyword": "token",
        "snippet": "    messages: list[dict[str, str]],\n    model: str = \"gpt-4-0125-preview\",\n    max_tokens=2000,\n    temperature=0.7,\n    tools=None, \n    stream=False,):\n\n    # Prepare the request parameters\n--\n        \"model\": model,\n        \"messages\": messages,\n        \"max_tokens\": max_tokens,\n        \"temperature\": temperature,\n        \"stream\": stream,\n    }\n\n    if tools and isinstance(tools, list):\n"
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/examples/customer_service_streaming/src/evals/eval_function.py",
        "keyword": "token",
        "snippet": "    completion_result = self.client.chat.completions.create(\n       model=\"gpt-4-turbo-preview\",\n       max_tokens=100,\n       temperature=0,\n       messages=[\n        {\"role\": \"system\",\n         \"content\": extract_name_prompt\n         },\n"
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/examples/support_bot/prep_data.py",
        "keyword": "embed",
        "snippet": "client = OpenAI()\nGPT_MODEL = \"gpt-4o\"\nEMBEDDING_MODEL = \"text-embedding-3-large\"\n\narticle_list = os.listdir(\"data\")\n\narticles = []\n\n--\nfor i, x in enumerate(articles):\n    try:\n        embedding = client.embeddings.create(model=EMBEDDING_MODEL, input=x[\"text\"])\n        articles[i].update({\"embedding\": embedding.data[0].embedding})\n    except Exception as e:\n        print(x[\"title\"])\n        print(e)\n\nqdrant = qdrant_client.QdrantClient(host=\"localhost\")\n--\ncollection_name = \"help_center\"\n"
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/examples/support_bot/customer_service.py",
        "keyword": "embed",
        "snippet": "qdrant = qdrant_client.QdrantClient(host=\"localhost\")\n\n# Set embedding model\nEMBEDDING_MODEL = \"text-embedding-3-large\"\n\n# Set qdrant collection\ncollection_name = \"help_center\"\n\n\n--\n\ndef query_qdrant(query, collection_name, vector_name=\"article\", top_k=5):\n    # Creates embedding vector from user query\n    embedded_query = (\n        client.embeddings.create(\n            input=query,\n            model=EMBEDDING_MODEL,\n        )\n        .data[0]\n        .embedding\n"
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/examples/airline/data/routines/prompts.py",
        "keyword": "relevant",
        "snippet": "\nNote: If the user demands to talk to a supervisor, or a human agent, call the escalate_to_agent function.\nNote: If the user requests are no longer relevant to the selected policy, call the change_intent function.\n\nYou have the chat history, customer and order context available to you.\nHere is the policy:\n\"\"\"\n\n"
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/examples/airline/data/routines/flight_modification/policies.py",
        "keyword": "relevant",
        "snippet": "\nNote: If the user demands to talk to a supervisor, or a human agent, call the escalate_to_agent function.\nNote: If the user requests are no longer relevant to the selected policy, call the transfer function to the triage agent.\n\nYou have the chat history, customer and order context available to you.\nHere is the policy:\n\"\"\"\n\n"
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/examples/customer_service_streaming/configs/prompts.py",
        "keyword": "summarize",
        "snippet": "      \"type\": \"function\",\n      \"function\": {{\n        \"name\": \"summarize\",\n        \"description\": \"Summarize input text\",\n        \"parameters\": {{\n          \"type\": \"object\",\n          \"properties\": {{\n            \"input\": {{\n              \"type\": \"string\",\n              \"description\": \"The text to summarize\"\n            }}\n          }},\n          \"required\": [\"input\"]\n        }}\n      }}\n"
      },
      {
        "repo": "swarm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/swarm/examples/customer_service_streaming/src/swarm/conversation.py",
        "keyword": "summarize",
        "snippet": "        self.history.append(output)\n\n    def summarize(self):\n        # Implement summarization logic here\n        self.summary = \"Summary of the conversation\"\n\n    def get_summary(self):\n        if not self.summary:\n            self.summarize()\n        return self.summary\n\n    def clear_current_messages(self):\n        self.current_messages = []\n\n"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/scripts/generate_api.py",
        "keyword": "context",
        "snippet": ")\nfrom playwright._impl._browser import Browser\nfrom playwright._impl._browser_context import BrowserContext\nfrom playwright._impl._browser_type import BrowserType\nfrom playwright._impl._cdp_session import CDPSession\nfrom playwright._impl._clock import Clock\nfrom playwright._impl._console_message import ConsoleMessage\nfrom playwright._impl._dialog import Dialog\n--\nfrom playwright._impl._api_structures import Cookie, SetCookieParam, FloatRect, FilePayload, Geolocation, HttpCredentials, PdfMargins"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/scripts/documentation_provider.py",
        "keyword": "context",
        "snippet": "            return \"pathlib.Path\"\n        match = re.match(\n            r\"playwright._impl._event_context_manager.EventContextManagerImpl\\[playwright._impl.[^.]+.(.*)\\]\",\n            str_value,\n        )\n        if match:\n            return \"EventContextManager[\" + match.group(1) + \"]\"\n        match = re.match(r\"^<class 'playwright\\._impl\\.[\\w_]+\\.([^']+)'>$\", str_value)\n"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/playwright/async_api/_generated.py",
        "keyword": "chunk",
        "snippet": "        )\n\n    async def start_chunk(\n        self, *, title: typing.Optional[str] = None, name: typing.Optional[str] = None\n    ) -> None:\n        \"\"\"Tracing.start_chunk\n\n        Start a new trace chunk. If you'd like to record multiple traces on the same `BrowserContext`, use\n        `tracing.start()` once, and then create multiple trace chunks with `tracing.start_chunk()` and\n        `tracing.stop_chunk()`.\n\n        **Usage**\n\n        ```py\n        await context.tracing.start(screenshots=True"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/playwright/sync_api/_generated.py",
        "keyword": "chunk",
        "snippet": "        )\n\n    def start_chunk(\n        self, *, title: typing.Optional[str] = None, name: typing.Optional[str] = None\n    ) -> None:\n        \"\"\"Tracing.start_chunk\n\n        Start a new trace chunk. If you'd like to record multiple traces on the same `BrowserContext`, use\n        `tracing.start()` once, and then create multiple trace chunks with `tracing.start_chunk()` and\n        `tracing.stop_chunk()`.\n\n        **Usage**\n\n        ```py\n        context.tracing.start(screenshots=True, snapshots="
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/scripts/generate_api.py",
        "keyword": "token",
        "snippet": "def signature(func: FunctionType, indent: int) -> str:\n    hints = get_type_hints(func, globals())\n    tokens = [\"self\"]\n    split = \",\\n\" + \" \" * indent\n\n    saw_optional = False\n    for [name, value] in hints.items():\n        if name == \"return\":\n--\n        ):\n            saw_optional = True\n            tokens.append(\"*\")\n        tokens.append(f\"{to_snake_case(name)}: {processed}\")\n    return split.join(tokens)\n\n\ndef arguments(func: FunctionType, indent: int) -> str:\n    hints = get_type_hints"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/playwright/async_api/_generated.py",
        "keyword": "token",
        "snippet": "        indexed_db : Union[bool, None]\n            Set to `true` to include [IndexedDB](https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API) in the storage\n            state snapshot. If your application uses IndexedDB to store authentication tokens, like Firebase Authentication,\n            enable this.\n\n        Returns\n        -------\n        {cookies: List[{name: str, value: str, domain: str, path: str, expires: float, httpOnly: bool, secure: bool, sameSite: Union[\"Lax\", \"None\", \"S"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/tests/sync/test_har.py",
        "keyword": "embed",
        "snippet": "    page1 = context1.new_page()\n    page1.route_from_har(\n        har_path, update=True, update_content=\"embed\", update_mode=\"full\"\n    )\n    page1.goto(server.PREFIX + \"/one-style.html\")\n    context1.close()\n\n    context2 = browser.new_context()\n"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/playwright/async_api/_generated.py",
        "keyword": "embed",
        "snippet": "\n        ```py\n        frame_locator = page.locator(\\\"iframe[name=\\\\\\\"embedded\\\\\\\"]\\\").content_frame\n        # ...\n        locator = frame_locator.owner\n        await expect(locator).to_be_visible()\n        ```\n\n--\n        not_found: typing.Optional[Literal[\"abort\", \"fallback\"]] = None,\n        update: typing.Optional[bool] = None,\n        update_content: typing.Optional[Literal[\"attach\", \"embed\"]] = None,\n        update_mode: typing.Optional[Literal[\"full\", \"minimal\"]] = None,\n    ) -> None:\n  "
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/playwright/async_api/_generated.py",
        "keyword": "relevant",
        "snippet": "        \"\"\"\n        Emitted when the page opens a new tab or window. This event is emitted in addition to the\n        `browser_context.on('page')`, but only for popups relevant to this page.\n\n        The earliest moment that page is available is when it has navigated to the initial url. For example, when opening a\n        popup with `window.open('http://example.com')`, this event will fire when the network request to\n        \\\"http://example.com\\\" is done and its response has started loading in "
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/tests/async/test_assertions.py",
        "keyword": "relevant",
        "snippet": "        <input role=\"textbox\" aria-invalid=\"true\" aria-errormessage=\"error-message\" />\n        <div id=\"error-message\">Hello</div>\n        <div id=\"irrelevant-error\">This should not be considered.</div>\n      </form>\n    \"\"\"\n    )\n\n    locator = page.locator('input[role=\"textbox\"]')\n--\n        <div id=\"error1\">First error message.</div>\n        <div id=\"error2\">Second error message.</div>\n        <div id=\"irrelevant-error\">This should not be considered.</div>\n      </form>\n    \"\"\"\n    )\n\n    loc"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/tests/server.py",
        "keyword": "compress",
        "snippet": "            if request_path in server.gzip_routes:\n                self.setHeader(\"Content-Encoding\", \"gzip\")\n                self.write(gzip.compress(file_content))\n            else:\n                self.setHeader(b\"Content-Length\", str(len(file_content)))\n                self.write(file_content)\n            self.setResponseCode(HTTPStatus.OK)\n        except (FileNotFoundError, IsADirectoryError, PermissionError):\n"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/tests/async/test_network.py",
        "keyword": "compress",
        "snippet": "\n\nasync def test_response_text_should_return_uncompressed_text(\n    page: Page, server: Server\n) -> None:\n    server.enable_gzip(\"/simple.json\")\n    response = await page.goto(server.PREFIX + \"/simple.json\")\n    assert response\n--\n\n\nasync def test_response_body_should_work_with_compression(\n    page: Page, server: Server, assetdir: Path\n) -> None:\n    server.enable_gzip(\"/pptr.png\")\n    response = await page.goto(server.PREFIX + \"/pptr.png\")\n    assert response\n"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/playwright/async_api/_generated.py",
        "keyword": "window",
        "snippet": "\n        ```py\n        handle = await page.evaluate_handle(\\\"({ window, document })\\\")\n        properties = await handle.get_properties()\n        window_handle = properties.get(\\\"window\\\")\n        document_handle = properties.get(\\\"document\\\")\n        await handle.dispose()\n        ```\n\n        Returns\n--\n\n        This method returns the bounding box of the element, or `null` if the element is not visible. The bounding box is\n        calculated relative to the main frame viewport - which is usua"
      },
      {
        "repo": "playwright-python",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/playwright-python/tests/async/test_websocket.py",
        "keyword": "window",
        "snippet": "        await page.evaluate(\n            \"\"\"port => {\n            window.ws = new WebSocket('ws://localhost:' + port + '/ws');\n        }\"\"\",\n            server.PORT,\n        )\n    ws = await ws_info.value\n    await ws.wait_for_event(\"framereceived\")\n    with pytest.raises(Error) as exc_info:\n        async with ws.expect_event(\"framesent\"):\n            await page.evaluate(\"window.ws.close()\")\n    assert exc_info.value.message == \"Socket closed\"\n\n\nasync def test_should_emit_error_event(\n    page: "
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/browser_use/code_use/namespace.py",
        "keyword": "context",
        "snippet": "\t\"\"\"\n\tRemove JavaScript comments before CDP evaluation.\n\tCDP's Runtime.evaluate doesn't handle comments in all contexts.\n\n\tArgs:\n\t\tjs_code: JavaScript code potentially containing comments\n\n\tReturns:\n--\n\t\t''')\n\t\"\"\"\n\t# Strip JavaScript comments before CDP evaluation (CDP doesn't support them in all contexts)\n\tcode = _strip_js_comments(code)\n\n\tcdp_session = await browser_session.get_or_create_cdp_session()\n\n\ttry:\n--\n\t\t\t\t\terror_details.append(str(exc_obj['value']))\n\n"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/browser_use/code_use/service.py",
        "keyword": "context",
        "snippet": "\t\tcode = code_blocks.get('python', response.completion)\n\n\t\t# Add to LLM messages (truncate for history to save context)\n\t\ttruncated_completion = truncate_message_content(response.completion)\n\t\tself._llm_messages.append(AssistantMessage(content=truncated_completion))\n\n\t\treturn code, full_response\n\n--\n\t\t\tstate = await self.browser_session.get_browser_state_summary(include_screenshot=include_screenshot)\n\n\t\t\t# Format browser state with namespace context\n\t\t\tbrowser_state_text = await format_browser_s"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/browser_use/code_use/utils.py",
        "keyword": "token",
        "snippet": "\n\ndef detect_token_limit_issue(\n\tcompletion: str,\n\tcompletion_tokens: int | None,\n\tmax_tokens: int | None,\n\tstop_reason: str | None,\n) -> tuple[bool, str | None]:\n\t\"\"\"\n\tDetect if the LLM response hit token limits or is repetitive garbage.\n\n\tReturns: (is_problematic, error_message)\n\t\"\"\"\n\t# Check 1: Stop reason indicates max_tokens\n\tif stop_reason == 'max_tokens':\n\t\treturn True, f'Response terminated due to max_tokens limit (stop_reason: {stop_reason})'\n\n\t# Check 2: Used 90%+ of max_tokens (if we "
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/browser_use/code_use/service.py",
        "keyword": "token",
        "snippet": "from browser_use.telemetry.service import ProductTelemetry\nfrom browser_use.telemetry.views import AgentTelemetryEvent\nfrom browser_use.tokens.service import TokenCost\nfrom browser_use.tokens.views import UsageSummary\nfrom browser_use.tools.service import CodeAgentTools, Tools\nfrom browser_use.utils import get_browser_use_version\n\nfrom .formatting import format_browser_state_for_llm\nfrom .namespace import EvaluateError, create_namespace\nfrom .utils import detect_token_limit_issue, extract_code_b"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/browser_use/browser/profile.py",
        "keyword": "embed",
        "snippet": "class RecordHarContent(str, Enum):\n\tOMIT = 'omit'\n\tEMBED = 'embed'\n\tATTACH = 'attach'\n\n\nclass RecordHarMode(str, Enum):\n\tFULL = 'full'\n"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/browser_use/agent/gif.py",
        "keyword": "embed",
        "snippet": "\ndef decode_unicode_escapes_to_utf8(text: str) -> str:\n\t\"\"\"Handle decoding any unicode escape sequences embedded in a string (needed to render non-ASCII languages like chinese or arabic in the GIF overlay text)\"\"\"\n\n\tif r'\\u' not in text:\n\t\t# doesn't have any escape sequences that need to be decoded\n\t\treturn text\n\n"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/examples/features/sensitive_data.py",
        "keyword": "relevant",
        "snippet": "}\n# Update task to use one of the credentials above\ntask = 'Go to https://httpbin.org/forms/post and put the secure information in the relevant fields.'\n\nagent = Agent(task=task, llm=llm, sensitive_data=sensitive_data)\n\n\nasync def main():\n"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/examples/ui/gradio_demo.py",
        "keyword": "relevant",
        "snippet": "\n\tfor i, section in enumerate(sections[1:], 1):  # Skip first empty section\n\t\t# Extract relevant information\n\t\tcontent = ''\n\t\tif 'extracted_content=' in section:\n\t\t\tcontent = section.split('extracted_content=')[1].split(',')[0].strip(\"'\")\n\n\t\tif content:\n"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/examples/demo_mode_example.py",
        "keyword": "summarize",
        "snippet": "async def main() -> None:\n\tagent = Agent(\n\t\ttask='Please find the latest commit on browser-use/browser-use repo and tell me the commit message. Please summarize what it is about.',\n\t\tllm=ChatBrowserUse(),\n\t\tdemo_mode=True,\n\t)\n\tawait agent.run(max_steps=5)\n\n"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/examples/cloud/02_fast_mode_gemini.py",
        "keyword": "summarize",
        "snippet": "\ttasks = [\n\t\t'Go to ProductHunt and roast the top product like a sarcastic tech reviewer',\n\t\t'Visit Reddit r/ProgrammerHumor and summarize the top post as a dramatic news story',\n\t\t\"Check GitHub trending and write a conspiracy theory about why everyone's switching to Rust\",\n\t]\n\n\tresults = []\n\n"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/examples/custom-functions/parallel_agents.py",
        "keyword": "window",
        "snippet": "\t\t\t'--disable-default-apps',\n\t\t\t'--disable-background-timer-throttling',\n\t\t\t'--disable-backgrounding-occluded-windows',\n\t\t\t'--disable-renderer-backgrounding',\n\t\t]\n\n\t\t# Create a new browser session for each agent with the custom profile\n\t\tbrowser_session = BrowserSession(browser_profile=profile)\n"
      },
      {
        "repo": "browser-use",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/browser-use/examples/custom-functions/action_filters.py",
        "keyword": "window",
        "snippet": "\t\tcdp_session = await browser_session.get_or_create_cdp_session()\n\t\tresult = await cdp_session.cdp_client.send.Runtime.evaluate(\n\t\t\tparams={'expression': 'window.location.href', 'returnByValue': True}, session_id=cdp_session.session_id\n\t\t)\n\t\turl = result.get('result', {}).get('value', '')\n\t\treturn 'login' in url.lower() or 'signin' in url.lower()\n\texcept Exception:\n\t\treturn False\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/crews/utils.py",
        "keyword": "context",
        "snippet": "        crew._interpolate_inputs(inputs)\n    crew._set_tasks_callbacks()\n    crew._set_allow_crewai_trigger_context_for_first_task()\n\n    setup_agents(\n        crew,\n        crew.agents,\n        crew.embedder,\n--\n\n    def __init__(self, use_async: bool = False) -> None:\n        \"\"\"Initialize streaming context.\n\n        Args:\n            use_async: Whether to use async streaming mode.\n        \"\"\"\n        self.result_holder: list[CrewOutput] = []\n--\n\n    def __init__(self) -> None:\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/cli/crew_chat.py",
        "keyword": "context",
        "snippet": ") -> str:\n    \"\"\"\n    Generates an input description using AI based on the context of the crew.\n\n    Args:\n        input_name (str): The name of the input placeholder.\n        crew (Crew): The crew object.\n        chat_llm: The chat language model to use for AI calls.\n--\n        str: A concise description of the input.\n    \"\"\"\n    # Gather context from tasks and agents where the input is used\n    context_texts = []\n    placeholder_pattern = re.compile(r\"\\{(.+?)}\")\n\n    for task in crew.tasks:\n  "
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/crews/utils.py",
        "keyword": "chunk",
        "snippet": "\n    Returns:\n        If streaming, a single CrewStreamingOutput that yields chunks from all crews.\n        Otherwise, a list of CrewOutput results.\n    \"\"\"\n    from crewai.types.usage_metrics import UsageMetrics\n    from crewai.utilities.streaming import (\n        create_async_chunk_generator,\n        signal_end,\n        signal_error,\n    )\n\n    crew_copies = [crew.copy() for _ in inputs]\n--\n                    stream_output: CrewStreamingOutput,\n                ) -> CrewOutput:\n               "
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/cli/provider.py",
        "keyword": "chunk",
        "snippet": "    total_size = int(response.headers.get(\"content-length\", 0))\n    block_size = 8192\n    data_chunks = []\n    with click.progressbar(\n        length=total_size, label=\"Downloading\", show_pos=True\n    ) as progress_bar:\n        for chunk in response.iter_content(block_size):\n            if chunk:\n                data_chunks.append(chunk)\n                progress_bar.update(len(chunk))\n    data_content = b\"\".join(data_chunks)\n    return json.loads(data_content.decode(\"utf-8\"))\n\n\ndef get_provider_"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/conftest.py",
        "keyword": "token",
        "snippet": "    \"openai-project\": \"OPENAI-PROJECT-XXX\",\n    \"x-ratelimit-limit-requests\": \"X-RATELIMIT-LIMIT-REQUESTS-XXX\",\n    \"x-ratelimit-limit-tokens\": \"X-RATELIMIT-LIMIT-TOKENS-XXX\",\n    \"x-ratelimit-remaining-requests\": \"X-RATELIMIT-REMAINING-REQUESTS-XXX\",\n    \"x-ratelimit-remaining-tokens\": \"X-RATELIMIT-REMAINING-TOKENS-XXX\",\n    \"x-ratelimit-reset-requests\": \"X-RATELIMIT-RESET-REQUESTS-XXX\",\n    \"x-ratelimit-reset-tokens\": \"X-RATELIMIT-RESET-TOKENS-XXX\",\n    \"x-goog-api-key\": \"X-GOOG-API-KEY-XXX\",\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/crews/crew_output.py",
        "keyword": "token",
        "snippet": "        description=\"Output of each task\", default=[]\n    )\n    token_usage: UsageMetrics = Field(\n        description=\"Processed token summary\", default_factory=UsageMetrics\n    )\n\n    @property\n    def json(self) -> str | None:  # type: ignore[override]\n        if self.tasks_output[-1].output_format != OutputFormat.JSON:\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/crews/utils.py",
        "keyword": "embed",
        "snippet": "from crewai.agents.agent_builder.base_agent import BaseAgent\nfrom crewai.crews.crew_output import CrewOutput\nfrom crewai.rag.embeddings.types import EmbedderConfig\nfrom crewai.types.streaming import CrewStreamingOutput, FlowStreamingOutput\nfrom crewai.utilities.streaming import (\n    StreamingState,\n    TaskInfo,\n    create_streaming_state,\n--\n    crew: Crew,\n    agents: Iterable[BaseAgent],\n    embedder: EmbedderConfig | None,\n    function_calling_llm: Any,\n    step_callback: Callable[..., Any]"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/tests/cli/test_cli.py",
        "keyword": "embed",
        "snippet": "\n\n@mock.patch(\"crewai.cli.add_crew_to_flow.create_embedded_crew\")\n@mock.patch(\"pathlib.Path.exists\", return_value=True)  # Mock the existence check\ndef test_flow_add_crew(mock_path_exists, mock_create_embedded_crew, runner):\n    crew_name = \"new_crew\"\n    result = runner.invoke(flow_add_crew, [crew_name])\n\n    # Log the output for debugging\n    print(result.output)\n--\n    assert f\"Adding crew {crew_name} to the flow\" in result.output\n\n    # Verify that create_embedded_crew was called with the co"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/experimental/evaluation/metrics/goal_metrics.py",
        "keyword": "relevant",
        "snippet": "1. Did the agent correctly interpret the task goal?\n2. Did the final output directly address the requirements?\n3. Did the agent focus on relevant aspects of the task?\n4. Did the agent provide all requested information or deliverables?\n\nReturn your evaluation as JSON with fields 'score' (number) and 'feedback' (string).\n\"\"\",\n            },\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/knowledge/knowledge.py",
        "keyword": "relevant",
        "snippet": "class Knowledge(BaseModel):\n    \"\"\"\n    Knowledge is a collection of sources and setup for the vector store to save and query relevant context.\n    Args:\n        sources: list[BaseKnowledgeSource] = Field(default_factory=list)\n        storage: KnowledgeStorage | None = Field(default=None)\n        embedder: EmbedderConfig | None = None\n    \"\"\"\n--\n    ) -> list[SearchResult]:\n        \"\"\"\n        Query across all knowledge sources to find the most relevant information.\n        Returns the top_k mos"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai-tools/src/crewai_tools/rag/core.py",
        "keyword": "summarize",
        "snippet": "    embedding_provider: str = \"openai\"\n    embedding_model: str = \"text-embedding-3-large\"\n    summarize: bool = False\n    top_k: int = 5\n    embedding_config: dict[str, Any] = Field(default_factory=dict)\n\n    _client: Any = PrivateAttr()\n    _collection: Any = PrivateAttr()\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai-tools/src/crewai_tools/aws/bedrock/browser/browser_toolkit.py",
        "keyword": "summarize",
        "snippet": "        research_agent = Agent(\n            role=\"Web Researcher\",\n            goal=\"Research and summarize web content\",\n            backstory=\"You're an expert at finding information online.\",\n            tools=browser_tools,\n        )\n\n        # Create a task for the agent\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai-tools/src/crewai_tools/__init__.py",
        "keyword": "compress",
        "snippet": "from crewai_tools.tools.file_read_tool.file_read_tool import FileReadTool\nfrom crewai_tools.tools.file_writer_tool.file_writer_tool import FileWriterTool\nfrom crewai_tools.tools.files_compressor_tool.files_compressor_tool import (\n    FileCompressorTool,\n)\nfrom crewai_tools.tools.firecrawl_crawl_website_tool.firecrawl_crawl_website_tool import (\n    FirecrawlCrawlWebsiteTool,\n)\n"
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai-tools/tests/tools/files_compressor_tool_test.py",
        "keyword": "compress",
        "snippet": "from unittest.mock import patch\n\nfrom crewai_tools.tools.files_compressor_tool import FileCompressorTool\nimport pytest\n\n\n@pytest.fixture\ndef tool():\n--\n@patch(\"os.path.exists\", return_value=True)\n@patch(\"os.getcwd\", return_value=\"/mocked/cwd\")\n@patch.object(FileCompressorTool, \"_compress_zip\")  # Mock actual compression\n@patch.object(FileCompressorTool, \"_prepare_output\", return_value=True)\ndef test_generate_output_path_default(\n    mock_prepare, mock_compress, mock_cwd, mock_exists, tool\n):\n   "
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/llm.py",
        "keyword": "window",
        "snippet": ")\nfrom crewai.utilities import InternalInstructor\nfrom crewai.utilities.exceptions.context_window_exceeding_exception import (\n    LLMContextLengthExceededError,\n)\nfrom crewai.utilities.logger_utils import suppress_warnings\n\n\n--\n        self.api_key = api_key\n        self.callbacks = callbacks\n        self.context_window_size = 0\n        self.reasoning_effort = reasoning_effort\n        self.additional_params = {\n            k: v for k, v in kwargs.items() if k not in (\"is_litellm\", \"provider\")\n "
      },
      {
        "repo": "crewAI",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/crewAI/lib/crewai/src/crewai/cli/constants.py",
        "keyword": "window",
        "snippet": "DEFAULT_LLM_MODEL = \"gpt-4.1-mini\"\n\nJSON_URL = \"https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json\"\n\nLITELLM_PARAMS = [\"api_key\", \"api_base\", \"api_version\"]\n"
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-qa/gradio_demo.py",
        "keyword": "context",
        "snippet": "import os\nfrom typing import List, Optional, Tuple\nfrom lavague.contexts.openai.base import OpenaiContext\nfrom lavague.core.agents import WebAgent\nimport gradio as gr\nfrom lavague.core.memory import ShortTermMemory\nfrom lavague.core.retrievers import SemanticRetriever\nfrom lavague.core.world_model import WorldModel\n--\n        driver (`BaseDriver`):\n            The driver\n        context (`ActionContext`):\n            An action context\n    \"\"\"\n\n    html = \"\"\"\n    <div class=\"list-container\">\n    "
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-qa/lavague/qa/cli.py",
        "keyword": "context",
        "snippet": "import os\nfrom lavague.qa.generator import TestGenerator\nfrom lavague.tests.cli import _load_context\n\ndefault_feature = None\ndefault_url = None\n\ncwd = os.getcwd()\n--\n)\n@click.option(\n    \"--context\",\n    \"-c\",\n    type=str,\n    default=None,\n    required=False,\n    help=\"Path of python file containing an initialized context and token_counter. Defaults to OpenAI GPT4o\",\n)\n@click.option(\n    \"--headless\",\n"
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-qa/gradio_demo.py",
        "keyword": "chunk",
        "snippet": "            logs = self.agent.logger.return_pandas()\n            html = self.agent.driver.get_html()\n            html_chunks = self.retriever.retrieve(self.scenario.expect[0], [html])\n            assert_code = TestGenerator._generate_assert_code(\n                self.scenario.expect[0], html_chunks, self.llm\n            )\n            code = TestGenerator._build_pytest_file(\n                logs, assert_code, self.scenario, original_url, \"gradio.gherkin\"\n            )\n            self.code = \"```"
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-server/lavague/server/channel.py",
        "keyword": "chunk",
        "snippet": "                    InteractiveXPathRetriever(self.agent.driver),\n                    FromXPathNodesExpansionRetriever(\n                        chunk_size=args_obj.get(\"contextExpansionSize\", 750)\n                    ),\n                    SemanticRetriever(\n                        embedding=None, top_k=args_obj.get(\"semanticTopK\", 5)\n                    ),\n                )\n"
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-tests/contexts/default_context.py",
        "keyword": "token",
        "snippet": "from lavague.core.token_counter import TokenCounter\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.multi_modal_llms.openai import OpenAIMultiModal\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom lavague.core.context import Context\n\n--\nembedding_name = \"text-embedding-3-large\"\n\n# declare the token counter before any LLMs are initialized\ntoken_counter = TokenCounter()\n\n# init models\nllm = OpenAI(model=llm_name)\nmm_llm = OpenAIMultiModal(model=llm_name)\nembedding = OpenA"
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-tests/contexts/anthropic_context.py",
        "keyword": "token",
        "snippet": "from lavague.core.token_counter import TokenCounter\nfrom lavague.contexts.anthropic import AnthropicContext\n\n# declare the token counter before any LLMs are initialized\ntoken_counter = TokenCounter()\n\n# init context\ncontext = AnthropicContext()\n"
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-tests/contexts/default_context.py",
        "keyword": "embed",
        "snippet": "from llama_index.llms.openai import OpenAI\nfrom llama_index.multi_modal_llms.openai import OpenAIMultiModal\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom lavague.core.context import Context\n\nllm_name = \"gpt-4o\"\nmm_llm_name = \"gpt-4o\"\nembedding_name = \"text-embedding-3-large\"\n\n# declare the token counter before any LLMs are initialized\ntoken_counter = TokenCounter()\n\n# init models\nllm = OpenAI(model=llm_name)\nmm_llm = OpenAIMultiModal(model=llm_name)\nembedding = OpenAIEmbedding(m"
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-tests/contexts/custom_context.py",
        "keyword": "embed",
        "snippet": "from llama_index.llms.gemini import Gemini\nfrom llama_index.multi_modal_llms.openai import OpenAIMultiModal\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom lavague.core.context import Context\n\nllm_name = \"gemini-1.5-flash-001\"\nmm_llm_name = \"gpt-4o-mini\"\nembedding_name = \"text-embedding-3-small\"\n\ntoken_counter = TokenCounter()\n\n# init models\nllm = Gemini(\n--\n)  # gemini models are prefixed with \"models/\" in LlamaIndex\nmm_llm = OpenAIMultiModal(model=mm_llm_name)\nembedding = OpenAI"
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-server/lavague/server/driver.py",
        "keyword": "relevant",
        "snippet": "Here are examples of previous answers:\nHTML:\n<div class=\"QS5gu ud1jmf\" role=\"none\" xpath=\"/html/body/div[2]/div[2]/div[3]/span/div/div/div/div[1]/div/div/button/div\">Inloggen</div></button></div></div></div><div class=\"GZ7xNe\" xpath=\"/html/body/div[2]/div[2]/div[3]/span/div/div/div/div[2]\"><h1 class=\"I90TVb\" id=\"S3BnEe\" xpath=\"/html/body/div[2]/div[2]/div[3]/span/div/div/div/div[2]/h1\">Voordat je verdergaat naar Google</h1><div class=\"AG96lb\" xpath=\"/html/body/div[2]/div[2]/div[3]/span/div/div/d"
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-core/lavague/core/python_engine.py",
        "keyword": "relevant",
        "snippet": "        You must respond with a JSON object in the following format:\n        {{\n            \"ret\": \"[any relevant text transcribed from the image in order to answer the query {instruction} - make sure to answer with full sentences so the reponse can be understood out of context.]\",\n            \"score\": [a confidence score between 0 and 1 that the necessary context has been captured in order to answer the following query]\n        }}\n        If you believe the transcription is incomplete or lacks "
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-integrations/contexts/lavague-contexts-cache/lavague/contexts/cache/embedding_cache.py",
        "keyword": "compress",
        "snippet": "\n    def _reduce_dimension(self, value: List[float]):\n        \"\"\"Linear dimension reduction compressing last features, single-vector but less accurate than PCA or t-SNE\"\"\"\n        while len(value) > self.max_dimensions:\n            last_value = value.pop()\n            value[-1] = value[-1] + last_value\n            if value[-1] == float(\"inf\"):\n                value[-1] = sys.float_info.min + last_value\n"
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-qa/lavague/qa/utils.py",
        "keyword": "window",
        "snippet": "        return \"pass\"\n    if \"MAXIMIZE_WINDOW\" in instruction:\n        return \"browser.maximize_window()\"\n    if \"SWITCH_TAB\" in instruction:\n        tab_id = int(instruction.split(\" \")[1])\n        return f\"browser.switch_tab(tab_id={tab_id})\"\n\n\n"
      },
      {
        "repo": "LaVague",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/LaVague/lavague-server/lavague/server/driver.py",
        "keyword": "window",
        "snippet": "\n                        viewport_size[\"width\"] = self.execute_script(\n                            \"return window.innerWidth;\"\n                        )\n                        viewport_size[\"height\"] = self.execute_script(\n                            \"return window.innerHeight;\"\n                        )\n                        output = {\n                            \"bounding_box\": bounding_box,\n                            \"viewport_size\": viewport_size,\n                        }\n--\n           "
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/stores.py",
        "keyword": "context",
        "snippet": "\n    The async versions of these methods are also provided, which are\n    meant to be used in async contexts. The async methods are named with\n    an `a` prefix, e.g., `amget`, `amset`, `amdelete`, and `ayield_keys`.\n\n    By default, the `amget`, `amset`, `amdelete`, and `ayield_keys` methods\n    are implemented using the synchronous methods. If the store can natively\n    support async  operations, it should override these methods.\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/embeddings/fake.py",
        "keyword": "context",
        "snippet": "\n# Please do not add additional fake embedding model implementations here.\nimport contextlib\nimport hashlib\n\nfrom pydantic import BaseModel\nfrom typing_extensions import override\n\nfrom langchain_core.embeddings import Embeddings\n\nwith contextlib.suppress(ImportError):\n    import numpy as np\n\n\nclass FakeEmbeddings(Embeddings, BaseModel):\n    \"\"\"Fake embedding model for unit testing purposes.\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/tests/benchmarks/test_async_callbacks.py",
        "keyword": "chunk",
        "snippet": "        token: str,\n        *,\n        chunk: GenerationChunk | ChatGenerationChunk | None = None,\n        run_id: UUID,\n        parent_run_id: UUID | None = None,\n        tags: list[str] | None = None,\n        **kwargs: Any,\n    ) -> None:\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/document_loaders/base.py",
        "keyword": "chunk",
        "snippet": "        self, text_splitter: TextSplitter | None = None\n    ) -> list[Document]:\n        \"\"\"Load `Document` and split into chunks. Chunks are returned as `Document`.\n\n        !!! danger\n\n            Do not override this method. It should be considered to be deprecated!\n\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/langchain_v1/langchain/chat_models/base.py",
        "keyword": "token",
        "snippet": "\n            - `temperature`: Model temperature for controlling randomness.\n            - `max_tokens`: Maximum number of output tokens.\n            - `timeout`: Maximum time (in seconds) to wait for a response.\n            - `max_retries`: Maximum number of retry attempts for failed requests.\n            - `base_url`: Custom API endpoint URL.\n            - `rate_limiter`: A\n                [`BaseRateLimiter`][langchain_core.rate_limiters.BaseRateLimiter]\n--\n        configurable_model_with_defau"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/rate_limiters.py",
        "keyword": "token",
        "snippet": "\n    Implementations are free to add a timeout parameter to their initialize method\n    to allow users to specify a timeout for acquiring the necessary tokens when\n    using a blocking call.\n\n    Current limitations:\n\n    - Rate limiting information is not surfaced in tracing or callbacks. This means\n        that the total time it takes to invoke a chat model will encompass both\n        the time spent waiting for tokens and the time spent making the request.\n    \"\"\"\n\n    @abc.abstractmethod\n    "
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/vectorstores/utils.py",
        "keyword": "embed",
        "snippet": "\ndef maximal_marginal_relevance(\n    query_embedding: np.ndarray,\n    embedding_list: list,\n    lambda_mult: float = 0.5,\n    k: int = 4,\n) -> list[int]:\n    \"\"\"Calculate maximal marginal relevance.\n\n    Args:\n        query_embedding: The query embedding.\n        embedding_list: A list of embeddings.\n        lambda_mult: The lambda parameter for MMR.\n        k: The number of embeddings to return.\n\n    Returns:\n        A list of indices of the embeddings to return.\n\n    Raises:\n        ImportErro"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/vectorstores/in_memory.py",
        "keyword": "embed",
        "snippet": "    from collections.abc import Callable, Iterator, Sequence\n\n    from langchain_core.embeddings import Embeddings\n\ntry:\n    import numpy as np\n\n    _HAS_NUMPY = True\n--\n\n    Key init args \u2014 indexing params:\n        embedding_function: Embeddings\n            Embedding function to use.\n\n    Instantiate:\n        ```python\n        from langchain_core.vectorstores import InMemoryVectorStore\n--\n    \"\"\"\n\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/vectorstores/base.py",
        "keyword": "relevant",
        "snippet": "            if len(docs_and_similarities) == 0:\n                logger.warning(\n                    \"No relevant docs were retrieved using the \"\n                    \"relevance score threshold %s\",\n                    score_threshold,\n                )\n        return docs_and_similarities\n\n--\n            if len(docs_and_similarities) == 0:\n                logger.warning(\n                    \"No relevant docs were retrieved using the \"\n                    \"relevance score threshold %s\",\n          "
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/example_selectors/__init__.py",
        "keyword": "relevant",
        "snippet": "\n**Example selector** implements logic for selecting examples to include them in prompts.\nThis allows us to select examples that are most relevant to the input.\n\"\"\"\n\nfrom typing import TYPE_CHECKING\n\nfrom langchain_core._import_utils import import_attr\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/langchain/langchain_classic/indexes/prompts/entity_summarization.py",
        "keyword": "summarize",
        "snippet": "{history}\n\nEntity to summarize:\n{entity}\n\nExisting summary of {entity}:\n{summary}\n\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/langchain/langchain_classic/evaluation/criteria/__init__.py",
        "keyword": "summarize",
        "snippet": "        prediction=\"The answer to life is 42.\",\n        reference=\"It's commonly known that the answer to life is 42.\",\n        input=\"Please summarize the following: The answer to life, the universe, and everything is unknowable.\",\n    )\n\"\"\"  # noqa: E501\n\nfrom langchain_classic.evaluation.criteria.eval_chain import (\n    Criteria,\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/documents/__init__.py",
        "keyword": "compress",
        "snippet": "if TYPE_CHECKING:\n    from .base import Document\n    from .compressor import BaseDocumentCompressor\n    from .transformers import BaseDocumentTransformer\n\n__all__ = (\"BaseDocumentCompressor\", \"BaseDocumentTransformer\", \"Document\")\n\n_dynamic_imports = {\n    \"Document\": \"base\",\n    \"BaseDocumentCompressor\": \"compressor\",\n    \"BaseDocumentTransformer\": \"transformers\",\n}\n\n\ndef __getattr__(attr_name: str) -> object:\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/documents/compressor.py",
        "keyword": "compress",
        "snippet": "\"\"\"Document compressor.\"\"\"\n\nfrom __future__ import annotations\n\nfrom abc import ABC, abstractmethod\nfrom typing import TYPE_CHECKING\n--\n\nclass BaseDocumentCompressor(BaseModel, ABC):\n    \"\"\"Base class for document compressors.\n\n    This abstraction is primarily used for post-processing of retrieved documents.\n\n    `Document` objects matching a given query are first retrieved.\n\n--\n\n    @abstractmethod\n    def compress_documents(\n        self,\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/language_models/model_profile.py",
        "keyword": "window",
        "snippet": "        This is a beta feature. The format of model profiles is subject to change.\n\n    Provides information about chat model capabilities, such as context window sizes\n    and supported features.\n    \"\"\"\n\n    # --- Input constraints ---\n\n    max_input_tokens: int\n    \"\"\"Maximum context window (tokens)\"\"\"\n\n    image_inputs: bool\n    \"\"\"Whether image inputs are supported.\"\"\"\n    # TODO: add more detail about formats?\n\n"
      },
      {
        "repo": "langchain",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/langchain/libs/core/langchain_core/language_models/base.py",
        "keyword": "window",
        "snippet": "        \"\"\"Get the number of tokens present in the text.\n\n        Useful for checking if an input fits in a model's context window.\n\n        This should be overridden by model-specific implementations to provide accurate\n        token counts via model-specific tokenizers.\n\n        Args:\n--\n        \"\"\"Get the number of tokens in the messages.\n\n        Useful for checking if an input fits in a model's context window.\n\n        This should be overridden by model-specific implementations to provide a"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/__main__.py",
        "keyword": "context",
        "snippet": "While you have the best performing agent in /home/agent/workdir, the file /home/agent/workdir/agent_change_log.md corresponds to the one from the last agent.\n\nYou should first read the README at /home/agent/workdir/README.md, and the agent_change_log.md file because this contains some crucial project context. Make sure you've understood the code before starting.\nYou should try to invoke the meta improvement reasoning structure as early as reasonably possible, and your procedure for this meta-imp"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/oversight/overseer.py",
        "keyword": "context",
        "snippet": "\nReasoning Structures\n    - When an agent invokes a tool ending in _reasoning_structure, it will get step-by-step instructions injected into its context.\n    - Do not be surprised to see it go down apparently non-related tangents, this is often normal.\n    - You MUST NOT cancel or notify the agent for taking surprising or odd paths after invoking a reasoning_structure.\n\nNotification instructions:\n    - Do not rush the agent; carefully think about how long operations should be taking, and then be"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/oversight/overseer.py",
        "keyword": "chunk",
        "snippet": "<needs_notification>true</needs_notification>\n<agent_to_notify>agent_b8a796h5</agent_to_notify>\n<notification_content>The summarization process appears to be stuck in a deep recursion. Consider revising the approach to summarize smaller chunks or setting a limit on recursion depth. You might want to change the strategy to divide the document into smaller sections and summarize each individually.</notification_content>\n<next_check_type>time</next_check_type>\n<next_check_delay>15</next_check_delay"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/llm/api.py",
        "keyword": "chunk",
        "snippet": "        provider = ModelProvider.get_provider(model.provider)\n\n        async for chunk in provider.create_streaming_continuation_completion(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            stop=stop,\n--\n            available_tools=available_tools,\n        ):\n            chunk: CompletionChunk\n\n            # If we're handling the last chunk (which has the aggregated usage\n            # statistics), then ap"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/__main__.py",
        "keyword": "token",
        "snippet": "\n    # Result is a tuple with metrics\n    total_tokens, cached_tokens, total_cost, duration = await agent.exec(prompt, timeout, cost_threshold)\n\n    await agent.create_report()\n\n    print(f\"{total_tokens}|{cached_tokens}|{total_cost}|{duration}\")\n\n\nasync def improve_agent(workdir: Path, logdir: Path, best_iter: int, current_iter: int):\n\n    testing_procedure = \"\"\"\n--\n\n    # Result is a tuple with metrics\n    total_tokens, cached_tokens, total_cost, duration = await agent.exec(\n        self_impro"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/oversight/overseer.py",
        "keyword": "token",
        "snippet": "from ..llm.base import Message, Model\nfrom ..schemas import xml_str_to_dict\nfrom ..utils.stop_tokens import OVERSEER_STOP_TOKEN\nfrom ..utils.parsing import extract_between_patterns\nfrom ..types.event_types import EventType, Event\nfrom ..types.llm_types import ReasoningEffort, TextContent\n\nOVERSEER_RELEVANT_EVENTS = {\n--\n<next_check_type>time</next_check_type>\n<next_check_delay>60</next_check_delay>\n<notes_for_next_iteration>Main agent healthy with ~7k tokens/msg and 4-5s ttft - alert if: no prog"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/web_server/server.py",
        "keyword": "relevant",
        "snippet": "    \"\"\"Initialize event subscriptions on server startup.\"\"\"\n    event_bus = await EventBus.get_instance()\n    # Subscribe to all relevant event types\n    event_bus.subscribe(set(EventType), event_callback)\n\n\n@app.on_event(\"shutdown\")\nasync def shutdown_event():\n"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/agents/implementations/coder.py",
        "keyword": "relevant",
        "snippet": "    AGENT_DESCRIPTION = \"\"\"A specialised agent for coding tasks. You must delegate to this agent when the problem at hand involves solving any sort of coding problem.\n\nNote that this agent doesn't get to see your current context, or the initial request or problem statement. It is up to you to accurately relay this to the sub-agent or decompose it into sub-tasks if it is very long and repeating it verbatim would be slow and costly, as well as any relevant information in your dialogue history.\n\nWh"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/oversight/overseer.py",
        "keyword": "summarize",
        "snippet": "<is_looping>false</is_looping>\n<currently_running_agent>agent_b8a796h5</currently_running_agent>\n<needs_notification_reasoning>The `summarize_section` agent [agent_c6b5a493] is being called recursively and seems to be going too deep without returning. It has been running for a while without making significant progress. The `summarize_document` agent [agent_b8a796h5] is the currently running agent, and should be the recipient of the notification.</needs_notification_reasoning>\n<needs_notification"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/benchmarks/livecodebench.py",
        "keyword": "compress",
        "snippet": "                return result\n\n            # Try decompressing private test cases\n            if isinstance(test_data, str) and \"=\" in test_data:\n                try:\n                    compressed = test_data.encode(\"utf-8\")\n                    decompressed = zlib.decompress(base64.b64decode(compressed))\n                    pickle_data = pickle.loads(decompressed)\n\n                    if isinstance(pickle_data, list):\n                        return LiveCodeBenchProblem.parse_test_cases(\n       "
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/utils/archive_analysis.py",
        "keyword": "window",
        "snippet": "\n        # Calculate rolling averages\n        df[\"rolling_avg_3\"] = df[\"mean_score\"].rolling(window=3, min_periods=1).mean()\n        df[\"rolling_avg_5\"] = df[\"mean_score\"].rolling(window=5, min_periods=1).mean()\n\n        # Select relevant columns\n        trend_cols = [\n            \"iteration\",\n            \"mean_score\",\n"
      },
      {
        "repo": "self_improving_coding_agent",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/self_improving_coding_agent/base_agent/src/tools/file_tools.py",
        "keyword": "window",
        "snippet": "class OpenFile(BaseTool):\n    TOOL_NAME = \"open_files\"\n    TOOL_DESCRIPTION = \"\"\"A file viewer tool that allows you to see the contents of one or more files in your context window.\n\nNote, you should use /home/agent/workdir as your working directory if possible, althogh specifying file paths outside this directory will work.\n\nFeatures:\n- View multiple files at once\n--\n    file_paths: list[str] = Field(\n        ...,\n        description=\"A list of one or more absolute filepaths to add to open in yo"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/scripts/legacy_benchmark.py",
        "keyword": "context",
        "snippet": "# list all folders in benchmark folder\n# for each folder, run the benchmark\nimport contextlib\nimport json\nimport os\nimport subprocess\n\nfrom datetime import datetime\n--\n        print()\n\n        with contextlib.suppress(KeyboardInterrupt):\n            subprocess.run(\n                [\n                    \"python\",\n                    \"-m\",\n                    \"gpt_engineer.cli.main\",\n"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/gpt_engineer/core/project_config.py",
        "keyword": "context",
        "snippet": "[paths]\nbase = \"./frontend\"  # base directory to operate in (for monorepos)\nsrc = \"./src\"        # source directory (under the base directory) from which context will be retrieved\n\n[gptengineer-app]  # this namespace is used for gptengineer.app, may be used for internal experiments\nproject_id = \"...\"\n\n# we support multiple OpenAPI schemas, used as context for the LLM\nopenapi = [\n    { url = \"https://api.gptengineer.app/openapi.json\" },\n    { url = \"https://some-color-translating-api/openapi.json"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/gpt_engineer/core/ai.py",
        "keyword": "chunk",
        "snippet": "        \"\"\"\n        data = json.loads(jsondictstr)\n        # Modify implicit is_chunk property to ALWAYS false\n        # since Langchain's Message schema is stricter\n        prevalidated_data = [\n            {**item, \"tools\": {**item.get(\"tools\", {}), \"is_chunk\": False}}\n            for item in data\n        ]\n        return list(messages_from_dict(prevalidated_data))  # type: ignore\n\n    def _create_chat_model(self) -> BaseChatModel:\n"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/gpt_engineer/tools/supported_languages.py",
        "keyword": "chunk",
        "snippet": "\"\"\"\nThis module defines the supported programming languages for document chunking.\n\nVariables:\n    SUPPORTED_LANGUAGES (list): A list of dictionaries defining supported languages.\n\"\"\"\n\n--\n    {\"name\": \"Markdown\", \"extensions\": [\".md\"], \"tree_sitter_name\": \"md\"},\n    {\"name\": \"Arduino C\", \"extensions\": [\".ino\"], \"tree_sitter_name\": \"ino\"}\n    # ---- the following are not supported by the current code chunker implementation ----\n    # {\n    #     \"name\": \"Swift\",\n    #     \"extensions\": [\".swift\"]"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/docs/examples/open_llms/openai_api_interface.py",
        "keyword": "token",
        "snippet": "    ],\n    temperature=0.7,\n    max_tokens=200,\n)\n\nprint(response.choices[0].message.content)\n"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/gpt_engineer/core/token_usage.py",
        "keyword": "token",
        "snippet": "from typing import List, Union\n\nimport tiktoken\n\nfrom langchain.schema import AIMessage, HumanMessage, SystemMessage\nfrom PIL import Image\n\n# workaround for function moved in:\n--\ntry:\n    from langchain.callbacks.openai_info import (\n        get_openai_token_cost_for_model,  # fmt: skip\n    )\nexcept ImportError:\n    from langchain_community.callbacks.openai_info import (\n        get_openai_token_cost_for_model,  # fmt: skip\n    )\n\n\nMessage = Union[AIMessage, HumanMessage, SystemMessage]\n"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/gpt_engineer/applications/cli/file_selector.py",
        "keyword": "relevant",
        "snippet": "        \"# Remove '#' to select a file or turn off linting.\\n\\n\"\n        \"# Linting with BLACK (Python) enhances code suggestions from LLMs. \"\n        \"To disable linting, uncomment the relevant option in the linting settings.\\n\\n\"\n        \"# gpt-engineer can only read selected files. \"\n        \"Including irrelevant files will degrade performance, \"\n        \"cost additional tokens and potentially overflow token limit.\\n\\n\"\n    )\n    LINTING_STRING = '[linting]\\n# \"linting\" = \"off\"\\n\\n'\n    is_li"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/tests/core/test_chat_to_files.py",
        "keyword": "relevant",
        "snippet": "\nexample_diff = \"\"\"\nIrrelevant line to be ignored\n\nanother irrelevant line to be ignored\n```diff\n--- example.txt\n+++ example.txt\n@@ -12,3 +12,4 @@\n     sample text 1\n--\n\nexample_line_dist_diff = \"\"\"\nIrrelevant line to be ignored\n\nanother irrelevant line to be ignored\n```diff\n--- example.txt\n+++ example.txt\n@@ -10,4 +13,5 @@\n"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/gpt_engineer/core/ai.py",
        "keyword": "window",
        "snippet": "    @staticmethod\n    def multiline_input():\n        print(\"Enter/Paste your content. Ctrl-D or Ctrl-Z ( windows ) to save it.\")\n        content = []\n        while True:\n            try:\n                line = input()\n            except EOFError:\n"
      },
      {
        "repo": "gpt-engineer",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/gpt-engineer/tests/tools/example_snake_files.py",
        "keyword": "window",
        "snippet": "\n        // Set up keyboard controls\n        window.addEventListener('keydown', (e) => this.handleKeydown(e));\n    }\n\n    // Method to handle keyboard input\n    handleKeydown(event: KeyboardEvent) {\n        switch (event.key) {\n--\n#include <iostream>\n#include <conio.h>\n#include <windows.h>\nusing namespace std;\n\nbool gameOver;\nconst int width = 20;\nconst int height = 20;\n--\n#include <stdio.h>\n#include <conio.h>\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/third_party/runtime/impl/runloop/runloop_runtime.py",
        "keyword": "context",
        "snippet": "\n        # Add some additional commands based on our image\n        # NB: start off as root, action_execution_server will ultimately choose user but expects all context\n        # (ie browser) to be installed as root\n        # Convert start_command list to a single command string with additional setup\n        start_command_str = (\n            \"export MAMBA_ROOT_PREFIX=/openhands/micromamba && \"\n            \"cd /openhands/code && \"\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/evaluation/benchmarks/discoverybench/run_infer.py",
        "keyword": "context",
        "snippet": "        'In the final answer, please write down a scientific hypothesis in '\n        'natural language, derived from the provided dataset, clearly stating the '\n        'context of hypothesis (if any), variables chosen (if any) and '\n        'relationship between those variables (if any) including any statistical significance.'\n        'Also generate a summary of the full workflow starting from data loading that led to the final answer as WORKFLOW SUMMARY:'\n    )\n\n    # Run the NL query through "
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/enterprise/sync/install_gitlab_webhooks.py",
        "keyword": "chunk",
        "snippet": "        webhook_store = await GitlabWebhookStore.get_instance()\n\n        # Load chunks of rows that need processing (webhook_exists == False)\n        webhooks_to_process = await self.fetch_rows(webhook_store)\n\n        logger.info(\n            'Processing webhook chunks',\n            extra={'webhooks_to_process': webhooks_to_process},\n        )\n\n        for webhook in webhooks_to_process:\n            try:\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/tests/unit/llm/test_acompletion.py",
        "keyword": "chunk",
        "snippet": "        mock_call_acompletion.return_value.__aiter__.return_value = iter(mock_response)\n        test_llm = _get_llm(StreamingLLM)\n        async for chunk in test_llm.async_streaming_completion(\n            messages=[{'role': 'user', 'content': 'Hello!'}], stream=True\n        ):\n            print(f'Chunk: {chunk[\"choices\"][0][\"delta\"][\"content\"]}')\n            # Assertions for streaming completion\n            assert chunk['choices'][0]['delta']['content'] in [\n                r['choices'][0]['del"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/evaluation/benchmarks/EDA/game.py",
        "keyword": "token",
        "snippet": "            model=self.answerer_model,\n            messages=user_messages,\n            max_tokens=6,\n            n=1,\n            stop=None,\n            temperature=0.2,\n        )\n        if any(\n--\n            model=self.answerer_model,\n            messages=user_messages,\n            max_tokens=6,\n            n=1,\n            stop=None,\n            temperature=0.2,\n        )\n        if re.search(rf'(?:^|\\W){self.item.lower()}(?:$|\\W)', question.lower()):\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/evaluation/benchmarks/EDA/run_infer.py",
        "keyword": "token",
        "snippet": "\n    guesser_kargs = {\n        'max_new_tokens': 64,\n        'temperature': 0.8,\n        'repetition_penalty': 1.0,\n        'do_sample': True,\n    }  # no penalty\n\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/evaluation/benchmarks/gorilla/utils.py",
        "keyword": "embed",
        "snippet": "    elif api_name == 'tf':\n        api_name = 'tensorhub'\n        domains = '1. $DOMAIN is inferred from the task description and should include one of {text-sequence-alignment, text-embedding, text-language-model, text-preprocessing, text-classification, text-generation, text-question-answering, text-retrieval-question-answering, text-segmentation, text-to-mel, image-classification, image-feature-vector, image-object-detection, image-segmentation, image-generator, image-pose-detection, image-rn"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/enterprise/integrations/solvability/models/featurizer.py",
        "keyword": "embed",
        "snippet": "class EmbeddingDimension(BaseModel):\n    \"\"\"\n    Represents a single dimension (feature evaluation) within a feature embedding sample.\n\n    Each dimension corresponds to one feature being evaluated as true/false for a given issue.\n    \"\"\"\n\n    feature_id: str\n--\n\n\n# Type alias for a single embedding sample - maps feature identifiers to boolean values\nEmbeddingSample = dict[str, bool]\n\"\"\"\nA single sample from the LLM evaluation of features for an issue.\nMaps feature identifiers to their boolean e"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/evaluation/benchmarks/swe_perf/run_infer.py",
        "keyword": "relevant",
        "snippet": "> Only proceed to README-based install if the above fails.\n\n**1.2. Identify relevant modules and logic:**\n\n* Use test cases mentioned in `<issue_description>` to locate the functions and files involved.\n* Focus on potential performance bottlenecks: loops, I/O, locks, cache access, data structures, etc.\n\n**1.3. Run initial benchmark:**\n--\n**5.3.** Describe **how the test reveals** the problem\n\n**5.4.** State **best practices** relevant to the fix\n\n**5.5.** Explain **how the fix resolves** the iss"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/evaluation/benchmarks/ml_bench/run_analysis.py",
        "keyword": "relevant",
        "snippet": "\n    The error categories are:\n    E1: Hallucination Errors - The model misinterpreted the user's intention, misplaced Python code and bash script, or generated random or irrelevant code.\n    E2: Lack of Knowledge or Information - The model lacks sufficient information or domain-specific knowledge to satisfy the user's requirements.\n    E3: Knowledge Manipulation - The model failed to integrate or manipulate information properly.\n    E4: Syntax Errors - The model generated code with syntax error"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/openhands/core/config/utils.py",
        "keyword": "summarize",
        "snippet": "\n    ```\n    [condenser.my_summarizer]\n    type = 'llm'\n    llm_config = 'gpt-4o' # References [llm.gpt-4o]\n    max_size = 50\n    ...\n    ```\n\n    The user-defined group name, like \"my_summarizer\", is the argument to this function.\n    The function will load the CondenserConfig object with the settings of this group,\n    from the config file.\n\n    Note that the group must be under the \"condenser\" group, or in other words,\n    the group name must start with \"condenser.\".\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/openhands/runtime/plugins/agent_skills/file_reader/file_readers.py",
        "keyword": "summarize",
        "snippet": "    )\n\n    task = task or 'This is one frame from a video, please summarize this frame.'\n    base64_frames = _base64_video(file_path)\n    selected_frames = base64_frames[::frame_interval]\n\n    if len(selected_frames) > 30:\n        new_interval = len(base64_frames) // 30\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/evaluation/benchmarks/multi_swe_bench/scripts/eval/combine_final_completions.py",
        "keyword": "compress",
        "snippet": "        f_out.write(json.dumps(data) + '\\n')\n\nprint(f'Saved compressed output to {output_path}')\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/evaluation/benchmarks/swe_bench/scripts/eval/combine_final_completions.py",
        "keyword": "compress",
        "snippet": "        f_out.write(json.dumps(data) + '\\n')\n\nprint(f'Saved compressed output to {output_path}')\n"
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/openhands/core/config/condenser_config.py",
        "keyword": "window",
        "snippet": "\n    type: Literal['observation_masking'] = Field(default='observation_masking')\n    attention_window: int = Field(\n        default=100,\n        description='The number of most-recent events where observations will not be masked.',\n        ge=1,\n    )\n\n--\n\n    type: Literal['browser_output_masking'] = Field(default='browser_output_masking')\n    attention_window: int = Field(\n        default=1,\n        description='The number of most recent browser output observations that will not be masked.',\n "
      },
      {
        "repo": "openhands-ai",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/openhands-ai/openhands/core/config/agent_config.py",
        "keyword": "window",
        "snippet": "    \"\"\"Whether to enable stuck/loop detection. When disabled, the agent will not automatically detect and recover from loops.\"\"\"\n    condenser: CondenserConfig = Field(\n        # The default condenser is set to the conversation window condenser -- if\n        # we use NoOp and the conversation hits the LLM context length limit,\n        # the agent will generate a condensation request which will never be\n        # handled.\n        default_factory=lambda: ConversationWindowCondenserConfig()\n    )\n"
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter6_to_do_write/src/core/history/history_manager.py",
        "keyword": "context",
        "snippet": "\n    @property                                                                                                                       \n    def current_context_window(self):                                                                                               \n        \"\"\"get current context window usage percentage\"\"\"                                                                           \n        if not self.history_token_usage or self._model_max_tokens == 0:                             "
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter8_mcp_client/src/core/conversation.py",
        "keyword": "context",
        "snippet": "            await self._handle_tool_calls(response_message.tool_calls)\n            # Update token usage in history manager\n            self._print_context_window_and_total_cost()\n            await self._recursive_message_handling()\n        else:\n            self._print_context_window_and_total_cost()\n            # No tool calls, wait for user input\n            if self._is_in_task:\n                return\n            user_input = await self._ui_manager.get_user_input()\n            user_message = {"
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter6_to_do_write/test/test_streaming.py",
        "keyword": "chunk",
        "snippet": "        message_obj = None\n        \n        for chunk in stream_gen:\n            if isinstance(chunk, str):\n                print(chunk, end=\"\", flush=True)\n                full_content += chunk\n            else:\n                message_obj = chunk\n                print(f\"\\n\u2705 Received final message object\")\n                print(f\"   Content length: {len(chunk.content) if chunk.content else 0}\")\n                print(f\"   Has tool_calls: {hasattr(chunk, 'tool_calls') and chunk.tool_calls is not "
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter6_to_do_write/test/test_streaming_with_tools.py",
        "keyword": "chunk",
        "snippet": "        message_obj = None\n        \n        for chunk in stream_gen:\n            if isinstance(chunk, str):\n                print(chunk, end=\"\", flush=True)\n                full_content += chunk\n            else:\n                message_obj = chunk\n                print(f\"\\n\u2705 Received final message object\")\n                print(f\"   Content length: {len(chunk.content) if chunk.content else 0}\")\n                print(f\"   Has tool_calls: {hasattr(chunk, 'tool_calls') and chunk.tool_calls is not "
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter6_to_do_write/test/test_history_compress.py",
        "keyword": "token",
        "snippet": "    print(\"\u6d4b\u8bd5: \u4e0d\u6ee1\u8db3\u538b\u7f29\u6761\u4ef6\u65f6\u4e0d\u89e6\u53d1\u538b\u7f29\")\n    \n    manager = HistoryManager(model_max_tokens=100, compress_threshold=0.8)\n    \n    # \u6dfb\u52a0\u4e00\u4e9b\u6d88\u606f\n    manager.add_message(MockMessage(Role.SYSTEM, \"You are a helpful assistant\"))\n    manager.add_message(MockMessage(Role.USER, \"Hello\"))\n    manager.add_message(MockMessage(Role.ASSISTANT, \"Hi there!\"))\n    \n    # \u521b\u5efa\u6a21\u62df\u7684token\u4f7f\u7528\u91cf\u5bf9\u8c61\n    class MockTokenUsage:\n        def __init__(self, prompt, completion, total):\n            self.prompt_tokens = prompt\n            self.co"
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter6_to_do_write/test/test_crop_message.py",
        "keyword": "token",
        "snippet": "    HistoryManager._instance = None\n    HistoryManager._initialized = False\n    return HistoryManager(model_max_tokens=100, compress_threshold=0.8)\n\n\ndef test_crop_insufficient_messages():\n    \"\"\"Test cropping when there are insufficient messages\"\"\"\n    print(\"\u6d4b\u8bd5: \u6d88\u606f\u6570\u91cf\u4e0d\u8db3\u65f6\u7684\u88c1\u526a\")\n"
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter6_to_do_write/src/tools/todo_write.py",
        "keyword": "relevant",
        "snippet": "   - Only have ONE task in_progress at any time\n   - Complete current tasks before starting new ones\n   - Remove tasks that are no longer relevant from the list entirely\n\n3. **Task Completion Requirements**:\n   - ONLY mark a task as completed when you have FULLY accomplished it\n   - If you encounter errors, blockers, or cannot finish, keep the task as in_progress\n   - When blocked, create a new task describing what needs to be resolved\n"
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter6_to_do_write/src/tools/cmd_runner.py",
        "keyword": "relevant",
        "snippet": "  - Ensure it accurately reflects the changes and their purpose\n3. You have the capability to call multiple tools in a single response. When multiple independent pieces of information are requested, batch your tool calls together for optimal performance. ALWAYS run the following commands in parallel:\n   - Add relevant untracked files to the staging area.\n   - Run git status to make sure the commit succeeded.\n4. If the commit fails due to pre-commit hook changes, retry the commit ONCE to include "
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter1_tool_call_api/xml_tool_call.py",
        "keyword": "summarize",
        "snippet": "{tool_result}\n\nPlease summarize these books for the user in a friendly tone.\n\"\"\"\n\nmessages.append({\n    \"role\": \"user\",\n    \"content\": follow_up\n"
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter6_to_do_write/src/tools/smart_context_cropper.py",
        "keyword": "summarize",
        "snippet": "assistant: Attempt 2: Investigated DB write locks; added an index to the hot table; no measurable improvement.\nassistant: Attempt 3: Correlated spikes with a batch job; disabled an experiment, but the spikes persisted.\n# Once the failures are summarized, the detailed attempt logs can be removed from the bottom to keep context focused.\n</example>\n<smart_context_cropper>\nsmart_context_cropper(\n    need_user_approve = false,\n    crop_direction = \"BOTTOM\",\n"
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter4_history_control/test/test_history_compress.py",
        "keyword": "compress",
        "snippet": "    content: str\n\ndef test_auto_compression_not_triggered():\n    \"\"\"\u6d4b\u8bd5\u5f53\u4e0d\u6ee1\u8db3\u538b\u7f29\u6761\u4ef6\u65f6\uff0c\u4e0d\u4f1a\u89e6\u53d1\u538b\u7f29\"\"\"\n    print(\"\u6d4b\u8bd5: \u4e0d\u6ee1\u8db3\u538b\u7f29\u6761\u4ef6\u65f6\u4e0d\u89e6\u53d1\u538b\u7f29\")\n    \n    manager = HistoryManager(model_max_tokens=100, compress_threshold=0.8)\n    \n    # \u6dfb\u52a0\u4e00\u4e9b\u6d88\u606f\n    manager.add_message(MockMessage(Role.SYSTEM, \"You are a helpful assistant\"))\n    manager.add_message(MockMessage(Role.USER, \"Hello\"))\n    manager.add_message(MockMessage(Role.ASSISTANT, \"Hi there!\"))\n--\n    \n    # \u6267\u884c\u81ea\u52a8\u538b\u7f29\n    manager.auto_messages_compression()\n    \n    # \u9a8c\u8bc1\u6d88\u606f\u6570"
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter6_to_do_write/test/test_history_compress.py",
        "keyword": "compress",
        "snippet": "    content: str\n\ndef test_auto_compression_not_triggered():\n    \"\"\"\u6d4b\u8bd5\u5f53\u4e0d\u6ee1\u8db3\u538b\u7f29\u6761\u4ef6\u65f6\uff0c\u4e0d\u4f1a\u89e6\u53d1\u538b\u7f29\"\"\"\n    print(\"\u6d4b\u8bd5: \u4e0d\u6ee1\u8db3\u538b\u7f29\u6761\u4ef6\u65f6\u4e0d\u89e6\u53d1\u538b\u7f29\")\n    \n    manager = HistoryManager(model_max_tokens=100, compress_threshold=0.8)\n    \n    # \u6dfb\u52a0\u4e00\u4e9b\u6d88\u606f\n    manager.add_message(MockMessage(Role.SYSTEM, \"You are a helpful assistant\"))\n    manager.add_message(MockMessage(Role.USER, \"Hello\"))\n    manager.add_message(MockMessage(Role.ASSISTANT, \"Hi there!\"))\n--\n    \n    # \u6267\u884c\u81ea\u52a8\u538b\u7f29\n    manager.auto_messages_compression()\n    \n    # \u9a8c\u8bc1\u6d88\u606f\u6570"
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter7_sub_agent/src/core/conversation.py",
        "keyword": "window",
        "snippet": "            await self._handle_tool_calls(response_message.tool_calls)\n            # Update token usage in history manager\n            self._print_context_window_and_total_cost()\n            await self._recursive_message_handling()\n        else:\n            self._print_context_window_and_total_cost()\n            # No tool calls, wait for user input\n            if self._is_in_task:\n                return\n            user_input = await self._ui_manager.get_user_input()\n            user_message = {"
      },
      {
        "repo": "build-your-claude-code-from-scratch",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/build-your-claude-code-from-scratch/chapter6_to_do_write/src/core/history/history_manager.py",
        "keyword": "window",
        "snippet": "\n    @property                                                                                                                       \n    def current_context_window(self):                                                                                               \n        \"\"\"get current context window usage percentage\"\"\"                                                                           \n        if not self.history_token_usage or self._model_max_tokens == 0:                             "
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/polyglot/docker_build.py",
        "keyword": "context",
        "snippet": "        platform (str): Platform to build the image for\n        client (docker.DockerClient): Docker client to use for building the image\n        build_dir (Path): Directory for the build context (will also contain logs, scripts, and artifacts)\n        nocache (bool): Whether to use the cache when building\n    \"\"\"\n    # Create a logger for the build process\n    logger = setup_logger(image_name, build_dir / \"build_image.log\")\n    logger.info(\n"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/prompts/self_improvement_prompt.py",
        "keyword": "context",
        "snippet": "Your response will be automatically parsed, so ensure that the string response is precisely in the correct format. Do NOT include the `<JSON>` tag in your output.\"\"\"\n\ndiagnose_prompt_stochasticity = \"\"\"Since the coding agent is stochastic, it may not produce the correct patch for the given problem statement on the first try. Take into account the agent's stochastic nature and provide a solution to handle such cases. For example, one solution could be to ask the agent to try multiple times and se"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/utils/docker_utils.py",
        "keyword": "chunk",
        "snippet": "        bits, stat = container.get_archive(str(source_path))\n        \n        # Concatenate all chunks into a single bytes object\n        archive_data = b''.join(bits)\n        \n        # Extract to temporary stream\n        stream = io.BytesIO(archive_data)\n        \n--\n    else:\n        # Handle streaming output\n        for chunk in exec_result.output:\n            if chunk:\n                safe_log(f\"Container output: {chunk.decode().strip()}\")\n    \n    # Check exit code\n    if exec_result.exit_c"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/polyglot/docker_build.py",
        "keyword": "chunk",
        "snippet": "        # Log the build process continuously\n        buildlog = \"\"\n        for chunk in response:\n            if \"stream\" in chunk:\n                # Remove ANSI escape sequences from the log\n                chunk_stream = ansi_escape.sub(\"\", chunk[\"stream\"])\n                logger.info(chunk_stream.strip())\n                buildlog += chunk_stream\n            elif \"errorDetail\" in chunk:\n                # Decode error message, raise BuildError\n                logger.error(\n                    f"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/llm.py",
        "keyword": "token",
        "snippet": "            ],\n            temperature=temperature,\n            max_tokens=MAX_OUTPUT_TOKENS,\n            n=n_responses,\n            stop=None,\n            seed=0,\n        )\n        content = [r.message.content for r in response.choices]\n--\n            ],\n            temperature=temperature,\n            max_tokens=MAX_OUTPUT_TOKENS,\n            n=n_responses,\n            stop=None,\n        )\n        content = [r.message.content for r in response.choices]\n        new_msg_history = [\n--\n        re"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/prompts/self_improvement_prompt.py",
        "keyword": "token",
        "snippet": "Your response will be automatically parsed, so ensure that the string response is precisely in the correct format. Do NOT include the `<JSON>` tag in your output.\"\"\"\n\ndiagnose_prompt_contextlength = \"\"\"While the coding agent is attempting to solve GitHub issues, it encounters an error due to the input being too long for the requested model. This error is likely due to the context length exceeding the model's maximum input size. Handle cases where the input is too long for the model. The coding a"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/prompts/diagnose_improvement_prompt.py",
        "keyword": "relevant",
        "snippet": "\n\ndiagnose_improvement_system_message = \"\"\"Here is the relevant code for the LLM Coding agent with the model patch applied.\n\n# LLM Coding Agent Code\nThe current code of LLM coding agent.\n----- LLM Coding Agent Code Start -----\n{code}\n"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/self_improve_step.py",
        "keyword": "relevant",
        "snippet": "            log_container_output(exec_result)\n\n    # Commit this version of dgm, so that irrelevant changes are not included in the patch\n    exec_result = container.exec_run(\"git add --all\", workdir='/dgm/')\n    log_container_output(exec_result)\n    exec_result = container.exec_run(\"git -c user.name='user' -c user.email='you@example.com' commit -m 'a nonsense commit message'\", workdir='/dgm/')\n    log_container_output(exec_result)\n    commit_output = exec_result.output.decode('utf-8')\n"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/prompts/self_improvement_prompt.py",
        "keyword": "summarize",
        "snippet": "\nIn <JSON>, provide a JSON response with the following fields:\n- \"log_summarization\": Analyze the above logs and summarize how the agent tried to solve the GitHub issue. Note which tools and how they are used, the agent's problem-solving approach, and any issues encountered.\n- \"potential_improvements\": Identify potential improvements to the coding agent that could enhance its coding capabilities. Focus on the agent's general coding abilities (e.g., better or new tools usable across any repositor"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/polyglot/benchmark.py",
        "keyword": "summarize",
        "snippet": "    raw_rows = []\n    for dirname in dirnames:\n        row = summarize_results(dirname, stats_languages)\n        raw_rows.append(row)\n\n    # return\n\n    seen = dict()\n--\n\n            all_results.append(results)\n            summarize_results(dirname)\n            if sleep:\n                time.sleep(sleep)\n    else:\n        run_test_threaded = lox.thread(threads)(run_test)\n        for test_path in test_dnames:\n--\n    print()\n    print()\n"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/prompts/self_improvement_prompt.py",
        "keyword": "window",
        "snippet": "Your response will be automatically parsed, so ensure that the string response is precisely in the correct format. Do NOT include the `<JSON>` tag in your output.\"\"\"\n\ndiagnose_prompt_contextlength = \"\"\"While the coding agent is attempting to solve GitHub issues, it encounters an error due to the input being too long for the requested model. This error is likely due to the context length exceeding the model's maximum input size. Handle cases where the input is too long for the model. The coding a"
      },
      {
        "repo": "dgm",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/dgm/llm_withtools.py",
        "keyword": "window",
        "snippet": "            return get_response_withtools(client, model, messages, tools, tool_choice, logging, max_retry - 1)\n\n        # Hitting the context window limit\n        if 'Input is too long for requested model' in str(e):\n            pass\n\n        raise  # Re-raise the exception after logging\n\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/scripts/clean_metadata.py",
        "keyword": "context",
        "snippet": "    script_dir = Path(__file__).parent.resolve()\n    # Adjust path relative to the script's location in the aider repo\n    litellm_path = script_dir.parent / \"../litellm/model_prices_and_context_window.json\"\n    aider_path = script_dir / \"../aider/resources/model-metadata.json\"\n\n    if not litellm_path.exists():\n        print(f\"Error: LiteLLM metadata file not found at {litellm_path}\")\n        return\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/benchmark/benchmark.py",
        "keyword": "context",
        "snippet": "    num_tests: int = typer.Option(-1, \"--num-tests\", \"-n\", help=\"Number of tests to run\"),\n    num_ctx: Optional[int] = typer.Option(\n        None, \"--num-ctx\", help=\"Override model context window size\"\n    ),\n    read_model_settings: str = typer.Option(\n        None, \"--read-model-settings\", help=\"Load aider model settings from YAML file\"\n    ),\n    reasoning_effort: Optional[str] = typer.Option(\n--\n    res.user_asks = 0\n    res.test_timeouts = 0\n    res.exhausted_context_windows = 0\n    res.nu"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/tests/scrape/test_playwright_disable.py",
        "keyword": "chunk",
        "snippet": "            return []\n\n        def format_chat_chunks(self):\n            return type(\"Chunks\", (), {\"repo\": [], \"readonly_files\": [], \"chat_files\": []})()\n\n        def event(self, *a, **k):\n            pass\n\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/aider/mdstream.py",
        "keyword": "chunk",
        "snippet": "\n        Splits the output into \"stable\" older lines and the \"last few\" lines\n        which aren't considered stable. They may shift around as new chunks\n        are appended to the markdown text.\n\n        The stable lines emit to the console above the Live window.\n        The unstable lines emit into the Live window so they can be repainted.\n\n--\n    def find_minimal_suffix(self, text, match_lines=50):\n        \"\"\"\n        Splits text into chunks on blank lines \"\\n\\n\".\n        \"\"\"\n\n\nif __name__ ="
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/scripts/issues.py",
        "keyword": "token",
        "snippet": "TOKEN = os.getenv(\"GITHUB_TOKEN\")\n\nheaders = {\"Authorization\": f\"token {TOKEN}\", \"Accept\": \"application/vnd.github.v3+json\"}\n\n\ndef get_issues(state=\"open\"):\n    issues = []\n    page = 1\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/scripts/homepage.py",
        "keyword": "token",
        "snippet": "GITHUB_STARS_TOOLTIP = \"Total number of GitHub stars the Aider project has received\"\nPYPI_DOWNLOADS_TOOLTIP = \"Total number of installations via pip from PyPI\"\nTOKENS_WEEKLY_TOOLTIP = \"Number of tokens processed weekly by Aider users\"\nOPENROUTER_TOOLTIP = \"Aider's ranking among applications on the OpenRouter platform\"\nSINGULARITY_TOOLTIP = \"Percentage of the new code in Aider's last release written by Aider itself\"\n\n# Cache settings\nCACHE_DIR = os.path.expanduser(\"~/.cache/aider-badges\")\n--\n    "
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/scripts/30k-image.py",
        "keyword": "embed",
        "snippet": "\n\ndef embed_font():\n    \"\"\"Returns base64 encoded font data for the GlassTTYVT220 font.\"\"\"\n    # Path to the font file\n    font_path = (\n        Path(__file__).parent.parent / \"aider\" / \"website\" / \"assets\" / \"Glass_TTY_VT220.ttf\"\n    )\n--\n    \"\"\"Generate a celebratory SVG for 30K GitHub stars.\"\"\"\n\n    # Font embedding\n    font_data = embed_font()\n    font_face = f\"\"\"\n    @font-face {{\n        font-family: 'GlassTTYVT220';\n        src: url(data:font/truetype;charset=utf-8;base64,{font_data}) for"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/scripts/logo_svg.py",
        "keyword": "embed",
        "snippet": "#!/usr/bin/env python3\n\"\"\"\nScript to generate an SVG logo for Aider with embedded font.\nReads the Glass_TTY_VT220.ttf font, subsets it to only include the letters needed,\nand creates an SVG with the word \"aider\" in terminal green (#14b014) on a transparent background.\n\"\"\"\n\nimport argparse\n--\n\n\ndef generate_svg_with_embedded_font(font_path, text=\"aider\", color=\"#14b014\", output_path=None):\n    \"\"\"\n    Generate an SVG with embedded TTF font data.\n\n    Args:\n        font_path (str): Path to the TTF"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/scripts/update-history.py",
        "keyword": "relevant",
        "snippet": "    diff_content = run_git_diff()\n\n    # Extract relevant portion of HISTORY.md\n    latest_ver = get_latest_version_from_history()\n    with open(\"HISTORY.md\", \"r\") as f:\n        history_content = f.read()\n\n    # Find the section for this version\n--\n    if next_version_idx == -1:\n        # No next version found, use the rest of the file\n        relevant_history = history_content[start_idx:]\n    else:\n        # Extract just up to the next version\n        relevant_history = history_content[start_id"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/tests/basic/test_main.py",
        "keyword": "relevant",
        "snippet": "            )\n            output = mock_stdout.getvalue()\n            relevant_output = \"\\n\".join(\n                line\n                for line in output.splitlines()\n                if \"AIDER_DARK_MODE\" in line or \"dark_mode\" in line\n            )  # this bit just helps failing assertions to be easier to read\n            self.assertIn(\"AIDER_DARK_MODE\", relevant_output)\n            self.assertIn(\"dark_mode\", relevant_output)\n            self.assertRegex(relevant_output, r\"AIDER_DARK_MODE:\\s+on"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/benchmark/benchmark.py",
        "keyword": "summarize",
        "snippet": "    raw_rows = []\n    for dirname in dirnames:\n        row = summarize_results(dirname, stats_languages)\n        raw_rows.append(row)\n\n    # return\n\n    seen = dict()\n--\n\n            all_results.append(results)\n            summarize_results(dirname)\n            if sleep:\n                time.sleep(sleep)\n    else:\n        run_test_threaded = lox.thread(threads)(run_test)\n        for test_path in test_dnames:\n--\n    print()\n    print()\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/tests/basic/test_coder.py",
        "keyword": "summarize",
        "snippet": "                coder.cur_messages = []\n                coder.done_messages = []\n                coder.summarizer = MagicMock()\n                coder.summarizer.too_big.return_value = False\n\n                # Mock editor_coder creation and execution\n                mock_editor = MagicMock()\n                with patch(\"aider.coders.architect_coder.Coder.create\", return_value=mock_editor):\n                    # Set partial response content\n--\n                coder.cur_messages = []\n               "
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/scripts/recording_audio.py",
        "keyword": "compress",
        "snippet": "\n\ndef compress_audio(input_file, output_file, bitrate=MP3_BITRATE):\n    \"\"\"Compress audio file using FFmpeg.\"\"\"\n    if not check_ffmpeg():\n        print(\"Warning: FFmpeg not found, skipping compression\")\n        return False\n\n    try:\n        subprocess.run(\n            [\n--\n        return True\n    except subprocess.SubprocessError as e:\n        print(f\"Error compressing audio: {e}\")\n        return False\n\n\ndef generate_audio_openai(text, output_file, voice=VOICE, bitrate=MP3_BITRATE):\n    \"\"\"Gen"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/scripts/clean_metadata.py",
        "keyword": "window",
        "snippet": "    script_dir = Path(__file__).parent.resolve()\n    # Adjust path relative to the script's location in the aider repo\n    litellm_path = script_dir.parent / \"../litellm/model_prices_and_context_window.json\"\n    aider_path = script_dir / \"../aider/resources/model-metadata.json\"\n\n    if not litellm_path.exists():\n        print(f\"Error: LiteLLM metadata file not found at {litellm_path}\")\n        return\n"
      },
      {
        "repo": "Aider Repo",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/Aider Repo/benchmark/benchmark.py",
        "keyword": "window",
        "snippet": "    num_tests: int = typer.Option(-1, \"--num-tests\", \"-n\", help=\"Number of tests to run\"),\n    num_ctx: Optional[int] = typer.Option(\n        None, \"--num-ctx\", help=\"Override model context window size\"\n    ),\n    read_model_settings: str = typer.Option(\n        None, \"--read-model-settings\", help=\"Load aider model settings from YAML file\"\n    ),\n    reasoning_effort: Optional[str] = typer.Option(\n--\n    res.user_asks = 0\n    res.test_timeouts = 0\n    res.exhausted_context_windows = 0\n    res.nu"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/docs/src/conf.py",
        "keyword": "context",
        "snippet": "}\n\nhtml_context = {\n    'display_github': True,\n    \"github_user\": \"microsoft\",\n    \"github_repo\": \"autogen\",\n    \"github_version\": \"main\",\n    \"doc_path\": \"python/docs/src/\",\n--\n\ndef setup_to_main(\n    app: Sphinx, pagename: str, templatename: str, context, doctree\n) -> None:\n    \"\"\"Add a function that jinja can access for returning an \"edit this page\" link pointing to `main`.\"\"\"\n\n    def to_main(link: str) -> str:\n        \"\"\"Transform \"edit on github\" links and make sure they always point to t"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/docs/src/_extension/gallery_directive.py",
        "keyword": "context",
        "snippet": "\n    Danger:\n        This directive can only be used in the context of a Myst documentation page as\n        the templates use Markdown flavored formatting.\n    \"\"\"\n\n    name = \"gallery-grid\"\n    has_content = True\n"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/samples/core_chainlit/app_agent.py",
        "keyword": "chunk",
        "snippet": "    ))\n\n    # Register the Closure Agent to process streaming chunks from agents by exeucting the output_result \n    # function, whihc sends the stream response to the output queue \n    await ClosureAgent.register_closure(\n        runtime, CLOSURE_AGENT_TYPE, output_result, subscriptions=lambda:[TypeSubscription(topic_type=TASK_RESULTS_TOPIC_TYPE, agent_type=CLOSURE_AGENT_TYPE)] \n    )\n\n"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/samples/core_chainlit/SimpleAssistantAgent.py",
        "keyword": "chunk",
        "snippet": "        # Call the LLM model to process the message \n        model_result = None\n        async for chunk in self._model_client.create_stream(\n            messages=[self._system_message] + self._chat_history,\n            tools=self._tools,\n            cancellation_token=cancellation_token,\n        ):\n            if isinstance(chunk, CreateResult):\n                model_result = chunk\n            elif isinstance(chunk, str):\n                yield chunk\n            else:\n                raise Runti"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/samples/core_async_human_in_the_loop/main.py",
        "keyword": "token",
        "snippet": "        )\n\n    async def run(self, args: ScheduleMeetingInput, cancellation_token: CancellationToken) -> ScheduleMeetingOutput:\n        print(f\"Meeting scheduled with {args.recipient} on {args.date} at {args.time}\")\n        return ScheduleMeetingOutput()\n\n\n@type_subscription(\"scheduling_assistant_conversation\")\n--\n                    raise ValueError(f\"Tool not found: {call.name}\")\n                arguments = json.loads(call.arguments)\n                await tool.run_json(arguments, ctx.cancellat"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/samples/agentchat_fastapi/app_agent.py",
        "keyword": "token",
        "snippet": "        # Get the agent and respond to the message.\n        agent = await get_agent()\n        response = await agent.on_messages(messages=[request], cancellation_token=CancellationToken())\n\n        # Save agent state to file.\n        state = await agent.save_state()\n        async with aiofiles.open(state_path, \"w\") as file:\n            await file.write(json.dumps(state))\n"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-ext/src/autogen_ext/tools/azure/_ai_search.py",
        "keyword": "embed",
        "snippet": "\nclass EmbeddingProvider(Protocol):\n    \"\"\"Protocol defining the interface for embedding generation.\"\"\"\n\n    async def _get_embedding(self, query: str) -> List[float]:\n        \"\"\"Generate embedding vector for the query text.\"\"\"\n        ...\n\n\nclass EmbeddingProviderMixin:\n    \"\"\"Mixin class providing embedding generation functionality.\"\"\"\n\n    search_config: AzureAISearchConfig\n\n    async def _get_embedding(self, query: str) -> List[float]:\n        \"\"\"Generate embedding vector for the query text."
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-ext/src/autogen_ext/tools/azure/_config.py",
        "keyword": "embed",
        "snippet": "\n    This class defines the configuration parameters for Azure AI Search tools, including\n    authentication, search behavior, caching, and embedding settings.\n\n    .. note::\n        This class requires the ``azure`` extra for the ``autogen-ext`` package.\n\n        .. code-block:: bash\n--\n\n           - Base functionality: ``azure-search-documents>=11.4.0``\n           - For Azure OpenAI embeddings: ``openai azure-identity``\n           - For OpenAI embeddings: ``openai``\n\n    Example Usage:\n       "
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/samples/core_async_human_in_the_loop/main.py",
        "keyword": "relevant",
        "snippet": "communication, the time taken can vary significantly. When waiting for the human to respond, it is\npossible that the system may be torn down. In such cases, it is important to save the state of the\nsystem with any relevant information that is needed to rehydrate the system. When designing such\nsystems, it can be helpful recognize the trade-offs at which point to save the system state.\nIn the given (simple) example, the system state is saved when the user input is needed. However, in\na more compl"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/samples/core_semantic_router/_semantic_router_agent.py",
        "keyword": "relevant",
        "snippet": "        return await self._classifier.classify_intent(message.content)\n\n    ## Use a lookup, search, or LLM to identify the most relevant agent for the intent\n    async def _find_agent(self, intent: str) -> str:\n        logger.debug(f\"Identified intent: {intent}\")\n        try:\n            agent = await self._registry.get_agent(intent)\n            return agent\n        except KeyError:\n            logger.debug(\"No relevant agent found for intent: \" + intent)\n            return \"termination\"\n\n    #"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-agentchat/src/autogen_agentchat/tools/_team.py",
        "keyword": "summarize",
        "snippet": "                    name=\"reviewer\", model_client=model_client, system_message=\"You are a critical reviewer.\"\n                )\n                summarizer = AssistantAgent(\n                    name=\"summarizer\",\n                    model_client=model_client,\n                    system_message=\"You combine the review and produce a revised response.\",\n                )\n                team = RoundRobinGroupChat(\n                    [writer, reviewer, summarizer], termination_condition=SourceMatchT"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-ext/src/autogen_ext/tools/mcp/_factory.py",
        "keyword": "summarize",
        "snippet": "                agent = AssistantAgent(name=\"fetcher\", model_client=model_client, tools=tools, reflect_on_tool_use=True)  # type: ignore\n\n                # Let the agent fetch the content of a URL and summarize it.\n                result = await agent.run(task=\"Summarize the content of https://en.wikipedia.org/wiki/Seattle\")\n                print(result.messages[-1])\n\n\n            asyncio.run(main())\n"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-ext/src/autogen_ext/runtimes/grpc/protos/agent_worker_pb2_grpc.py",
        "keyword": "compress",
        "snippet": "            call_credentials=None,\n            insecure=False,\n            compression=None,\n            wait_for_ready=None,\n            timeout=None,\n            metadata=None):\n        return grpc.experimental.stream_stream(\n            request_iterator,\n--\n            insecure,\n            call_credentials,\n            compression,\n            wait_for_ready,\n            timeout,\n            metadata,\n            _registered_method=True)\n\n--\n            call_credentials=None,\n            ins"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/docs/src/conf.py",
        "keyword": "window",
        "snippet": "    t=l.createElement(r);t.async=1;t.src=\"https://www.clarity.ms/tag/\"+i;\n    y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);\n})(window, document, \"clarity\", \"script\", \"lnxpe6skj1\");\"\"\"\n    app.add_js_file(None, body=clarity_analytics)\n\n    return {\n        \"parallel_read_safe\": True,\n        \"parallel_write_safe\": True,\n"
      },
      {
        "repo": "autogen",
        "file": "/home/tobi/cloned_repositorys/_ARCHIVE/autogen/python/packages/autogen-ext/src/autogen_ext/tools/azure/_ai_search.py",
        "keyword": "window",
        "snippet": "                tool = AzureAISearchTool.create_full_text_search(\n                    name=\"doc-search\",\n                    endpoint=\"https://your-search.search.windows.net\",  # Your Azure AI Search endpoint\n                    index_name=\"<your-index>\",  # Name of your search index\n                    credential=AzureKeyCredential(\"<your-key>\"),  # Your Azure AI Search admin key\n                    query_type=\"simple\",  # Enable keyword search\n                    search_fields=[\"content\", \"tit"
      }
    ]
  },
  "tests_to_generate": [
    {
      "category": "file_discovery",
      "name": "test_find_files_by_ast",
      "description": "Find files using AST parsing (from aider repo_map)",
      "inspired_by": "aider"
    },
    {
      "category": "task_completion",
      "name": "test_context_relevance_ranking",
      "description": "Rank files by relevance to query (from OpenHands)",
      "inspired_by": "OpenHands"
    }
  ]
}