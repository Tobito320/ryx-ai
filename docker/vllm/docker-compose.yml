version: "3.8"

services:
  vllm:
    # Using ROCm 6.4.1 with vLLM 0.9.1 - tested with RDNA3
    image: rocm/vllm:rocm6.4.1_vllm_0.9.1_20250702
    container_name: ryx-vllm
    
    # AMD GPU access
    devices:
      - /dev/kfd
      - /dev/dri
    group_add:
      - video
      - render
    
    # Security for GPU access
    security_opt:
      - seccomp:unconfined
    cap_add:
      - SYS_PTRACE
    
    # Environment
    environment:
      - HIP_VISIBLE_DEVICES=0
      - HSA_OVERRIDE_GFX_VERSION=11.0.0  # RX 7800 XT = gfx1101
      - VLLM_LOGGING_LEVEL=INFO
      - HF_HOME=/models
      - VLLM_USE_TRITON_AWQ=1
      - VLLM_USE_TRITON_FLASH_ATTN=0
    
    # Volumes
    volumes:
      - /home/tobi/vllm-models:/models
      - /home/tobi/.cache/huggingface:/root/.cache/huggingface
    
    # Ports
    ports:
      - "8001:8001"  # OpenAI-compatible API (8001 to avoid RyxHub conflict)
    
    # Start vLLM server with 14B GPTQ model (best for German docs/letters)
    entrypoint: ["python", "-m", "vllm.entrypoints.openai.api_server"]
    command:
      - "--model"
      - "/models/powerful/general/qwen2.5-14b-gptq"
      - "--max-model-len"
      - "8192"
      - "--gpu-memory-utilization"
      - "0.90"
      - "--tensor-parallel-size"
      - "1"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8001"
      - "--trust-remote-code"
      - "--enforce-eager"
    
    # Restart policy
    restart: unless-stopped

  # Optional: Model download helper
  model-downloader:
    image: python:3.11-slim
    container_name: ryx-model-downloader
    volumes:
      - /home/tobi/vllm-models:/models
      - /home/tobi/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HF_HOME=/models
    profiles:
      - download
    command: >
      bash -c "pip install huggingface_hub && 
               huggingface-cli download Qwen/Qwen2.5-Coder-14B-Instruct-AWQ --local-dir /models/qwen2.5-coder-14b-awq"
