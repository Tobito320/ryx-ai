# ğŸŸ£ Ryx AI V2 - Local Agentic CLI

Your intelligent terminal companion powered by local AI models. Production-grade redesign with LLM-based intent classification, configurable model tiers, and agentic workflows.

## âœ¨ What's New in V2

### ğŸ¯ Single Interactive Experience
```bash
ryx                           # Start interactive session
ryx "refactor the parser"     # One-shot query
ryx --tier powerful "..."     # Use specific model tier
```

**No more weird `ryx ::command` syntax** - just type naturally!

### ğŸ§  LLM-Based Intent Classification
- **Replaces brittle keyword lists** with intelligent classification
- Automatic detection of: CHAT, CODE_EDIT, CONFIG_EDIT, FILE_OPS, WEB_RESEARCH, SYSTEM_TASK
- Minimal rule layer for obvious patterns, LLM for ambiguous cases

### âš¡ Configurable Model Tiers
| Tier | Model | Use Case |
|------|-------|----------|
| `fast` | mistral:7b | Quick responses |
| `balanced` | qwen2.5-coder:14b | Default coding (primary) |
| `powerful` | deepseek-coder-v2:16b | Complex tasks |
| `ultra` | Qwen3-Coder:30B | Heavy reasoning |
| `uncensored` | gpt-oss:20b | Personal reflection |

### ğŸ”§ Agentic Workflows
For complex tasks, Ryx automatically:
1. ğŸ“‹ **Plan**: Generate numbered plan
2. ğŸ” **Execute**: Call tools, feed outputs back
3. ğŸ§ª **Validate**: Run tests/linters
4. âœ… **Summarize**: Bullet list of changes

### ğŸ¨ Purple-Themed UI
```
ğŸŸ£ ryx â€“ Local AI Agent | Tier: balanced (qwen2.5-coder:14b)

ğŸ“‹ Planning...
  ğŸ” Step 1: Search for files
  ğŸ› ï¸ Step 2: Apply changes
  âœ… Done
```

## ğŸš€ Quick Start

### Installation
```bash
git clone https://github.com/Tobito320/ryx-ai
cd ryx-ai
pip install -r requirements.txt
chmod +x ryx
sudo ln -sf $(pwd)/ryx /usr/local/bin/ryx
```

### Usage
```bash
# Start interactive session
ryx

# One-shot queries
ryx "how do I reload hyprland?"
ryx "refactor the intent parser"
ryx "edit my waybar config"

# Use specific tier
ryx --tier powerful "design a REST API"
ryx --tier ultra "analyze this architecture"

# Session commands
/help       Show help
/status     Current status
/tier fast  Switch tier
/clear      Clear history
/quit       Exit
```

## ğŸ“– Examples

### Refactoring Code
```bash
ryx "refactor the intent parser to use LLM classification"

ğŸ“‹ Planning...
  1. Read current intent_parser.py
  2. Design new LLM-based classification
  3. Implement changes
  4. Run tests

  ğŸ” Step 1: Reading file...
  ğŸ› ï¸ Step 2: Applying changes...
  ğŸ§ª Step 3: Running tests...
  âœ… Done

**Summary**
- Replaced keyword lists with LLM classification
- Added IntentType enum
- All 29 tests passing
```

### Config Editing
```bash
ryx "analyze my Hyprland config, research best practices, update it"

ğŸŒ Searching for Hyprland best practices...
ğŸ“‚ Reading ~/.config/hypr/hyprland.conf...
ğŸ“‹ Generating improvements...

Suggested changes:
1. Add workspace animation settings
2. Optimize window rules
3. Add screenshot keybinds

Apply changes? [y/N]
```

### Web Research
```bash
ryx "research AI coding assistants, scrape and store comparison note"

ğŸ” Searching: AI coding assistants comparison
ğŸŒ Found 5 results

1. **Top AI Coding Assistants 2024**
   https://example.com/...
   
ğŸ’¾ Saved note to knowledge base
```

### Uncensored Conversation
```bash
ryx --tier uncensored "have an honest conversation about..."

âš ï¸ (uncensored mode)

[Response without filters]
```

## ğŸ› ï¸ Configuration

### Model Tiers (`configs/model_tiers.json`)
```json
{
  "ollama_base_url": "http://localhost:11434",
  "tiers": {
    "fast": {"model": "mistral:7b", ...},
    "balanced": {"model": "qwen2.5-coder:14b", ...},
    "powerful": {"model": "deepseek-coder-v2:16b", ...}
  },
  "default_tier": "balanced"
}
```

### Safety Settings (`configs/ryx_config.json`)
```json
{
  "safety": {
    "level": "normal",
    "require_confirmation": ["rm -rf", "chmod -R"],
    "block": ["rm -rf /", "dd if=/dev"]
  }
}
```

## ğŸ—ï¸ Architecture

```
ryx (main entry)
    â”‚
    â”œâ”€â–º SessionLoop (UI/Input)
    â”‚       â”‚
    â”‚       â”œâ”€â–º IntentClassifier (LLM-based)
    â”‚       â”‚       â””â”€â–º Returns: CHAT, CODE_EDIT, CONFIG_EDIT, etc.
    â”‚       â”‚
    â”‚       â”œâ”€â–º ModelRouter (Tier selection)
    â”‚       â”‚       â””â”€â–º fast/balanced/powerful/ultra/uncensored
    â”‚       â”‚
    â”‚       â””â”€â–º WorkflowOrchestrator (Complex tasks)
    â”‚               â””â”€â–º Plan â†’ Execute â†’ Validate â†’ Summary
    â”‚
    â”œâ”€â–º ToolRegistry
    â”‚       â”œâ”€â–º Filesystem (search, read, write, patch)
    â”‚       â”œâ”€â–º Web (fetch, search)
    â”‚       â”œâ”€â–º Shell (with safety controls)
    â”‚       â””â”€â–º Git (status, diff)
    â”‚
    â””â”€â–º RAGSystem (Caching)
```

See [docs/ARCHITECTURE_V2.md](docs/ARCHITECTURE_V2.md) for detailed documentation.

## ğŸ“‹ Requirements

- **OS**: Linux (Arch Linux recommended)
- **Python**: 3.11+
- **Ollama**: Running locally or in Docker
- **GPU**: AMD RX 7800 XT (16 GB VRAM) or equivalent
- **RAM**: 32 GB recommended

### Recommended Models
```bash
ollama pull qwen2.5-coder:14b      # Default coding
ollama pull deepseek-coder-v2:16b  # Complex tasks
ollama pull mistral:7b             # Fast responses
```

## ğŸ§ª Testing
```bash
# Run all tests
python -m pytest tests/ -v

# Run new V2 architecture tests
python -m pytest tests/test_v2_architecture.py -v
```

## ğŸ”§ Troubleshooting

### Ollama not running
```bash
# Start Ollama
ollama serve

# Or set custom URL
export OLLAMA_BASE_URL=http://localhost:11434
```

### Model not available
```bash
# List available models
ollama list

# Pull missing model
ollama pull qwen2.5-coder:14b
```

### Slow responses
```bash
# Switch to faster tier
/tier fast

# Or use --tier flag
ryx --tier fast "quick question"
```

## ğŸ“ License

MIT License - Use freely!

## ğŸ™ Acknowledgments

- Built with [Ollama](https://ollama.ai)
- Models: Qwen2.5-Coder, DeepSeek-Coder, Mistral
- Designed for Arch Linux with Hyprland

---

**ğŸŸ£ Ryx AI V2** - *Local AI that just works*