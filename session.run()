
Let me also update the intent parser to better handle the new requirements:

core/intent_parser.py
```python
"""
Ryx AI - Intent Parser
Parses user prompts to determine intended action before querying AI
"""

from dataclasses import dataclass
from typing import List, Optional
import re


@dataclass
class Intent:
    """Represents parsed user intent"""
    action: str  # 'chat', 'locate', 'execute', 'browse'
    target: Optional[str] = None  # What to act on (e.g., "hyprland config")
    modifiers: List[str] = None  # Additional flags like "new_terminal"
    original_prompt: str = ""  # Original user input
    model_switch: Optional[str] = None  # If user requested model switch

    def __post_init__(self):
        if self.modifiers is None:
            self.modifiers = []


class IntentParser:
    """Parse user intent from natural language prompts"""

    # Action keywords that determine what to do
    EXECUTE_KEYWORDS = ['open', 'edit', 'run', 'execute', 'launch', 'start', 'create', 'generate']
    BROWSE_KEYWORDS = ['look up', 'browse', 'google', 'search', 'what is', 'who is', 'search for', 'find information']
    LOCATE_KEYWORDS = ['find', 'where is', 'show me', 'locate', 'path to', 'where']

    # Modifier keywords that change how action is performed
    NEW_TERMINAL_KEYWORDS = ['new terminal', 'new window', 'separate terminal', 'separate window']

    # Model fallback chains (if preferred model not available, try these in order)
    MODEL_FALLBACKS = {
        'gpt-oss:20b': ['SimonPu/Qwen3-Coder:30B-Instruct_Q4_K_XL', 'deepseek-coder:6.7b', 'qwen2.5:3b', 'qwen2.5:1.5b'],
        'SimonPu/Qwen3-Coder:30B-Instruct_Q4_K_XL': ['gpt-oss:20b', 'deepseek-coder:6.7b', 'qwen2.5:3b', 'qwen2.5:1.5b'],
        'deepseek-coder:6.7b': ['qwen2.5:3b', 'qwen2.5:1.5b'],
        'qwen2.5:3b': ['qwen2.5:1.5b'],
        'qwen2.5:1.5b': [],  # Smallest model, no fallback
    }

    # Model switching keywords - maps user terms to actual model names
    MODEL_SWITCH_KEYWORDS = {
        # Specific model names
        'deepseek': 'deepseek-coder:6.7b',
        'qwen': 'qwen2.5:1.5b',
        'gpt-oss': 'gpt-oss:20b',
        'gpt oss': 'gpt-oss:20b',
        'llama': 'llama3.2:1b',
        'phi3': 'phi3:mini',

        # Size-based (try to find biggest available)
        'strongest': 'gpt-oss:20b',  # Largest model
        'smartest': 'gpt-oss:20b',   # Largest model
        'biggest': 'gpt-oss:20b',    # Largest model
        'best': 'gpt-oss:20b',       # Largest model
        'most powerful': 'gpt-oss:20b',
        'higher': 'deepseek-coder:6.7b',  # Next tier up
        'better': 'deepseek-coder:6.7b',  # Next tier up
        'stronger': 'deepseek-coder:6.7b',

        # Speed/performance
        'fast': 'qwen2.5:1.5b',
        'faster': 'qwen2.5:1.5b',
        'quick': 'qwen2.5:1.5b',
        'small': 'qwen2.5:1.5b',

        # Balanced
        'balanced': 'deepseek-coder:6.7b',
        'medium': 'deepseek-coder:6.7b',
        'moderate': 'deepseek-coder:6.7b',

        # Coding-specific
        'coder': 'deepseek-coder:6.7b',
        'coding': 'deepseek-coder:6.7b',

        # Compatibility with session mode names
        'powerful': 'gpt-oss:20b',
        'strong': 'deepseek-coder:6.7b',
    }

    def parse(self, prompt: str) -> Intent:
        """
        Parse prompt to determine user intent

        Args:
            prompt: User's natural language prompt

        Returns:
            Intent object with action, target, and modifiers
        """
        prompt_lower = prompt.lower()

        # Check for model switching first
        model_switch = self._detect_model_switch(prompt_lower)
        if model_switch:
            # Remove model switch request from prompt
            cleaned_prompt = self._remove_model_switch_text(prompt, prompt_lower)

            # If the cleaned prompt is empty or very short (just filler words), treat it as pure model switch
            cleaned_stripped = cleaned_prompt.strip()
            filler_words = ['please', 'now', 'i', 'want', 'to', 'the', 'model', 'a']
            remaining_words = [w for w in cleaned_stripped.lower().split() if w not in filler_words]

            if not cleaned_stripped or len(remaining_words) == 0:
                # Just a model switch request, no other action
                return Intent(
                    action='model_switch',
                    model_switch=model_switch,
                    original_prompt=prompt
                )
            else:
                # Continue with remaining prompt
                prompt = cleaned_prompt
                prompt_lower = prompt.lower()

        # Detect modifiers
        modifiers = []
        if any(kw in prompt_lower for kw in self.NEW_TERMINAL_KEYWORDS):
            modifiers.append('new_terminal')

        # Determine action based on keywords
        action = self._detect_action(prompt_lower)

        # Extract target (what user wants to act on)
        target = self._extract_target(prompt, action)

        return Intent(
            action=action,
            target=target,
            modifiers=modifiers,
            original_prompt=prompt,
            model_switch=model_switch
        )

    def _detect_action(self, prompt_lower: str) -> str:
        """Detect the intended action from prompt"""

        # Check execute keywords first (most specific)
        if any(kw in prompt_lower for kw in self.EXECUTE_KEYWORDS):
            return 'execute'

        # Check browse keywords
        if any(kw in prompt_lower for kw in self.BROWSE_KEYWORDS):
            return 'browse'

        # Check locate keywords
        if any(kw in prompt_lower for kw in self.LOCATE_KEYWORDS):
            return 'locate'

        # Check if prompt mentions a config file or path (implicit locate)
        if self._is_implicit_locate(prompt_lower):
            return 'locate'

        # Default to chat (just conversation)
        return 'chat'

    def _is_implicit_locate(self, prompt_lower: str) -> bool:
        """
        Check if prompt is implicitly asking for file location
        e.g., "hyprland config" without "open" should just locate
        FIXED: Now excludes conversational patterns to prevent false positives
        """
        # Exclude common conversational patterns
        conversation_starters = [
            'i need', 'i want', 'help me', 'how do', 'how can',
            'what is', 'tell me', 'show me how', 'can you', 'could you'
        ]

        # If it starts with conversation starter, it's not an implicit locate
        for starter in conversation_starters:
            if prompt_lower.startswith(starter):
                return False

        # Check if it's a simple file reference without action words
        # This is a more sophisticated check for implicit file location requests
        simple_file_patterns = [
            r'\.py$', r'\.js$', r'\.sh$', r'\.md$', r'\.txt$', r'\.conf$', r'\.yaml$', r'\.yml$'
        ]
        
        # If it's a simple file reference, it's likely a locate request
        for pattern in simple_file_patterns:
            if re.search(pattern, prompt_lower):
                return True

        # If it's a common config file name, it's likely a locate request
        config_files = ['config', 'settings', 'conf', 'rc', 'bashrc', 'zshrc', 'vimrc']
        for file in config_files:
            if file in prompt_lower:
                return True

        return False

    def _extract_target(self, prompt: str, action: str) -> Optional[str]:
        """Extract the target from the prompt based on action"""
        if action == 'locate':
            # For locate, we want to extract the file or directory name
            # This is a simplified approach - in practice, this would be more sophisticated
            return prompt
            
        elif action == 'execute':
            # For execute, we want to extract what's being requested to execute
            # This would be more complex in a real implementation
            return prompt
            
        elif action == 'browse':
            # For browse, we want to extract the search query
            return prompt
            
        else:
            # For chat, return None or the full prompt
            return None

    def get_complexity_score(self, prompt: str) -> float:
        """
        Calculate a complexity score for the prompt to help with model selection
        """
        prompt_lower = prompt.lower()
        
        # Simple scoring based on keywords
        complexity = 0.0
        
        # Add points for complexity indicators
        if any(word in prompt_lower for word in ['refactor', 'architecture', 'design', 'complex']):
            complexity += 0.3
            
        if any(word in prompt_lower for word in ['debug', 'fix', 'solve', 'problem']):
            complexity += 0.2
            
        if any(word in prompt_lower for word in ['research', 'investigate', 'analyze']):
            complexity += 0.25
            
        if any(word in prompt_lower for word in ['create', 'generate', 'build']):
            complexity += 0.15
            
        if any(word in prompt_lower for word in ['compare', 'contrast', 'evaluate']):
            complexity += 0.2
            
        # Add points for length (longer prompts often mean more complex tasks)
        word_count = len(prompt.split())
        complexity += min(word_count * 0.005, 0.3)  # Cap at 0.3
        
        # Ensure score is between 0 and 1
        return max(0.0, min(1.0, complexity))

    def get_model_for_complexity(self, complexity: float) -> str:
        """
        Select appropriate model based on complexity score
        """
        if complexity < 0.3:
            return "qwen2.5:1.5b"  # Fast model
        elif complexity < 0.7:
            return "deepseek-coder:6.7b"  # Balanced model
        else:
            return "qwen2.5-coder:14b"  # Powerful model

    def get_model_for_intent(self, intent: Intent) -> str:
        """
        Select appropriate model based on intent
        """
        if intent.action == 'chat':
            return "qwen2.5:1.5b"  # Fast model for chat
        elif intent.action == 'locate':
            return "qwen2.5:1.5b"  # Fast model for location
        elif intent.action == 'execute':
            return "deepseek-coder:6.7b"  # Balanced model for execution
        elif intent.action == 'browse':
            return "qwen2.5-coder:14b"  # Powerful model for browsing
        else:
            return "qwen2.5:1.5b"  # Default to fast model

    def get_model_for_prompt(self, prompt: str) -> str:
        """
        Get the appropriate model for a given prompt
        """
        intent = self.parse(prompt)
        complexity = self.get_complexity_score(prompt)
        
        # Use complexity-based selection for complex tasks
        if complexity > 0.5:
            return self.get_model_for_complexity(complexity)
        else:
            # Use intent-based selection for simpler tasks
            return self.get_model_for_intent(intent)

    def get_model_for_task(self, task_description: str) -> str:
        """
        Get appropriate model for a multi-step task
        """
        # For complex tasks, use powerful models
        complexity = self.get_complexity_score(task_description)
        return self.get_model_for_complexity(complexity)

    def _remove_model_switch_text(self, prompt: str, prompt_lower: str) -> str:
        """Remove model switch request from prompt"""

        # Common patterns to remove (more comprehensive)
        patterns = [
            r'please\s+switch\s+to\s+\w+(?:\s+\w+)?',  # "please switch to X" or "please switch to X Y"
            r'switch\s+to\s+\w+(?:\s+\w+)?',           # "switch to X" or "switch to X Y"
            r'use\s+\w+(?:\s+\w+)?\s+model',           # "use X model" or "use X Y model"
            r'use\s+\w+(?:\s+\w+)?',                   # "use X" or "use X Y"
            r'change\s+to\s+\w+(?:\s+\w+)?',           # "change to X" or "change to X Y"
            r'let\s+me\s+talk\s+to\s+\w+(?:\s+\w+)?',  # "let me talk to X"
            r'talk\s+to\s+\w+(?:\s+\w+)?',             # "talk to X"
            r'i\s+want\s+to\s+use\s+\w+(?:\s+\w+)?',   # "i want to use X"
            r'i\s+wanna\s+use\s+\w+(?:\s+\w+)?',       # "i wanna use X"
            r'now\s+use\s+\w+(?:\s+\w+)?',             # "now use X"
        ]

        for pattern in patterns:
            prompt = re.sub(pattern, '', prompt, flags=re.IGNORECASE)

        # Clean up punctuation and extra spaces
        prompt = re.sub(r'\s*,\s*$', '', prompt)  # Remove trailing comma
        prompt = re.sub(r'\s+', ' ', prompt)  # Normalize whitespace
        prompt = re.sub(r'^\s*and\s+', '', prompt, flags=re.IGNORECASE)  # Remove leading "and"

        return prompt.strip()

    def _detect_model_switch(self, prompt_lower: str) -> Optional[str]:
        """Detect if the prompt contains a model switch request"""
        # Check for model switch patterns
        for model_type, keywords in self.MODEL_SWITCH_KEYWORDS.items():
            if any(keyword in prompt_lower for keyword in keywords):
                return keywords
        
        # Check for explicit model names
        model_names = ['qwen2.5:1.5b', 'deepseek-coder:6.7b', 'qwen2.5-coder:14b', 'gpt-oss:20b']
        for model in model_names:
            if model in prompt_lower:
                return model
                
        return None
